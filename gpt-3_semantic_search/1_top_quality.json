[
  {
    "textbook-paragraph": "This set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is a digital finite state machine?",
      "answer": "\nA digital finite state machine is a digital circuit that can be in one of a finite number of states. The machine is usually initialized to a start state, and then transitions to other states based on inputs. Outputs are generated based on the current state."
    }
  },
  {
    "textbook-paragraph": "This set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.",
    "GPT-3-Generations": {
      "question": "\n\nWhen is it necessary to use an FSM?",
      "answer": "\nIt is necessary to use an FSM when the desired output cannot be achieved by using a simple combinational logic circuit. \n An FSM can be used to implement a sequential circuit, which has memory elements that store the present state of the machine.\n The output of an FSM depends on both the present inputs and past inputs (i.e., its present state)."
    }
  },
  {
    "textbook-paragraph": "We then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the design process for a counter with multiple inputs to control its behavior?",
      "answer": "\nThe design process for a counter with multiple inputs to control its behavior is as follows: \n\n1. First, an abstract model of the counter is created. This model captures the essential behavior of the counter, without worrying about implementation details. \n\n2. Next, this abstract model is used to generate a concrete implementation of the counter. This implementation can be in hardware or software, depending on the requirements of the system. \n\n3. Finally, the internal state of the FSM is represented in a way that makes it easy to understand and maintain. This representation can be in terms of a truth table, state diagram, or other suitable form."
    }
  },
  {
    "textbook-paragraph": "We then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the choice of representation for the FSM's internal state affect the complexity of the implementation?",
      "answer": "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation in a number of ways. Firstly, it can affect the number of states that need to be considered in the implementation. Secondly, it can affect the way in which state transitions are represented. And finally, it can affect the amount of memory required to store the FSM's internal state."
    }
  },
  {
    "textbook-paragraph": "Before we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\n\nWhat is the six-step process for designing an FSM?",
      "answer": "\nThe six-step process for designing an FSM is as follows:\n\n1. Identify the system inputs and outputs.\n2. Identify the system states.\n3. Identify the state transition rules.\n4. Identify the output generation rules.\n5. Draw the state transition diagram.\n6. Implement the FSM in hardware or software."
    }
  },
  {
    "textbook-paragraph": "{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\n\nHow does one develop an abstract model?",
      "answer": "\nThe first step in developing an abstract model is to specify the I/O behavior. This means specifying what inputs the system will accept and what outputs the system will produce. Next, the specification must be completed. This means specifying all of the requirements of the system in detail. Once the specification is complete, a state representation must be chosen. This will determine how the internal state of the system will be represented. Next, logic expressions must be calculated. These expressions will determine the output of the system based on its current state and input. Finally, the abstract model must be implemented with flip-flops and gates."
    }
  },
  {
    "textbook-paragraph": "In Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.",
    "GPT-3-Generations": {
      "question": "\n\nIn Step {step-abs}, what are we translating our description into?\nA. A model with states and desired behavior\nB. A human language model\nC. A precise and factual description\nD. An intent capture",
      "answer": "\nA. A model with states and desired behavior"
    }
  },
  {
    "textbook-paragraph": "In Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.",
    "GPT-3-Generations": {
      "question": "\n\nHow do we translate our description in human language into a model with states and desired behavior?",
      "answer": "\nOne way to translate our description in human language into a model with states and desired behavior is to create a state machine. This state machine would have different states that represent the different steps in the process described. For each state, there would be a corresponding set of desired behaviors. These desired behaviors would be what we want the system to do when it is in that particular state."
    }
  },
  {
    "textbook-paragraph": "Step {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the goal of step {step-io}?",
      "answer": "\nThe goal of step {step-io} is to formalize the model's input and output behavior."
    }
  },
  {
    "textbook-paragraph": "Step {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.",
    "GPT-3-Generations": {
      "question": "\n\nHow does one formalize the model's input and output behavior?",
      "answer": "\nOne can formalize the model's input and output behavior by specifying the input and output signals in terms of their bit values."
    }
  },
  {
    "textbook-paragraph": "Once we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the next step after making implicit assumptions clear and making other decisions necessary to the design in FSM?\n\nA. We start to make any implicit assumptions clear and to make any other decisions necessary to the design.\nB. Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.\nC. We check our work by retracing all possible states and inputs/outputs for each state transition until we are convinced that our FSM will work as intended for all possible input strings.\nD. We implement our FSM by drawing a state diagram and writing Verilog code or using a hardware description language (HDL) of our choice.",
      "answer": "\nThe next step after making implicit assumptions clear and making other decisions necessary to the design in FSM is to check our work by retracing all possible states and inputs/outputs for each state transition until we are convinced that our FSM will work as intended for all possible input strings."
    }
  },
  {
    "textbook-paragraph": "Once we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the I/O behavior for our FSM?",
      "answer": "\nThe I/O behavior for our FSM is that it will take in an input, and based on that input, it will either output a 0 or a 1."
    }
  },
  {
    "textbook-paragraph": "Let's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is a Gray code? \nA. A cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.\nB. A code that is used to send data over a noisy channel\nC. A way of representing numbers using a balanced ternary system\nD. None of the above",
      "answer": "\nA Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit."
    }
  },
  {
    "textbook-paragraph": "Let's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is a two-bit Gray code counter?",
      "answer": "\nA two-bit Gray code counter is a binary counter that cycles through all four possible combinations of two bits, in which each successive combination differs from the previous one in only one bit."
    }
  },
  {
    "textbook-paragraph": "For simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.",
    "GPT-3-Generations": {
      "question": "\n\nHow does an FSM use its internal state as output values?",
      "answer": "\nThe FSM uses its internal state as output values by taking the current state and outputting it as a binary number. This is done by using a truth table to determine the output for each state."
    }
  },
  {
    "textbook-paragraph": "The inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.",
    "GPT-3-Generations": {
      "question": "\n\nWhat are the three inputs typically given to a counter?",
      "answer": "\nThe inputs to a counter are typically a clock signal, a reset signal, and a load signal."
    }
  },
  {
    "textbook-paragraph": "We then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is the next-state logic?\nA. A type of computer memory\nB. A way of designing digital circuits\nC. A method of programming software\nD. None of the above",
      "answer": "\nThe next-state logic is a way of designing digital circuits. It is a method of programming software that uses the next-state logic to generate the next state of the circuit."
    }
  },
  {
    "textbook-paragraph": "We then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the implementation of the counter design?",
      "answer": "\nThe implementation of the counter design is to use the next-state logic to develop the implementation shown on the far right, completing our first counter design."
    }
  },
  {
    "textbook-paragraph": "Now we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.",
    "GPT-3-Generations": {
      "question": "\n\nQ: What is the main advantage of using a Gray code as the basis for the state sequence?\n\nA. It is easy to generate the next state in the sequence.\nB. It minimizes the number of state transitions.\nC. It is easy to decode.\nD. All of the above.",
      "answer": "\nThe main advantage of using a Gray code as the basis for the state sequence is that it minimizes the number of state transitions."
    }
  },
  {
    "textbook-paragraph": "Now we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the Gray code?",
      "answer": "\nThe Gray code is a sequence of binary numbers that are used to represent different states in a system. The code is named after its inventor, Frank Gray."
    }
  },
  {
    "textbook-paragraph": "Looking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is the main reason for ordering the vertical lines in the counter's implementation diagram?\nA. To make it easy to read and check the correctness of the logic\nB. To simplify understanding the diagram\nC. To make use of a reasonable order",
      "answer": "\nB. To simplify understanding the diagram"
    }
  },
  {
    "textbook-paragraph": "Looking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.",
    "GPT-3-Generations": {
      "question": "\n\nWhy is it important to have a reasonable order when designing a logic diagram?",
      "answer": "\nWhen designing any logic diagram, it is important to have a reasonable order so as to make it easy for someone (including yourself!) to read and check the correctness of the logic. This is because a well-ordered diagram will be easier to understand and will be less likely to contain errors."
    }
  },
  {
    "textbook-paragraph": "{Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main advantage of using a Finite State Machine (FSM) model?\n\nA. FSMs are broadly applicable in a range of engineering contexts.\nB. FSMs are simple and easy to design.\nC. FSMs allow for the development of more advanced combinational logic devices.\nD. FSMs provide a powerful abstraction for formalizing and reasoning about digital systems",
      "answer": "\nD. FSMs provide a powerful abstraction for formalizing and reasoning about digital systems"
    }
  },
  {
    "textbook-paragraph": "{Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a bit-sliced design and a serial design?",
      "answer": "\nA bit-sliced design is a digital circuit design in which the logic functions are performed by a series of interconnected logic gates, each of which performs a specific function on one or more bits of data. A serial design is a digital circuit design in which the data is stored in flip-flops and then fed back into the logic gates in the next cycle."
    }
  },
  {
    "textbook-paragraph": "{Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a bit-sliced design and a serial design?",
      "answer": "\nThe difference between a bit-sliced design and a serial design is that in a bit-sliced design, multiple copies of the logic for a single bit slice are connected in sequence, while in a serial design, only one copy of the logic for a single bit slice is used. In addition, in a serial design, the bits to be passed to the next bit slice are stored in flip-flops and provided back to the same physical bit slice as inputs in the next cycle."
    }
  },
  {
    "textbook-paragraph": "{Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhy is a serial design usually slower than a bit-sliced design?",
      "answer": "\nThe main reason a serial design is usually slower than a bit-sliced design is that it requires more time to complete the operation. In a bit-sliced design, each bit slice operates on its own set of input bits and produces its own set of output bits. This means that the entire operation can be completed in one clock cycle. In a serial design, however, the same bit slice must operate on the input bits and produce the output bits in each clock cycle. This requires more time and results in a slower overall operation."
    }
  },
  {
    "textbook-paragraph": "{Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about binary ripple counters?\n\nA) They are easy to design.\nB) They are less likely to have timing issues.\nC) They consume more energy.\nD) Their outputs take longer to settle after a clock edge.",
      "answer": "\nD) Their outputs take longer to settle after a clock edge."
    }
  },
  {
    "textbook-paragraph": "{Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n",
    "GPT-3-Generations": {
      "question": "What is a ripple counter?",
      "answer": "    \n    \n    \nA ripple counter is a type of digital counter where the output of each flip-flop is fed back to the clock input of the next flip-flop. This causes a 'ripple' effect as each successive flip-flop changes state. Ripple counters are typically used in devices with restricted energy supplies, as they consume less power than synchronous counters."
    }
  },
  {
    "textbook-paragraph": "{Timing Issues*}\n\nRipple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.\n\nMore aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.\n\nIf you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the benefits of using clock gating techniques?",
      "answer": "\nThe benefits of using clock gating techniques are that they can reduce power consumption and improve performance. Clock gating can also be used to reduce the complexity of a design by hiding the clock signal from flip-flops."
    }
  },
  {
    "textbook-paragraph": "{Machine Models}\n\nBefore we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.\n\nHistorically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.\n\nHowever, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.\n\nWhat is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.\n\nAs we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.\n\nThe disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.\n\nIn practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.\n\nThe coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.\n\nBy adding more states to the FSM, we were able to hold the servo in\nplace, as desired.\n\nWhy are we protecting you from the model used in practice?\n\nFirst, timing issues add complexity to a topic that is complex enough \nfor an introductory course.\n\nAnd, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.\n\nIn many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.\n\nWe now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.\n\nAs already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.  \n\nA Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.\n\nThe machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.\n\nNotice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.\n\nNotice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.\n\n\n\n{{file=part3/figs/lec17-3.eps,width=5in}}\n\n\n\nFor a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.\n\n\n\n{{file=part3/figs/lec17-4.eps,width=5in}}\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the difference between a Moore machine and a Mealy machine?",
      "answer": "\nThe main difference between a Moore machine and a Mealy machine is that in a Moore machine, outputs only depend on the internal state bits of the FSM, while in a Mealy machine, outputs may be expressed as functions of both internal state and FSM inputs. The benefit of using input signals to calculate outputs (the Mealy machine model) is that input bits effectively serve as additional system state, which means that the number of internal state bits can be reduced. The disadvantage of including input signals in the expressions for output signals is that timing characteristics of input signals may not be known, whereas an FSM designer may want to guarantee certain timing characteristics for output signals."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 3 of the Course}\n\n\nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nStudents often find this part of the course more challenging than the\nearlier parts of the course.\n\nIn addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.  \n\nStudents typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).  \n\nWe'll start with the easy stuff.  \n\nYou should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy\n\n}\n\n{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)\n\n}\n\n{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram\n\n}\n\n{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers\n\n}\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size\n\n}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM\n\n}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)\n\n}\n{processor datapath}\n\n{control signal}\n\n}\n\n{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux\n\n}\n\n{instruction processing}\n{-}{{}{}\n{}{}{}\n\n\n\n{register transfer language (RTL)}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}\n\n{data movement}\n{control flow}\n\n{addressing modes}\n{-}{{}{}\n{}{}{}\n\n\n{PC-relative}\n\n{base + offset}\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n\nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}\n\n\n\n\n\n{   }  empty 3rd page\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the difference between the Moore and Mealy machine models, as well as why you might find each of them useful when designing an FSM?",
      "answer": "\nThe Moore and Mealy machine models are two different ways of designing a finite state machine. The Moore model is a sequential circuit where the output depends only on the present state, while the Mealy model is a combinational circuit where the output depends on both the present state and the present input. The main difference between these two models is that in the Moore model, the output changes only when there is a change in state, while in the Mealy model, the output can change for both changes in state and changes in input. There are advantages and disadvantages to both models. The Moore model is simpler to design and understand, but it may require more states than the Mealy model to implement the same functionality. The Mealy model can be more efficient in terms of number of states required, but it may be more difficult to design and understand."
    }
  },
  {
    "textbook-paragraph": "{Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhich of the following is not a special-purpose register?\nA) The stack pointer\nB) The program counter\nC) The processor status register\nD) The zero register\nE) The general purpose register",
      "answer": "\n\nE) The general purpose register is not a special-purpose register."
    }
  },
  {
    "textbook-paragraph": "{Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the zero register?",
      "answer": "\nThe zero register is a special-purpose register that contains a value of zero. This register is read-only and is used as a constant value or as a destination for operations that are performed only for their side-effects. The availability of a zero register allows certain opcodes to serve double duty. For example, a register-to-register add instruction can be used as a register move instruction when one source operand is zero. Similarly, an immediate add instruction can be used as an immediate load instruction when one source operand is zero."
    }
  },
  {
    "textbook-paragraph": "{Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n",
    "GPT-3-Generations": {
      "question": "\n1. What is the name for the computers that were created in response to the reduced impact of variable-length instructions?\nA. RISC machines\nB. CISC machines\nC. VAX machines\nD. Alpha machines",
      "answer": "\nThe answer is A. RISC machines."
    }
  },
  {
    "textbook-paragraph": "{Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between RISC and CISC machines?",
      "answer": "\nRISC machines have fixed-length instructions and a load-store architecture, allowing only a few addressing modes and small offsets. This combination of design decisions enables deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and for years, RISC machines were viewed by many researchers as the proper design for future ISAs. \n\nCISC machines have variable-length instructions and support complex addressing modes as well as complex instructions. This allows compilers to generate more efficient code, but can also lead to longer instruction decoding times and shallower pipelines."
    }
  },
  {
    "textbook-paragraph": "{Interrupts and Exceptions*}\n\nUnexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.\n\n{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no \n\n}\n\nInterrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.\n\nThe code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.\n\nInterrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.\n\nAs several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the difference between an interrupt and an exception?",
      "answer": "\nThe main difference between an interrupt and an exception is that interrupts are generated by external devices, while exceptions are generated by the processor itself. Interrupts are asynchronous, while exceptions are not. Interrupts can be masked, while exceptions cannot. Finally, interrupts generally cause the current program to terminate, while exceptions may or may not do so."
    }
  },
  {
    "textbook-paragraph": "{Control Flow Conditions*}\n\nControl flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.  \n\nUnconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.\n\nMany ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.\n\n\n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}\n\nThe status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}\n\nFinally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}\n\nThe three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the three approaches to implementing control flow instructions?",
      "answer": "\nThe three approaches to implementing control flow instructions are:\n1. Unconditional control flow instructions\n2. Conditional control flow instructions\n3. Bit tests"
    }
  },
  {
    "textbook-paragraph": "{I/O*}\n\nAs a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.\n\nThe question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.\n\nAlternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about I/O ports?\n\nA) I/O ports are accessed using the same load and store instructions as are used to access memory.\nB) I/O ports are created with special instructions, such as the in and out instructions of the x86 architecture.\nC) A processor is designed with a number of I/O ports---usually read-only or write-only registers to which devices can be attached with opposite semantics.\nD) Device registers can be accessed using a region of the memory address space set aside for I/O.",
      "answer": "\nC) A processor is designed with a number of I/O ports---usually read-only or write-only registers to which devices can be attached with opposite semantics."
    }
  },
  {
    "textbook-paragraph": "{Control Unit Design}\n\nAppendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.\n\nIn this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.\n\nThe control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.\n\nIn this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.\n\nLet's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.\n\nOther von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.  \n\nThe processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).  \n\nThe outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.\n\nThe basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.\n\n",
    "GPT-3-Generations": {
      "question": "Which of the following is not a part of the control unit?\nA) Program counter\nB) Instruction register\nC) Memory unit\nD) Processing unit\nE) I/O interfaces",
      "answer": "\nC) Memory unit"
    }
  },
  {
    "textbook-paragraph": "{Using a Memory for Logic Functions}\n\nConsider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.\n\nSynthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.\n\nThis strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.\n\nIn programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.\n\nThe FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.\n\nFor many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.\n\nLet's return to our {LC-3} example.\n\nInstead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).\n\nWe just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.\n\nThe ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.\n\nWe can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.\n\nNext, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.\n\nAnd our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.\n\nFinally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.\n\nOur final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhich of the following is true about the context?\n\nA) A memory can be used to compute N arbitrary functions on m variables.\n\nB) If a mistake is made in computing one of the functions, a new bit or two just need to be changed in the memory.\n\nC) FPGAs have been used for many years to serve as a hardware prototyping platform.\nD) All of the above.",
      "answer": "\nD) All of the above."
    }
  },
  {
    "textbook-paragraph": "{Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is microprogrammed control?\nA) A method of designing a control unit by treating the state diagram as a program and using a small memory to hold microinstructions.\nB) A method of designing a control unit by decoding instructions and choosing one of several possible next states.\nC) A method of designing a control unit by sequencing microinstructions and using a small memory to hold state information.\nD) A method of designing a control unit by applying the FSM state number to the control ROM and getting back a set of control signals.",
      "answer": "\nMicroprogrammed control is a method of designing a control unit by treating the state diagram as a program and using a small memory to hold microinstructions. This approach allows for more flexibility in the design of the control unit, as well as simplifies the process of sequencing microinstructions."
    }
  },
  {
    "textbook-paragraph": "{Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is microprogrammed control?",
      "answer": "\nMicroprogrammed control is a method of implementing the control unit of a central processing unit (CPU) using a microprogram. In this approach, the control signals that drive the datapath are generated by a microprogram stored in a read-only memory (ROM). The address of the next microinstruction to execute is determined by the current state of the finite state machine (FSM) and the value of certain input signals. This method offers several advantages over hardwired control, including flexibility and ease of design modification."
    }
  },
  {
    "textbook-paragraph": "{Redundancy and Coding}\n\nThis set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.\n\nWe begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.\n\nBuilding on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.\n\nWe leave discussion of more sophisticated schemes to classes on\ncoding and information theory.\n\n\n",
    "GPT-3-Generations": {
      "question": "\n1. What is the primary purpose of using sparsely populated representations?\n\nA. To increase storage capacity\nB. To reduce the amount of data that needs to be processed\nC. To protect against accidental changes to bits\nD. To improve performance by reducing memory contention",
      "answer": "\nC. To protect against accidental changes to bits"
    }
  },
  {
    "textbook-paragraph": "{Hamming Distance}\n\nNext, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.\n\nAs a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.  \n\nWe refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.  \n\nThe metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.\n\nThe Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.\n\nThe Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.\n\nIn contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.\n\nNow let's think about the problem slightly differently.\n\nGiven a particular representation, \n\nhow many bit errors can we detect in values using that representation?\n\n{ A representation with Hamming distance d can detect up to d-1 bit errors.}\n\nTo understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.\n\nA digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the Hamming distance of the {2-out-of-5} code?\n\nA. 1\nB. 2\nC. 3\nD. 4",
      "answer": "\n\nThe Hamming distance of the {2-out-of-5} code is 2."
    }
  },
  {
    "textbook-paragraph": "{Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main advantage of Hamming codes?\n\na) They are able to correct one bit error\nb) They are able to correct multiple bit errors\nc) They require only logarithmic overhead for large numbers of bits\nd) None of the above",
      "answer": "\nThe main advantage of Hamming codes is that they are able to correct one bit error."
    }
  },
  {
    "textbook-paragraph": "{Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the minimum number of parity check bits required for a Hamming code?",
      "answer": "\nThe minimum number of parity check bits required for a Hamming code is 3."
    }
  },
  {
    "textbook-paragraph": "{SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about SEC-DED codes?\n\nA) They are used to enable a system to perform single error correction while also detecting any two bit errors.\n\nB) They are created by adding a parity bit to a Hamming code.\n\nC) They have a Hamming distance of 3.\n\nD) Adding a parity bit to any representation with an odd Hamming distance creates a new representation with Hamming distance one greater than the original representation.",
      "answer": "\nThe answer is D. Adding a parity bit to any representation with an odd Hamming distance creates a new representation with Hamming distance one greater than the original representation."
    }
  },
  {
    "textbook-paragraph": "{SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the definition of SEC-DED codes?",
      "answer": "\nSEC-DED codes are codes that can be used to correct a single error while also detecting any two bit errors. To create such a code, a parity bit is added to a Hamming code. This extra parity bit increases the Hamming distance to 4, which still allows for single bit error correction, but avoids the problem of Hamming distance 3 codes when two bit errors occur."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the three types of subdivisions in systematic decomposition?",
      "answer": "\nThe three types of subdivisions in systematic decomposition are sequential, conditional and iterative."
    }
  },
  {
    "textbook-paragraph": "{One Bit at a Time}\n\nMany of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.\n\nWhen we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.\n\nWhen we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.\n\nWhen we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.\n\nThe resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.\n\n\n",
    "GPT-3-Generations": {
      "question": "1. What is the best way to add two binary numbers? \nA. By adding the most significant bits first and then moving downward in significance.\nB. By adding the least significant bits first and then moving upward in significance.\nC. By building a combinational logic circuit that adds a single bit and passes any necessary information into another copy of itself.\nD. By using copies of a {bit-sliced} adder circuit to mimic our approach as humans and build adders of any size",
      "answer": "\nC. By building a combinational logic circuit that adds a single bit and passes any necessary information into another copy of itself."
    }
  },
  {
    "textbook-paragraph": "{Abstracting the Human Process}\n\n\nThink about how we as humans add two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation.\n\nAs you know, addition for 2's complement is identical except for the\ncalculation of overflow.\n\nWe start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.\n\nFocus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.\n\nThe diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the\n\n\n\n{{file=part2/figs/add-abs.eps,width=1.90in}}\n\n\n\n\nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.\n\nInternally, we use C_ to denote the carry input,\nand C_ to denote the carry output.\n\nSimilarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\nThe abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.\n\n{{file=part2/figs/add-integrated.eps,width=5.5in}}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a full adder and a half adder?",
      "answer": "\nA full adder is a logic circuit that adds three input bits and produces two output bits. A half adder is a logic circuit that only adds two input bits."
    }
  }
]