[
  {
    "textbook-paragraph": "This set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is a digital finite state machine?",
      "answer": "\nA digital finite state machine is a digital circuit that can be in one of a finite number of states. The machine is usually initialized to a start state, and then transitions to other states based on inputs. Outputs are generated based on the current state."
    }
  },
  {
    "textbook-paragraph": "This set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.",
    "GPT-3-Generations": {
      "question": "\n\nWhen is it necessary to use an FSM?",
      "answer": "\nIt is necessary to use an FSM when the desired output cannot be achieved by using a simple combinational logic circuit. \n An FSM can be used to implement a sequential circuit, which has memory elements that store the present state of the machine.\n The output of an FSM depends on both the present inputs and past inputs (i.e., its present state)."
    }
  },
  {
    "textbook-paragraph": "We then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the design process for a counter with multiple inputs to control its behavior?",
      "answer": "\nThe design process for a counter with multiple inputs to control its behavior is as follows: \n\n1. First, an abstract model of the counter is created. This model captures the essential behavior of the counter, without worrying about implementation details. \n\n2. Next, this abstract model is used to generate a concrete implementation of the counter. This implementation can be in hardware or software, depending on the requirements of the system. \n\n3. Finally, the internal state of the FSM is represented in a way that makes it easy to understand and maintain. This representation can be in terms of a truth table, state diagram, or other suitable form."
    }
  },
  {
    "textbook-paragraph": "We then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the choice of representation for the FSM's internal state affect the complexity of the implementation?",
      "answer": "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation in a number of ways. Firstly, it can affect the number of states that need to be considered in the implementation. Secondly, it can affect the way in which state transitions are represented. And finally, it can affect the amount of memory required to store the FSM's internal state."
    }
  },
  {
    "textbook-paragraph": "Before we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\n\nWhat is the six-step process for designing an FSM?",
      "answer": "\nThe six-step process for designing an FSM is as follows:\n\n1. Identify the system inputs and outputs.\n2. Identify the system states.\n3. Identify the state transition rules.\n4. Identify the output generation rules.\n5. Draw the state transition diagram.\n6. Implement the FSM in hardware or software."
    }
  },
  {
    "textbook-paragraph": "{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\n\nHow does one develop an abstract model?",
      "answer": "\nThe first step in developing an abstract model is to specify the I/O behavior. This means specifying what inputs the system will accept and what outputs the system will produce. Next, the specification must be completed. This means specifying all of the requirements of the system in detail. Once the specification is complete, a state representation must be chosen. This will determine how the internal state of the system will be represented. Next, logic expressions must be calculated. These expressions will determine the output of the system based on its current state and input. Finally, the abstract model must be implemented with flip-flops and gates."
    }
  },
  {
    "textbook-paragraph": "In Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.",
    "GPT-3-Generations": {
      "question": "\n\nIn Step {step-abs}, what are we translating our description into?\nA. A model with states and desired behavior\nB. A human language model\nC. A precise and factual description\nD. An intent capture",
      "answer": "\nA. A model with states and desired behavior"
    }
  },
  {
    "textbook-paragraph": "In Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.",
    "GPT-3-Generations": {
      "question": "\n\nHow do we translate our description in human language into a model with states and desired behavior?",
      "answer": "\nOne way to translate our description in human language into a model with states and desired behavior is to create a state machine. This state machine would have different states that represent the different steps in the process described. For each state, there would be a corresponding set of desired behaviors. These desired behaviors would be what we want the system to do when it is in that particular state."
    }
  },
  {
    "textbook-paragraph": "Step {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the goal of step {step-io}?",
      "answer": "\nThe goal of step {step-io} is to formalize the model's input and output behavior."
    }
  },
  {
    "textbook-paragraph": "Step {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.",
    "GPT-3-Generations": {
      "question": "\n\nHow does one formalize the model's input and output behavior?",
      "answer": "\nOne can formalize the model's input and output behavior by specifying the input and output signals in terms of their bit values."
    }
  },
  {
    "textbook-paragraph": "Once we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the next step after making implicit assumptions clear and making other decisions necessary to the design in FSM?\n\nA. We start to make any implicit assumptions clear and to make any other decisions necessary to the design.\nB. Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.\nC. We check our work by retracing all possible states and inputs/outputs for each state transition until we are convinced that our FSM will work as intended for all possible input strings.\nD. We implement our FSM by drawing a state diagram and writing Verilog code or using a hardware description language (HDL) of our choice.",
      "answer": "\nThe next step after making implicit assumptions clear and making other decisions necessary to the design in FSM is to check our work by retracing all possible states and inputs/outputs for each state transition until we are convinced that our FSM will work as intended for all possible input strings."
    }
  },
  {
    "textbook-paragraph": "Once we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the I/O behavior for our FSM?",
      "answer": "\nThe I/O behavior for our FSM is that it will take in an input, and based on that input, it will either output a 0 or a 1."
    }
  },
  {
    "textbook-paragraph": "Let's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is a Gray code? \nA. A cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.\nB. A code that is used to send data over a noisy channel\nC. A way of representing numbers using a balanced ternary system\nD. None of the above",
      "answer": "\nA Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit."
    }
  },
  {
    "textbook-paragraph": "Let's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is a two-bit Gray code counter?",
      "answer": "\nA two-bit Gray code counter is a binary counter that cycles through all four possible combinations of two bits, in which each successive combination differs from the previous one in only one bit."
    }
  },
  {
    "textbook-paragraph": "For simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.",
    "GPT-3-Generations": {
      "question": "\n\nHow does an FSM use its internal state as output values?",
      "answer": "\nThe FSM uses its internal state as output values by taking the current state and outputting it as a binary number. This is done by using a truth table to determine the output for each state."
    }
  },
  {
    "textbook-paragraph": "The inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.",
    "GPT-3-Generations": {
      "question": "\n\nWhat are the three inputs typically given to a counter?",
      "answer": "\nThe inputs to a counter are typically a clock signal, a reset signal, and a load signal."
    }
  },
  {
    "textbook-paragraph": "We then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is the next-state logic?\nA. A type of computer memory\nB. A way of designing digital circuits\nC. A method of programming software\nD. None of the above",
      "answer": "\nThe next-state logic is a way of designing digital circuits. It is a method of programming software that uses the next-state logic to generate the next state of the circuit."
    }
  },
  {
    "textbook-paragraph": "We then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the implementation of the counter design?",
      "answer": "\nThe implementation of the counter design is to use the next-state logic to develop the implementation shown on the far right, completing our first counter design."
    }
  },
  {
    "textbook-paragraph": "Now we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.",
    "GPT-3-Generations": {
      "question": "\n\nQ: What is the main advantage of using a Gray code as the basis for the state sequence?\n\nA. It is easy to generate the next state in the sequence.\nB. It minimizes the number of state transitions.\nC. It is easy to decode.\nD. All of the above.",
      "answer": "\nThe main advantage of using a Gray code as the basis for the state sequence is that it minimizes the number of state transitions."
    }
  },
  {
    "textbook-paragraph": "Now we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the Gray code?",
      "answer": "\nThe Gray code is a sequence of binary numbers that are used to represent different states in a system. The code is named after its inventor, Frank Gray."
    }
  },
  {
    "textbook-paragraph": "Looking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is the main reason for ordering the vertical lines in the counter's implementation diagram?\nA. To make it easy to read and check the correctness of the logic\nB. To simplify understanding the diagram\nC. To make use of a reasonable order",
      "answer": "\nB. To simplify understanding the diagram"
    }
  },
  {
    "textbook-paragraph": "Looking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.",
    "GPT-3-Generations": {
      "question": "\n\nWhy is it important to have a reasonable order when designing a logic diagram?",
      "answer": "\nWhen designing any logic diagram, it is important to have a reasonable order so as to make it easy for someone (including yourself!) to read and check the correctness of the logic. This is because a well-ordered diagram will be easier to understand and will be less likely to contain errors."
    }
  },
  {
    "textbook-paragraph": "{Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main advantage of using a Finite State Machine (FSM) model?\n\nA. FSMs are broadly applicable in a range of engineering contexts.\nB. FSMs are simple and easy to design.\nC. FSMs allow for the development of more advanced combinational logic devices.\nD. FSMs provide a powerful abstraction for formalizing and reasoning about digital systems",
      "answer": "\nD. FSMs provide a powerful abstraction for formalizing and reasoning about digital systems"
    }
  },
  {
    "textbook-paragraph": "{Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a bit-sliced design and a serial design?",
      "answer": "\nA bit-sliced design is a digital circuit design in which the logic functions are performed by a series of interconnected logic gates, each of which performs a specific function on one or more bits of data. A serial design is a digital circuit design in which the data is stored in flip-flops and then fed back into the logic gates in the next cycle."
    }
  },
  {
    "textbook-paragraph": "{Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a bit-sliced design and a serial design?",
      "answer": "\nThe difference between a bit-sliced design and a serial design is that in a bit-sliced design, multiple copies of the logic for a single bit slice are connected in sequence, while in a serial design, only one copy of the logic for a single bit slice is used. In addition, in a serial design, the bits to be passed to the next bit slice are stored in flip-flops and provided back to the same physical bit slice as inputs in the next cycle."
    }
  },
  {
    "textbook-paragraph": "{Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhy is a serial design usually slower than a bit-sliced design?",
      "answer": "\nThe main reason a serial design is usually slower than a bit-sliced design is that it requires more time to complete the operation. In a bit-sliced design, each bit slice operates on its own set of input bits and produces its own set of output bits. This means that the entire operation can be completed in one clock cycle. In a serial design, however, the same bit slice must operate on the input bits and produce the output bits in each clock cycle. This requires more time and results in a slower overall operation."
    }
  },
  {
    "textbook-paragraph": "{Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about binary ripple counters?\n\nA) They are easy to design.\nB) They are less likely to have timing issues.\nC) They consume more energy.\nD) Their outputs take longer to settle after a clock edge.",
      "answer": "\nD) Their outputs take longer to settle after a clock edge."
    }
  },
  {
    "textbook-paragraph": "{Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n",
    "GPT-3-Generations": {
      "question": "What is a ripple counter?",
      "answer": "    \n    \n    \nA ripple counter is a type of digital counter where the output of each flip-flop is fed back to the clock input of the next flip-flop. This causes a 'ripple' effect as each successive flip-flop changes state. Ripple counters are typically used in devices with restricted energy supplies, as they consume less power than synchronous counters."
    }
  },
  {
    "textbook-paragraph": "{Timing Issues*}\n\nRipple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.\n\nMore aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.\n\nIf you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the benefits of using clock gating techniques?",
      "answer": "\nThe benefits of using clock gating techniques are that they can reduce power consumption and improve performance. Clock gating can also be used to reduce the complexity of a design by hiding the clock signal from flip-flops."
    }
  },
  {
    "textbook-paragraph": "{Machine Models}\n\nBefore we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.\n\nHistorically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.\n\nHowever, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.\n\nWhat is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.\n\nAs we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.\n\nThe disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.\n\nIn practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.\n\nThe coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.\n\nBy adding more states to the FSM, we were able to hold the servo in\nplace, as desired.\n\nWhy are we protecting you from the model used in practice?\n\nFirst, timing issues add complexity to a topic that is complex enough \nfor an introductory course.\n\nAnd, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.\n\nIn many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.\n\nWe now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.\n\nAs already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.  \n\nA Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.\n\nThe machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.\n\nNotice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.\n\nNotice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.\n\n\n\n{{file=part3/figs/lec17-3.eps,width=5in}}\n\n\n\nFor a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.\n\n\n\n{{file=part3/figs/lec17-4.eps,width=5in}}\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the difference between a Moore machine and a Mealy machine?",
      "answer": "\nThe main difference between a Moore machine and a Mealy machine is that in a Moore machine, outputs only depend on the internal state bits of the FSM, while in a Mealy machine, outputs may be expressed as functions of both internal state and FSM inputs. The benefit of using input signals to calculate outputs (the Mealy machine model) is that input bits effectively serve as additional system state, which means that the number of internal state bits can be reduced. The disadvantage of including input signals in the expressions for output signals is that timing characteristics of input signals may not be known, whereas an FSM designer may want to guarantee certain timing characteristics for output signals."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 3 of the Course}\n\n\nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nStudents often find this part of the course more challenging than the\nearlier parts of the course.\n\nIn addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.  \n\nStudents typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).  \n\nWe'll start with the easy stuff.  \n\nYou should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy\n\n}\n\n{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)\n\n}\n\n{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram\n\n}\n\n{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers\n\n}\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size\n\n}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM\n\n}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)\n\n}\n{processor datapath}\n\n{control signal}\n\n}\n\n{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux\n\n}\n\n{instruction processing}\n{-}{{}{}\n{}{}{}\n\n\n\n{register transfer language (RTL)}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}\n\n{data movement}\n{control flow}\n\n{addressing modes}\n{-}{{}{}\n{}{}{}\n\n\n{PC-relative}\n\n{base + offset}\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n\nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}\n\n\n\n\n\n{   }  empty 3rd page\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the difference between the Moore and Mealy machine models, as well as why you might find each of them useful when designing an FSM?",
      "answer": "\nThe Moore and Mealy machine models are two different ways of designing a finite state machine. The Moore model is a sequential circuit where the output depends only on the present state, while the Mealy model is a combinational circuit where the output depends on both the present state and the present input. The main difference between these two models is that in the Moore model, the output changes only when there is a change in state, while in the Mealy model, the output can change for both changes in state and changes in input. There are advantages and disadvantages to both models. The Moore model is simpler to design and understand, but it may require more states than the Mealy model to implement the same functionality. The Mealy model can be more efficient in terms of number of states required, but it may be more difficult to design and understand."
    }
  },
  {
    "textbook-paragraph": "{Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhich of the following is not a special-purpose register?\nA) The stack pointer\nB) The program counter\nC) The processor status register\nD) The zero register\nE) The general purpose register",
      "answer": "\n\nE) The general purpose register is not a special-purpose register."
    }
  },
  {
    "textbook-paragraph": "{Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the zero register?",
      "answer": "\nThe zero register is a special-purpose register that contains a value of zero. This register is read-only and is used as a constant value or as a destination for operations that are performed only for their side-effects. The availability of a zero register allows certain opcodes to serve double duty. For example, a register-to-register add instruction can be used as a register move instruction when one source operand is zero. Similarly, an immediate add instruction can be used as an immediate load instruction when one source operand is zero."
    }
  },
  {
    "textbook-paragraph": "{Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n",
    "GPT-3-Generations": {
      "question": "\n1. What is the name for the computers that were created in response to the reduced impact of variable-length instructions?\nA. RISC machines\nB. CISC machines\nC. VAX machines\nD. Alpha machines",
      "answer": "\nThe answer is A. RISC machines."
    }
  },
  {
    "textbook-paragraph": "{Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between RISC and CISC machines?",
      "answer": "\nRISC machines have fixed-length instructions and a load-store architecture, allowing only a few addressing modes and small offsets. This combination of design decisions enables deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and for years, RISC machines were viewed by many researchers as the proper design for future ISAs. \n\nCISC machines have variable-length instructions and support complex addressing modes as well as complex instructions. This allows compilers to generate more efficient code, but can also lead to longer instruction decoding times and shallower pipelines."
    }
  },
  {
    "textbook-paragraph": "{Interrupts and Exceptions*}\n\nUnexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.\n\n{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no \n\n}\n\nInterrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.\n\nThe code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.\n\nInterrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.\n\nAs several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the difference between an interrupt and an exception?",
      "answer": "\nThe main difference between an interrupt and an exception is that interrupts are generated by external devices, while exceptions are generated by the processor itself. Interrupts are asynchronous, while exceptions are not. Interrupts can be masked, while exceptions cannot. Finally, interrupts generally cause the current program to terminate, while exceptions may or may not do so."
    }
  },
  {
    "textbook-paragraph": "{Control Flow Conditions*}\n\nControl flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.  \n\nUnconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.\n\nMany ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.\n\n\n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}\n\nThe status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}\n\nFinally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}\n\nThe three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the three approaches to implementing control flow instructions?",
      "answer": "\nThe three approaches to implementing control flow instructions are:\n1. Unconditional control flow instructions\n2. Conditional control flow instructions\n3. Bit tests"
    }
  },
  {
    "textbook-paragraph": "{I/O*}\n\nAs a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.\n\nThe question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.\n\nAlternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about I/O ports?\n\nA) I/O ports are accessed using the same load and store instructions as are used to access memory.\nB) I/O ports are created with special instructions, such as the in and out instructions of the x86 architecture.\nC) A processor is designed with a number of I/O ports---usually read-only or write-only registers to which devices can be attached with opposite semantics.\nD) Device registers can be accessed using a region of the memory address space set aside for I/O.",
      "answer": "\nC) A processor is designed with a number of I/O ports---usually read-only or write-only registers to which devices can be attached with opposite semantics."
    }
  },
  {
    "textbook-paragraph": "{Control Unit Design}\n\nAppendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.\n\nIn this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.\n\nThe control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.\n\nIn this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.\n\nLet's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.\n\nOther von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.  \n\nThe processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).  \n\nThe outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.\n\nThe basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.\n\n",
    "GPT-3-Generations": {
      "question": "Which of the following is not a part of the control unit?\nA) Program counter\nB) Instruction register\nC) Memory unit\nD) Processing unit\nE) I/O interfaces",
      "answer": "\nC) Memory unit"
    }
  },
  {
    "textbook-paragraph": "{Using a Memory for Logic Functions}\n\nConsider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.\n\nSynthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.\n\nThis strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.\n\nIn programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.\n\nThe FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.\n\nFor many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.\n\nLet's return to our {LC-3} example.\n\nInstead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).\n\nWe just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.\n\nThe ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.\n\nWe can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.\n\nNext, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.\n\nAnd our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.\n\nFinally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.\n\nOur final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhich of the following is true about the context?\n\nA) A memory can be used to compute N arbitrary functions on m variables.\n\nB) If a mistake is made in computing one of the functions, a new bit or two just need to be changed in the memory.\n\nC) FPGAs have been used for many years to serve as a hardware prototyping platform.\nD) All of the above.",
      "answer": "\nD) All of the above."
    }
  },
  {
    "textbook-paragraph": "{Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is microprogrammed control?\nA) A method of designing a control unit by treating the state diagram as a program and using a small memory to hold microinstructions.\nB) A method of designing a control unit by decoding instructions and choosing one of several possible next states.\nC) A method of designing a control unit by sequencing microinstructions and using a small memory to hold state information.\nD) A method of designing a control unit by applying the FSM state number to the control ROM and getting back a set of control signals.",
      "answer": "\nMicroprogrammed control is a method of designing a control unit by treating the state diagram as a program and using a small memory to hold microinstructions. This approach allows for more flexibility in the design of the control unit, as well as simplifies the process of sequencing microinstructions."
    }
  },
  {
    "textbook-paragraph": "{Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is microprogrammed control?",
      "answer": "\nMicroprogrammed control is a method of implementing the control unit of a central processing unit (CPU) using a microprogram. In this approach, the control signals that drive the datapath are generated by a microprogram stored in a read-only memory (ROM). The address of the next microinstruction to execute is determined by the current state of the finite state machine (FSM) and the value of certain input signals. This method offers several advantages over hardwired control, including flexibility and ease of design modification."
    }
  },
  {
    "textbook-paragraph": "{Redundancy and Coding}\n\nThis set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.\n\nWe begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.\n\nBuilding on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.\n\nWe leave discussion of more sophisticated schemes to classes on\ncoding and information theory.\n\n\n",
    "GPT-3-Generations": {
      "question": "\n1. What is the primary purpose of using sparsely populated representations?\n\nA. To increase storage capacity\nB. To reduce the amount of data that needs to be processed\nC. To protect against accidental changes to bits\nD. To improve performance by reducing memory contention",
      "answer": "\nC. To protect against accidental changes to bits"
    }
  },
  {
    "textbook-paragraph": "{Hamming Distance}\n\nNext, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.\n\nAs a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.  \n\nWe refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.  \n\nThe metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.\n\nThe Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.\n\nThe Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.\n\nIn contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.\n\nNow let's think about the problem slightly differently.\n\nGiven a particular representation, \n\nhow many bit errors can we detect in values using that representation?\n\n{ A representation with Hamming distance d can detect up to d-1 bit errors.}\n\nTo understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.\n\nA digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the Hamming distance of the {2-out-of-5} code?\n\nA. 1\nB. 2\nC. 3\nD. 4",
      "answer": "\n\nThe Hamming distance of the {2-out-of-5} code is 2."
    }
  },
  {
    "textbook-paragraph": "{Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main advantage of Hamming codes?\n\na) They are able to correct one bit error\nb) They are able to correct multiple bit errors\nc) They require only logarithmic overhead for large numbers of bits\nd) None of the above",
      "answer": "\nThe main advantage of Hamming codes is that they are able to correct one bit error."
    }
  },
  {
    "textbook-paragraph": "{Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the minimum number of parity check bits required for a Hamming code?",
      "answer": "\nThe minimum number of parity check bits required for a Hamming code is 3."
    }
  },
  {
    "textbook-paragraph": "{SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about SEC-DED codes?\n\nA) They are used to enable a system to perform single error correction while also detecting any two bit errors.\n\nB) They are created by adding a parity bit to a Hamming code.\n\nC) They have a Hamming distance of 3.\n\nD) Adding a parity bit to any representation with an odd Hamming distance creates a new representation with Hamming distance one greater than the original representation.",
      "answer": "\nThe answer is D. Adding a parity bit to any representation with an odd Hamming distance creates a new representation with Hamming distance one greater than the original representation."
    }
  },
  {
    "textbook-paragraph": "{SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the definition of SEC-DED codes?",
      "answer": "\nSEC-DED codes are codes that can be used to correct a single error while also detecting any two bit errors. To create such a code, a parity bit is added to a Hamming code. This extra parity bit increases the Hamming distance to 4, which still allows for single bit error correction, but avoids the problem of Hamming distance 3 codes when two bit errors occur."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the three types of subdivisions in systematic decomposition?",
      "answer": "\nThe three types of subdivisions in systematic decomposition are sequential, conditional and iterative."
    }
  },
  {
    "textbook-paragraph": "{One Bit at a Time}\n\nMany of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.\n\nWhen we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.\n\nWhen we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.\n\nWhen we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.\n\nThe resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.\n\n\n",
    "GPT-3-Generations": {
      "question": "1. What is the best way to add two binary numbers? \nA. By adding the most significant bits first and then moving downward in significance.\nB. By adding the least significant bits first and then moving upward in significance.\nC. By building a combinational logic circuit that adds a single bit and passes any necessary information into another copy of itself.\nD. By using copies of a {bit-sliced} adder circuit to mimic our approach as humans and build adders of any size",
      "answer": "\nC. By building a combinational logic circuit that adds a single bit and passes any necessary information into another copy of itself."
    }
  },
  {
    "textbook-paragraph": "{Abstracting the Human Process}\n\n\nThink about how we as humans add two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation.\n\nAs you know, addition for 2's complement is identical except for the\ncalculation of overflow.\n\nWe start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.\n\nFocus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.\n\nThe diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the\n\n\n\n{{file=part2/figs/add-abs.eps,width=1.90in}}\n\n\n\n\nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.\n\nInternally, we use C_ to denote the carry input,\nand C_ to denote the carry output.\n\nSimilarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\nThe abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.\n\n{{file=part2/figs/add-integrated.eps,width=5.5in}}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a full adder and a half adder?",
      "answer": "\nA full adder is a logic circuit that adds three input bits and produces two output bits. A half adder is a logic circuit that only adds two input bits."
    }
  },
  {
    "textbook-paragraph": "K-maps, we must start again with new K-maps for the lower-case checker.  If instead we have taken the approach of designing logic for the upper and  lower bits of the ASCII character, we can reuse most of that logic, since the functions T_4 and T_5 are identical when checking for a lower-case character.  Recalling the algebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below.   {eqnarray*} U&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5  L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5  &=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+ &&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0}) {eqnarray*}   {file=part2/figs/ascii-cmp-lower.eps,width=3.6in}   Finally, if we have used a design based on comparators or adders, the design of a lower-case checker becomes trivial: simply change the numbers that we input to these components, as shown in the figure on the right above for the comparator-based design.  The only changes from the upper-case checker design are the inputs to the comparators and the output produced,  highlighted with blue text in the figure.    {The Multiplexer}  Using the more abstract designs for checking ranges of ASCII characters, we can go a step further and create a checker for both upper- and lower-case letters.  To do so, we add another input S that allows us to select the  function that we want---either the upper-case checker U(C) or the  lower-case checker L(C).  For this purpose, we make use of a logic block called a { multiplexer}, or { mux}. Multiplexers are an important abstraction for digital logic.  In  general, a multiplexer allows us to use one digital signal to  select which of several others is forwarded to an output.   The simplest form of the multiplexer is the 2-to-1 multiplexer shown to  the right.   The logic diagram illustrates how the mux works.  The block  has two inputs from the left and one from the top.  The top input allows  us to choose which of the left inputs is forwarded to the output.   When the input S=0, the upper AND gate outputs 0, and the lower AND gate outputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0. Similarly, when input S=1, the upper AND gate outputs D_1, and the lower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.      The symbolic form of the mux is a trapezoid with data inputs on the  larger side, an output on the smaller side, and a select input on the angled part of the trapezoid.  The labels inside the trapezoid indicate  the value of the select input S for which the adjacent data signal,  D_1 or D_0, is copied to the output Q.  We can generalize multiplexers in two ways.  First, we can extend the  single select input to a group of select inputs.  An {N-bit} select input allows selection from amongst 2^N inputs.  A {4-to-1}  multiplexer is shown below, for example.  The logic diagram on the left shows how the {4-to-1 mux} operates.  For any combination of S_1S_0, three of the AND gates produce 0, and the fourth outputs the D input corresponding to the interpretation of S as an unsigned number. Given three zeroes and one D input, the OR gate thus reproduces one of  the D's.  When S_1S_0=10, for example, the third AND gate copies D_2, and Q=D_2.  {{file=part2/figs/mux4-to-1.eps,width=5.60in}}  As shown in the middle figure, a {4-to-1} mux can also be built from three {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux  appears on the right in the figure.    The second way in which we can generalize multiplexers is by using several multiplexers of the same type and using the same signals for  selection.  For example, we might use a single select bit T to choose  between any number of paired inputs.  Denote input pair by i D_1^i  and D_0^i.  For each pair, we have an output Q_i.    When T=0, Q_i=D_0^i for each value of i.  And, when T=1, Q_i=D_1^i for each value of i.  Each value of i requires a {2-to-1} mux with its select input driven by the global select signal T.  Returning to the example of the upper-",
    "GPT-3-Generations": {
      "question": "What is a multiplexer?\\n",
      "answer": "\nA multiplexer is a digital logic block that allows one digital signal to select which of several others is forwarded to an output. The simplest form of the multiplexer is the 2-to-1 multiplexer, which has two inputs from the left and one from the top. The top input allows us to choose which of the left inputs is forwarded to the output."
    }
  },
  {
    "textbook-paragraph": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals.",
    "GPT-3-Generations": {
      "question": "What is the design process for a digital FSM?\\n",
      "answer": "\nThe design process for a digital FSM typically consists of six steps: (1) developing an abstract model, (2) specifying I/O behavior, (3) completing the specification, (4) choosing a state representation, (5) calculating logic expressions, and (6) implementing with flip-flops and gates."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Generations": {
      "question": "What is a multiplexer?', \"\\n",
      "answer": "\nA multiplexer is a device that allows multiple signals to be routed onto a single channel. This is done by selecting one of several input signals and connecting it to the output. Multiplexers are used in a variety of applications, such as telecommunications and computer networking."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Generations": {
      "question": " How does one use abstraction to simplify an implementation?\\n",
      "answer": "\nAbstraction is the process of simplifying a complex system by identifying the most important aspects of that system and ignoring the rest. In the context of implementation, abstraction can be used to simplify a design by identifying the most important features of the design and ignoring the rest. This can make the design more manageable and easier to understand."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Generations": {
      "question": " Why is it important to make implicit assumptions clear in the design process?\\n",
      "answer": "\nThere are a few reasons why it is important to make implicit assumptions clear in the design process. First, if assumptions are not made clear, it can lead to confusion and misunderstanding among the team members working on the design. Second, making assumptions explicit can help to ensure that everyone is on the same page and working towards the same goal. Finally, explicit assumptions can help to prevent errors and mistakes in the design process."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Generations": {
      "question": " How does the choice of an FSM's input and output behavior affect the design process?\\n",
      "answer": "\nThe choice of an FSM's input and output behavior affects the design process by determining the set of possible states and the set of possible outputs. The set of possible states is determined by the set of possible inputs, and the set of possible outputs is determined by the set of possible outputs."
    }
  },
  {
    "textbook-paragraph": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more",
    "GPT-3-Generations": {
      "question": " What is a Gray code?\\n",
      "answer": "\nA Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Generations": {
      "question": " What is the purpose of a Gray code counter?\\n",
      "answer": "\nA Gray code counter is a device that is used to count the number of operations in a logic expression. The device works by counting the number of variables in the expression and adding the number of operations. The device is used to optimize logic expressions by reducing the number of operations needed to implement the logic function."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Generations": {
      "question": " What is the transition diagram for a two-bit Gray code counter?\\n",
      "answer": "\nThe transition diagram for a two-bit Gray code counter is a loop with the counter moving from one state to the next each cycle. Each state in the diagram is marked with the internal state value S_1S_0 (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are always equal for this counter."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Generations": {
      "question": " What are the inputs and outputs of a Gray code counter?\\n",
      "answer": "\nA Gray code counter is a type of digital counter that uses a Gray code to generate its output. The input to a Gray code counter is a digital signal that represents the number of clock pulses that have been applied to the counter. The output of a Gray code counter is a digital signal that represents the number of clock pulses that have been applied to the counter, plus one."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Generations": {
      "question": " How do you design a Gray code counter?']",
      "answer": "\nA Gray code counter is designed by first creating a transition diagram that shows the states and outputs of the counter. Next, the next-state logic is developed by creating K-maps for the next-state values and deriving algebraic expressions. Finally, the next-state logic is implemented in a diagram."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Generations": {
      "question": " What are some applications of a Gray code counter?\\n",
      "answer": "\nA Gray code counter is used in applications where a digital value must be changed incrementally with the least number of steps. This is because a Gray code counter only changes one bit at a time, which minimizes the number of digital values that must be changed."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Generations": {
      "question": " What are the inputs and outputs of a Gray code counter?\\n",
      "answer": "\nA Gray code counter is a type of digital counter that uses a Gray code to generate its output. The input to a Gray code counter is a digital signal that represents the number of clock pulses that have been applied to the counter. The output of a Gray code counter is a digital signal that represents the number of clock pulses that have been applied to the counter, plus one."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Generations": {
      "question": " What is the fully-specified transition diagram for a three-bit Gray code counter?\\n",
      "answer": "\nThe fully-specified transition diagram for a three-bit Gray code counter is as follows:\n\nThe states of the counter form a loop, with the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 (before \" / \") and the output Z_2Z_1Z_0 (after \" / \").\n\nBased on the transition diagram, the next-state values S_2^+, S_1^+, and S_0^+ can be filled in as shown."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Generations": {
      "question": " What is the equation for the next-state logic of a three-bit Gray code counter?\\n",
      "answer": "\nThe next-state logic for a three-bit Gray code counter is as follows:\n\nS_2^+ = S_2 + S_0\nS_1^+ = S_2\nS_0^+ = S_2 S_1"
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Generations": {
      "question": " What is the implementation diagram for a three-bit Gray code counter?\\n",
      "answer": "\nThe implementation diagram for a three-bit Gray code counter is a logic diagram that shows the inputs, outputs, and next-state logic for the counter. The diagram is ordered from left to right as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2, which makes it easy to read and check for correctness."
    }
  },
  {
    "textbook-paragraph": "have other names for the smaller values: a point for N=0, a line segment for N=1, a square for N=2, and a cube for N=3.  The diagrams above and to the right illustrate the cases that  are easily drawn on paper.  The black dots represent specific input combinations, and the blue edges connect input combinations that differ in exactly one input value (one bit).    By viewing a function's domain in this way, we can make a connection between a product of literals and the structure of the domain.  Let's use the {3-dimensional} version as an example.  We call the  variables A, B, and C, and note that the cube has 2^3=8 corners corresponding to the 2^3 possible combinations of A, B, and C. The simplest product of literals in this case is 1, which is the product of 0 literals.  Obviously, the product 1  evaluates to 1 for any variable values.  We can thus think of it as covering the entire domain of the function.  In the case of our example, the product 1 covers the whole cube.  In order for the product 1 to  be an implicant of a function, the function itself must be the function 1.  What about a product consisting of a single literal, such as A  or ?  The dividing lines in the diagram illustrate the answer: any such product term evaluates to 1 on a face of the cube, which includes 2^2=4 of the corners.  If a function evaluates to 1 on any of the six faces of the cube, the corresponding product term (consisting of a single literal) is an implicant of the function.  Continuing with products of two literals, we see that any product of  two literals, such as A or C, corresponds to an edge of our {3-dimensional} cube.  The edge includes 2^1=2 corners.  And, if a function evaluates to 1 on any of the 12 edges of the cube, the corresponding product term (consisting of two literals) is an implicant of the function.  Finally, any product of three literals, such as B, corresponds to a corner of the cube.  But for a function on three variables, these are just the minterms.  As you know, if a function evaluates to 1 on any of the 8 corners of the cube, that minterm is an implicant of the function (we used this idea to construct the function to prove logical completeness).  How do these connections help us to simplify functions?  If we're careful, we can map cubes onto paper in such a way that product terms (the possible implicants of the function) usually form contiguous  groups of 1s, allowing us to spot them easily.  Let's work upwards starting from one variable to see how this idea works.  The end result is called a Karnaugh map.   The first drawing shown to the right replicates our view of  the {1-dimensional} hypercube, corresponding to the domain of a function on one variable, in this case the variable A.   To the right of the hypercube (line segment) are two variants of a Karnaugh map on one variable.  The middle variant clearly  indicates the column corresponding to the product A (the other  column corresponds to ).  The right variant simply labels the column with values for A.       The three drawings shown to the right illustrate the three possible product terms on one variable.  { The functions shown in  these Karnaugh maps are arbitrary, except that we have chosen them such that each implicant shown is a prime implicant for the illustrated function.}        Let's now look at two-variable functions.  We have replicated our drawing of the {2-dimensional} hypercube (square) to the right  along with two variants of Karnaugh maps on two variables. With only two variables (A and B), the extension is fairly  straightforward, since we can use the second dimension of the  paper (vertical) to express the second variable (B).       The number of possible products of literals grows rapidly with the  number of variables. For two variables, nine are possible, as shown to the right. Notice that all implicants have two properties.  First, they occupy contiguous regions of the grid.  And, second, their height and width are always powers of two.",
    "GPT-3-Generations": {
      "question": " What is a K-map?\\n",
      "answer": "\nA Karnaugh map is a graphical representation of a Boolean function that helps to simplify the function. The map consists of a grid of squares, each of which corresponds to a possible combination of input values. The map is used to find groups of product terms that are implicants of the function."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Generations": {
      "question": " What is the purpose of an abstract model?\\n",
      "answer": "\nAn abstract model is a simplified representation of a system, typically used to aid in understanding the system. In the context of logic optimization, an abstract model can be used to help understand the tradeoffs between different logic expressions."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Generations": {
      "question": " What is the difference between a next-state table and a transition diagram?\\n",
      "answer": "\nA next-state table lists all of the possible states that the FSM can be in, as well as the inputs and outputs for each state. A transition diagram shows the order in which the states will be visited, based on the inputs and outputs."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Generations": {
      "question": " What is the purpose of an FSM?\\n",
      "answer": "\nAn FSM is a finite state machine, which is a mathematical model of computation used to design both computer programs and digital logic circuits. It is an abstract machine that can be in one of a finite number of states."
    }
  },
  {
    "textbook-paragraph": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor",
    "GPT-3-Generations": {
      "question": " What are the benefits of using an FSM?\\n",
      "answer": "\nThere are several benefits of using an FSM. One benefit is that an FSM can be used to implement a wide range of logic functions. Another benefit is that an FSM can be used to construct larger memories from multiple smaller memories. Finally, an FSM can be used to reduce the number of pins needed on a chip."
    }
  },
  {
    "textbook-paragraph": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which",
    "GPT-3-Generations": {
      "question": " What are the disadvantages of using an FSM?\\n",
      "answer": "\nThere are a few disadvantages of using an FSM. First, if the FSM is not designed carefully, it can be hard to understand and debug. Second, an FSM can use more resources than a simpler design, such as a state machine. Finally, an FSM can be more difficult to change or extend than a simpler design."
    }
  },
  {
    "textbook-paragraph": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at",
    "GPT-3-Generations": {
      "question": " How do you design an FSM?\\n",
      "answer": "\nOne way to design an FSM is to start with a flow chart of the program to be implemented. This flow chart can be used to identify high-level states that can be implemented in a fixed number of clock cycles. Another way to design an FSM is to start with a list of components that are needed to implement the program. This list of components can be used to identify high-level states that can be implemented in a fixed number of clock cycles."
    }
  },
  {
    "textbook-paragraph": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more",
    "GPT-3-Generations": {
      "question": " What is the two-bit Gray code?\", '\\n\\n",
      "answer": "\nThe two-bit Gray code is a sequence of two-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit."
    }
  },
  {
    "textbook-paragraph": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango.",
    "GPT-3-Generations": {
      "question": " What should happen if the user presses the ``go'' button while in state { COUNT A}?\\n",
      "answer": "\nIf the user presses the \"go\" button while in state {COUNT A}, then the outputs are C_L=11 and C_M=01. This means that the user will get one and a half servings of lychee and a half serving of mango."
    }
  },
  {
    "textbook-paragraph": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset",
    "GPT-3-Generations": {
      "question": " What should happen if the user presses both buttons while in state { HALT A}?\", \"\\n",
      "answer": "\nThere are two possible ways that the user could press both buttons while in state { HALT A}. The first way is that the user could press the \"halt\" button and then the \"go\" button. In this case, the FSM would move to state { HALT A} and then to state { COUNT B}. The second way is that the user could press the \"go\" button and then the \"halt\" button. In this case, the FSM would move to state { COUNT B} and then to state { HALT A}."
    }
  },
  {
    "textbook-paragraph": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango.",
    "GPT-3-Generations": {
      "question": " What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?\\n",
      "answer": "\nIf the \"halt\" button is pressed (HG=10), the logic outputs C_L=00 and C_M=00, so the dispenser gives no ice cream. If both buttons are pressed (HG=11), the outputs are C_L=11 and C_M=11, so the dispenser gives two servings of each flavor, or four servings total."
    }
  },
  {
    "textbook-paragraph": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in",
    "GPT-3-Generations": {
      "question": " How does the choice of state representation affect design complexity?\\n",
      "answer": "\nThe choice of state representation affects design complexity in a few ways. First, it can affect the number of bits needed to represent the states. Second, it can affect the number of possible states that can be represented. Third, it can affect the number of transitions that need to be considered when designing the state machine."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Generations": {
      "question": " How can the use of abstraction in selecting a state representation lead to a better design?', '\\n",
      "answer": "\nAbstraction in selecting a state representation can lead to a better design by allowing the engineer to focus on the important aspects of the problem and ignoring the details that are not important. This can help to simplify the problem and make it easier to find a solution."
    }
  },
  {
    "textbook-paragraph": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in",
    "GPT-3-Generations": {
      "question": " Why is it important to use abstraction when selecting a state representation?\\n",
      "answer": "\nAbstraction is important when selecting a state representation because it allows the designer to select a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Generations": {
      "question": " How does the choice of state representation affect the design of the FSM?\\n",
      "answer": "\nDifferent state representations can lead to different FSM designs. For example, if states are represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits. If an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits."
    }
  },
  {
    "textbook-paragraph": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux",
    "GPT-3-Generations": {
      "question": " What is the value of HOLD when the counter is in the HALT A state and S_2=1?\\n",
      "answer": "\nHOLD = H + S_2\n\nWhen the counter is in the HALT A state and S_2=1, HOLD = 1. This is because the counter should stop (move into a halted state without changing value) when H is true, and in the halted states, when S_2=1, the counter stops (stays in a halted state) when H+ is true."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Generations": {
      "question": " What is the purpose of the HOLD variable?\\n",
      "answer": "\nThe HOLD variable is used to save energy. When a chip finishes its work, it can turn itself off, saving energy. This variable allows the chip to turn itself off when it is finished, saving energy."
    }
  },
  {
    "textbook-paragraph": "{Abstracting the Human Process}\n\n\nThink about how we as humans add two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation.\n\nAs you know, addition for 2's complement is identical except for the\ncalculation of overflow.\n\nWe start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.\n\nFocus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.\n\nThe diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the\n\n\n\n{{file=part2/figs/add-abs.eps,width=1.90in}}\n\n\n\n\nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.\n\nInternally, we use C_ to denote the carry input,\nand C_ to denote the carry output.\n\nSimilarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\nThe abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.\n\n{{file=part2/figs/add-integrated.eps,width=5.5in}}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is a full adder?",
      "answer": "\nA full adder is a logic device that adds three input bits and produces two output bits. The inputs are the carry bit from the previous column, A_M and B_M from the numbers A and B. The outputs are the sum bit S_M and the carry bit C^{M+1} for the next column."
    }
  },
  {
    "textbook-paragraph": "{Abstracting the Human Process}\n\n\nThink about how we as humans add two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation.\n\nAs you know, addition for 2's complement is identical except for the\ncalculation of overflow.\n\nWe start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.\n\nFocus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.\n\nThe diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the\n\n\n\n{{file=part2/figs/add-abs.eps,width=1.90in}}\n\n\n\n\nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.\n\nInternally, we use C_ to denote the carry input,\nand C_ to denote the carry output.\n\nSimilarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\nThe abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.\n\n{{file=part2/figs/add-integrated.eps,width=5.5in}}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhy is it necessary to write a 0 instead of leaving the value blank when adding two numbers in digital form?",
      "answer": "\nSince adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column. Thus, in general, we are actually adding three input bits. The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank."
    }
  },
  {
    "textbook-paragraph": "{Designing the Logic}\n\nNow we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below.\n\nTo the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}.\n\nWe suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations.\n\n{\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1\n\n\n\n{file=part2/figs/add-cin.eps,width=1in}\n{file=part2/figs/add-s.eps,width=1in}\n\n\n{eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*}\n\n}\n\nThe equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement.\n\nWe rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version.\n\n{{file=part2/figs/adder-bit.eps,width=2.55in}\n{file=part2/figs/adder-bit-nn.eps,width=2.55in}}\n\nThe gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR.\n\nLet's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors).\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway.\n\nWhen we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main reason for designing a majority function into the adder bit slice?\n\nA) To produce a carry whenever at least two out of the three input bits are 1s.\nB) To build any logic function from NAND gates.\nC) To implement a XOR gate from transistors.\nD) To provide an optimized variant of XOR.",
      "answer": "\nThe main reason for designing a majority function into the adder bit slice is to produce a carry whenever at least two out of the three input bits are 1s."
    }
  },
  {
    "textbook-paragraph": "{Adders and Word Size}\n\n\nNow that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right.\n\nThe adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder.\n\n\n{file=part2/figs/adder-trad.eps,width=1.3in}\n\n\nYou may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size.\n\n\nHaving seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right.\n\nIf we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement).\n\n\n{file=part2/figs/adder-x2.eps,width=2.15in}\n\n\nYou should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "Q: What is the purpose of an adder in a computer system?\n\nA. To add two numbers \nB. To calculate the word size \nC. To create a 2N-bit adder \nD. To design hardware that allows for the management of bits",
      "answer": "\nThe purpose of an adder in a computer system is to add two numbers. The adder is important enough to computer systems to merit its own symbol in logic diagrams, which is shown with the inputs and outputs from our design added as labels. The width of the operand input and output lines then tells you the size of the adder. Most desktop and laptop machines now have a word size of 64 bits, but many phone processors (and desktops/laptops a few years ago) use a 32-bit word size. Embedded microcontrollers may use a 16-bit or even an 8-bit word size."
    }
  },
  {
    "textbook-paragraph": "{Adders and Word Size}\n\n\nNow that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right.\n\nThe adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder.\n\n\n{file=part2/figs/adder-trad.eps,width=1.3in}\n\n\nYou may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size.\n\n\nHaving seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right.\n\nIf we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement).\n\n\n{file=part2/figs/adder-x2.eps,width=2.15in}\n\n\nYou should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the word size of most desktop and laptop machines?",
      "answer": "\nThe word size of most desktop and laptop machines is 64 bits."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 2 of the Course}\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nThe difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nWe'll start with the skills, and leave the easy stuff for the next page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.}\n\n{Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.}\n\n{Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).}\n\n{More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.}\n\n\n\nWhen designing combinational logic, we expect you to be able to apply\nthe following design strategies:\n\n{}{{}{}\n{}{}{}\n\n{Make use of human algorithms \n(for example, multiplication from addition).}\n\n{Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.}\n\n{Break truth tables into parts so as to solve each part of a function \nseparately.}\n\n{Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.}\n\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).}\n\n{Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.}\n\n{Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''}\n\n{Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.}\n\n{Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.}\n\n{Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work.\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  \n\n[t]\n{}{{}{}\n{}{}{}\n\n{Boolean functions and logic gates}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n\n\n{majority function}\n\n\n{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load}\n\n\n{design metrics}\n{-}{{}{}\n{}{}{}\n\n\n\n\n{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay}\n\n\n{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube}\n\n\n{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)}\n\n{prime implicant}\n{bit-slicing}\n{timing diagram}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain}\n\n\n{Boolean logic terms}\n{-}{{}{}\n{}{}{}\n\n{algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence}\n\n\n{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)}\n\n{sequential logic}\n{-}{{}{}\n{}{}{}\n\n{active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered}\n\n{clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating}\n\n{clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift}\n\n\n\n\n\n\n\n\n\n{   }  blank 3rd page\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a clocked synchronous sequential circuit and a sequential logic circuit?",
      "answer": "\nAnswer : A clocked synchronous sequential circuit is a digital circuit in which the output of each flip-flop is triggered by the edges (either rising or falling) of a common clock signal. In contrast, a sequential logic circuit's output can be triggered by its own input signals."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 2 of the Course}\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nThe difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nWe'll start with the skills, and leave the easy stuff for the next page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.}\n\n{Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.}\n\n{Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).}\n\n{More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.}\n\n\n\nWhen designing combinational logic, we expect you to be able to apply\nthe following design strategies:\n\n{}{{}{}\n{}{}{}\n\n{Make use of human algorithms \n(for example, multiplication from addition).}\n\n{Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.}\n\n{Break truth tables into parts so as to solve each part of a function \nseparately.}\n\n{Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.}\n\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).}\n\n{Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.}\n\n{Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''}\n\n{Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.}\n\n{Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.}\n\n{Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work.\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  \n\n[t]\n{}{{}{}\n{}{}{}\n\n{Boolean functions and logic gates}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n\n\n{majority function}\n\n\n{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load}\n\n\n{design metrics}\n{-}{{}{}\n{}{}{}\n\n\n\n\n{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay}\n\n\n{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube}\n\n\n{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)}\n\n{prime implicant}\n{bit-slicing}\n{timing diagram}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain}\n\n\n{Boolean logic terms}\n{-}{{}{}\n{}{}{}\n\n{algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence}\n\n\n{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)}\n\n{sequential logic}\n{-}{{}{}\n{}{}{}\n\n{active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered}\n\n{clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating}\n\n{clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift}\n\n\n\n\n\n\n\n\n\n{   }  blank 3rd page\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the difference between a SOP and POS form?",
      "answer": "\nThe main difference between SOP and POS forms is that SOP form is a sum of products while POS form is a product of sums. SOP form consists of AND operations followed by OR operations while in POS form, it is vice versa."
    }
  },
  {
    "textbook-paragraph": "\n\nThis set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design.\n\n { In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the definition of a register?",
      "answer": "\nA register is a digital circuit that stores a group of bits. The bits in a register can be shifted left or right, allowing the register to be used as a shift register."
    }
  },
  {
    "textbook-paragraph": "\n\nThis set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design.\n\n { In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow are registers used in digital systems?",
      "answer": "\nRegisters are used in digital systems to store groups of bits. The bits can be shifted in and out of the register, allowing for the storage of information in a shift register."
    }
  },
  {
    "textbook-paragraph": "\n\n\nA { register} is a storage element composed from one or more\nflip-flops operating on a common clock.\n\nIn addition to the flip-flops,\nmost registers include logic to control the bits stored by the register.\n\nFor example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle).\n\nTo enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right.\n\nThe LOAD input controls the clock signals through a method known as\n{ clock gating}.\n\n\n{{file=part2/figs/lec16-1a.eps,width=2.45in}}\n{file=part2/figs/lec16-1b.eps,width=3.4in}\n\n\nWhen LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value.\n\nThe problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that\n\nuse your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram.\n\n\nA better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.  \n\nWhen LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.  \n\nWhen LOAD is high, the mux selects the IN input, and the register \nloads a new value.  \n\nThe result is similar to a gated D latch with distinct write enable \nand clock lines.\n\n\n{file=part2/figs/lec16-2.eps,width=2in}\n\n\n\nWe can use this extended flip-flop as a bit slice for a multi-bit register.\n\nA four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput,\n\n\n{file=part2/figs/lec16-3.eps,width=5in}\n\n\nand the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nHow does a register store data?",
      "answer": "\nA register stores data by copying the input data to the output at the rising edge of each clock cycle. The input data is then discarded and the output is retained."
    }
  },
  {
    "textbook-paragraph": "{Shift Registers}\n\n\nCertain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this\n\n\n{file=part2/figs/lec16-4.eps,width=5in}\n\n\ntype.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook.\n\nAn example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates.\n\nReturning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary?\n\nThe shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow.\n\n{{file=part2/figs/lec16-5.eps,width=5.3in}}\n\nUsing a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design.\n\n\nThe bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices.\n\nAt each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load.\n\n\n{file=part2/figs/lec16-6.eps,width=2.3in}\n{c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low)\n\n{-4pt}\n\n{{file=part2/figs/lec16-7.eps,width=5.2in}}\n\nSeveral specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle.\n\nFinally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0.\n\n{{file=part2/figs/lec16-8.eps,width=5.2in}}\n\n\n\n",
    "GPT-3-Generations": {
      "question": "Q: What is the simplest type of shift register?\nA. A shift register that is a series of D flip-flops\nB. A shift register that is a series of muxes\nC. A shift register that has parallel load capability\nD. A shift register that can retain its value indefinitely",
      "answer": "\nAnswer : The simplest type of shift register is a series of D flip-flops."
    }
  },
  {
    "textbook-paragraph": "{Shift Registers}\n\n\nCertain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this\n\n\n{file=part2/figs/lec16-4.eps,width=5in}\n\n\ntype.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook.\n\nAn example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates.\n\nReturning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary?\n\nThe shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow.\n\n{{file=part2/figs/lec16-5.eps,width=5.3in}}\n\nUsing a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design.\n\n\nThe bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices.\n\nAt each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load.\n\n\n{file=part2/figs/lec16-6.eps,width=2.3in}\n{c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low)\n\n{-4pt}\n\n{{file=part2/figs/lec16-7.eps,width=5.2in}}\n\nSeveral specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle.\n\nFinally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0.\n\n{{file=part2/figs/lec16-8.eps,width=5.2in}}\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the difference between a logical shift and an arithmetic shift?",
      "answer": "\nThe difference between a logical shift and an arithmetic shift is that a logical shift simply moves the bits over by the specified amount, while an arithmetic shift also copies the sign bit to the vacated bits."
    }
  },
  {
    "textbook-paragraph": "{Shift Registers}\n\n\nCertain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this\n\n\n{file=part2/figs/lec16-4.eps,width=5in}\n\n\ntype.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook.\n\nAn example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates.\n\nReturning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary?\n\nThe shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow.\n\n{{file=part2/figs/lec16-5.eps,width=5.3in}}\n\nUsing a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design.\n\n\nThe bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices.\n\nAt each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load.\n\n\n{file=part2/figs/lec16-6.eps,width=2.3in}\n{c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low)\n\n{-4pt}\n\n{{file=part2/figs/lec16-7.eps,width=5.2in}}\n\nSeveral specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle.\n\nFinally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0.\n\n{{file=part2/figs/lec16-8.eps,width=5.2in}}\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy is a shift register important?",
      "answer": "\nA shift register is important as it includes logic to manipulate data held within the register. It is a type of register which serves many purposes in modern systems, from providing a fixed delay and performing bit shifts for processor arithmetic, to rate matching between components and reducing the pin count on programmable logic devices such as field programmable gate arrays (FPGAs)."
    }
  },
  {
    "textbook-paragraph": "{Boolean Properties and Don't Care Simplification}\n\nThis set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables.\n\nWe then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation.\n\nThis technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly.\n\nWe conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems.\n\n",
    "GPT-3-Generations": {
      "question": "Which of the following is not a property of Boolean logic?\nA) Commutative\nB) Associative\nC) Distributive\nD) Identity\nE) Inverse",
      "answer": "\nThe answer is E) Inverse."
    }
  },
  {
    "textbook-paragraph": "{Logic Properties}\n\nTable  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.\n\nAs mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.\n\nTo calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.\n\n{ Variables are not changed when finding the dual form.}\n\nThe dual form of a dual form is the original logic statement.\n\nBe careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.\n\n{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}\n\nDuality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).\n\nThe rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.\n\nSecond, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.  \n\nA function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.\n\nHowever, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').\n\nFinally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.\n\n",
    "GPT-3-Generations": {
      "question": "\n\n1. Which of the following is not a property of Boolean logic?\nA) Commutative Property\nB) Associative Property\nC) Distributive Property\nD) Identity Property\nE) Negation Property\nF) Inverse Property\nG) Transitive Property",
      "answer": " \n\nThe answer is C) Distributive Property."
    }
  },
  {
    "textbook-paragraph": "{Choosing the Best Function}\n\nWhen we specify how something works using a human language, we\nleave out details.  Sometimes we do so deliberately, assuming that a\nreader or listener can provide the details themselves: ``Take me to the \nairport!''\nrather than ``Please bend your right arm at the elbow and shift your\nright upper arm forward so as to place your hand near the ignition key.\nNext, ...''  \n\n\n{\n{|lll|}\n1+A=1& 0=0&\n1=A& 0+A=A&\nA+A=A& A=A&\nA=0& A+=1&\n{A+B}= &\n=+& DeMorgan's laws\n(A+B)C=AC+BC&\nA B+C=(A+C)(B+C)&distribution\n(A+B)(+C)(B+C)=(A+B)(+C)&\nA B+ C+B C=A B+ C& consensus\n\n}\n{Boolean logic properties.  The two columns are dual forms of\none another.}\n\n\n\nYou know the basic technique for implementing a Boolean function\nusing { combinational logic}: use a {K-map} to identify a\nreasonable SOP or POS form, draw the resulting design, and perhaps\nconvert to NAND/NOR gates.\n\n\n\nWhen we develop combinational logic designs, we may also choose to\nleave some aspects unspecified.  In particular, the value of a\nBoolean logic function to be implemented may not matter for some\ninput combinations.  If we express the function as a truth table,\nwe may choose to mark the function's value for some input combinations \nas ``{ don't care},'' which is written as ``x'' (no quotes).\n\nWhat is the benefit of using ``don't care'' values?  \n\nUsing ``don't care'' values allows you to choose from among several\npossible logic functions, all of which produce the desired results\n(as well as some combination of 0s and 1s in place of the ``don't\ncare'' values).  \n\nEach input combination marked as ``don't care'' doubles the number\nof functions that can be chosen to implement the design, often enabling \nthe logic needed for implementation to be simpler.\n\n\nFor example, the {K-map} to the right specifies a function F(A,B,C)\nwith two ``don't care'' entries.  \n\nIf you are asked to design combinational logic for this function,\nyou can\nchoose any values for the two ``don't care'' entries.  When identifying\nprime implicants, each ``x'' can either be a 0 or a 1.\n\n\n\n\n\n\nDepending on the choices made for the x's, we obtain one of \nthe following four functions:\n\n{eqnarray*}\nF&=& B+B C\nF&=& B+B C+A  \nF&=&B\nF&=&B+A \n{eqnarray*}\n\n\n\n\n\nGiven this set of choices, a designer typically chooses the third: F=B,\nwhich corresponds to the {K-map} shown to the right of the \nequations.  The design then \nproduces F=1 when A=1, B=1, and C=0 (ABC=110), \nand produces F=0 when A=1, B=0, and C=0 (ABC=100).\nThese differences are marked with shading and green italics in the new\n{K-map}.  No implementation ever produces an ``x.''\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the benefit of using ``don't care'' values?\n\nA) It allows you to choose from among several possible logic functions, all of which produce the desired results.\nB) Using ``don't care'' values allows you to choose from among several possible logic functions, all of which produce the desired results as well as some combination of 0s and 1s in place of the ``don't care'' values.\nC) Each input combination marked as ``don't care'' doubles the number of functions that can be chosen to implement the design, often enabling the logic needed for implementation to be simpler.\nD) It allows you to choose any values for the two ``don't care'' entries.",
      "answer": "\nC) Each input combination marked as ``don't care'' doubles the number of functions that can be chosen to implement the design, often enabling the logic needed for implementation to be simpler."
    }
  }
]