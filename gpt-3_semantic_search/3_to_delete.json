[
  {
    "textbook-paragraph": "Before we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\n1. {-8pt}\n2. {-8pt}\n3. {-8pt}\n4. {-8pt}\n5. {-8pt}\n6. \n\nWhat is the general approach that is taken when designing an FSM? \nA) We follow a six step process that includes real world curiosity and factual reasoning. \nB) We take a general approach that includes reasoning and procedural steps to explore designs. \nC) We follow a six step process that includes precise and factual questions about the context. \nD) We take a general approach that includes all the possibilities of the context to explore designs.",
      "answer": "\nB) We take a general approach that includes reasoning and procedural steps to explore designs."
    }
  },
  {
    "textbook-paragraph": "Step {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the most likely reason for incompatibilities when composing two or more separately designed pieces into an integrated system?\n\nA) Lack of standardization in design\nB) Incompatibility in the digital systems being connected\nC) Poor planning in the development process\nD) None of the above",
      "answer": "\nB) Incompatibility in the digital systems being connected"
    }
  },
  {
    "textbook-paragraph": "In Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand.",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is not a step in the process of encoding the state of an FSM?\nA) Select an internal representation for the bits necessary to encode the state of our FSM\nB) In practice, for small designs, this representation can be selected by a computer \nC) For large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.\nD) Optimize the implementation",
      "answer": "\nC) For large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand."
    }
  },
  {
    "textbook-paragraph": "In Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand.",
    "GPT-3-Generations": {
      "question": "\n\nHow does one select an internal representation for the bits necessary to encode the state of our FSM?",
      "answer": "\nThe internal representation for the bits necessary to encode the state of our FSM can be selected by a computer in such a way as to optimize the implementation."
    }
  },
  {
    "textbook-paragraph": "In the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly.",
    "GPT-3-Generations": {
      "question": "\n\nWhy is it important to have a small design when selecting the representation?",
      "answer": "\nA small design is easier to understand and work with when selecting the representation. It is also simpler to build correctly."
    }
  },
  {
    "textbook-paragraph": "We also show how one can\nuse abstraction to simplify an implementation.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the purpose of abstraction in software development?",
      "answer": "\nAbstraction is the process of hiding the details of an implementation in order to simplify the use of that implementation. \n\nIn software development, abstraction is used to hide the details of an implementation in order to make it easier to use. For example, when you use a library or an API, you don't need to know how it works internally; you just need to know how to use it."
    }
  },
  {
    "textbook-paragraph": "By Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the next step in the process?\nA) Deriving logic expressions for the next-state variables and output signals.\nB) Familiarizing yourself with the process.\nC) Completing the specification in terms of bits.\nD) None of the above.",
      "answer": "\n\nThe next step in the process is to derive logic expressions for the next-state variables and output signals."
    }
  },
  {
    "textbook-paragraph": "By Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the process for deriving logic expressions for the next-state variables and output signals?",
      "answer": "\nThe process for deriving logic expressions for the next-state variables and output signals is to first determine the desired output, then to determine what inputs are needed to generate that output. Next, the inputs are combined using Boolean algebra to create logic expressions for the next-state variables and output signals."
    }
  },
  {
    "textbook-paragraph": "By Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the design process differ for combinational logic versus sequential logic?",
      "answer": "\nThe design process for sequential logic is similar to that of combinational logic, except that it takes into account the order in which the various steps must be executed. In addition, sequential logic designs often require the use of flip-flops or other storage elements to remember the state of the system from one clock cycle to the next."
    }
  },
  {
    "textbook-paragraph": "Finally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the final step in building a finite state machine?\nA) Translating logic expressions into gates\nB) Using flip-flops to hold internal state bits\nC) Implementing more complex building blocks \nD) Creating abstractions to simplify the design process",
      "answer": "\nB) Using flip-flops to hold internal state bits"
    }
  },
  {
    "textbook-paragraph": "Finally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the final step in implementing an FSM?",
      "answer": "\nThe final step in implementing an FSM is to translate the logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM."
    }
  },
  {
    "textbook-paragraph": "Finally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic.",
    "GPT-3-Generations": {
      "question": "\n\nHow does translating our logic expressions into gates help us build up abstractions to simplify the design process?",
      "answer": "\nTranslating our logic expressions into gates helps us build up abstractions to simplify the design process by allowing us to use more complex building blocks when implementing an FSM. By using more complex building blocks, we can abstract away some of the details of the implementation, making the design process simpler."
    }
  },
  {
    "textbook-paragraph": "Let's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.",
    "GPT-3-Generations": {
      "question": "\n\nWhy is a Gray code a cycle over all bit patterns of a certain length?",
      "answer": "\nA Gray code is a cycle over all bit patterns of a certain length because it contains all possible combinations of bits for that length. The reason for this is that each pattern in a Gray code differs from the previous pattern by only one bit. This means that all possible combinations of bits are represented in the Gray code."
    }
  },
  {
    "textbook-paragraph": "For simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the output value of the FSM if the current state is s2 and the input is 1?\n\nA) The output value is 0.\nB) The output value is 1.\nC) The output value is 2.\nD) The output value is 3.",
      "answer": "\nThe output value is 1."
    }
  },
  {
    "textbook-paragraph": "The inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the input to a counter?\nA) A function to start and stop the counter\nB) A function to control the counting direction\nC) A function to reset the counter to a particular state\nD) All of the above",
      "answer": "\nThe inputs to a counter are typically limited to functions such as starting and stopping the counter, controlling the counting direction, and resetting the counter to a particular state."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle.",
    "GPT-3-Generations": {
      "question": "\n\n\nWhat is the output of this two-bit Gray code counter when it is reset?\n\nA. 01\nB. 10\nC. 11\nD. 00",
      "answer": "\nThe output of this two-bit Gray code counter when it is reset is 00."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is a two-bit Gray code counter?",
      "answer": "\n\nA two-bit Gray code counter is a digital circuit that counts in binary, with a twist. In a normal binary counter, each time the count reaches its maximum value (in this case, 2), it wraps around to 0 and starts counting again. With a Gray code counter, each time the count reaches its maximum value, it wraps around to the next lowest value instead of starting back at 0."
    }
  },
  {
    "textbook-paragraph": "Each state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter.",
    "GPT-3-Generations": {
      "question": "\n\nWhich state diagram correctly shows the sequence of states for a 3-bit up-down counter with an enable input E?\n\nA)\nB)\nC)\nD)\n\n                                                                                           \nA)",
      "answer": "\nThe correct state diagram for a 3-bit up-down counter with an enable input E is shown in Diagram C."
    }
  },
  {
    "textbook-paragraph": "Each state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the output value Z_1Z_0 for the state S_1S_0 = 11?",
      "answer": "\nThe output value Z_1Z_0 for the state S_1S_0 = 11 is 10."
    }
  },
  {
    "textbook-paragraph": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}.",
    "GPT-3-Generations": {
      "question": "\n\nBased on the transition diagram and the K-maps for the next-state values S_1^+ and S_0^+, what will be the value of S_0 when S_1 is 1?\n\nA) 0\nB) 1\nC) 2\nD) 3",
      "answer": "\nThe value of S_0 when S_1 is 1 will be 0."
    }
  },
  {
    "textbook-paragraph": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the algebraic expression for S_1^+?",
      "answer": "\nThe algebraic expression for S_1^+ is S_0."
    }
  },
  {
    "textbook-paragraph": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}.",
    "GPT-3-Generations": {
      "question": "\n\nWhy does the next state value S_1^+ equal the current state value S_0?",
      "answer": "\nThe next state value S_1^+ equals the current state value S_0 because, according to the transition diagram, the only way to get to state 1 is from state 0."
    }
  },
  {
    "textbook-paragraph": "Now we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.",
    "GPT-3-Generations": {
      "question": "\n\nHow does using a Gray code as the basis for the state sequence add a third bit to our counter?",
      "answer": "\n\nThe Gray code is a sequence of numbers where each successive number has only one bit different from the previous number. This means that it is easy to add a third bit to our counter by using a Gray code as the basis for the state sequence."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle.",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is a correct statement about the counter in the given context?\n\nA) The counter will move from one state to the next each cycle.\nB) With no inputs, the states will form a loop.\nC) The counter is fully specified by the transition diagram.\nD) The transition diagram appears to the right.",
      "answer": "\nThe counter is fully specified by the transition diagram."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the purpose of a fully-specified transition diagram?",
      "answer": "\n\nA fully-specified transition diagram is used to show the behavior of a system or device over time. In this case, it shows how a counter will behave when it receives no input."
    }
  },
  {
    "textbook-paragraph": "Each state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/'').",
    "GPT-3-Generations": {
      "question": " The current state is \nassumed to be S_2S_1S_0 = 000. The input X is 1 for all time.\n\nWhat will be the output when the state value is S_2S_1S_0 = 001?\n\nA) Z_2Z_1Z_0 = 001\nB) Z_2Z_1Z_0 = 010\nC) Z_2Z_1Z-00 = 100 \nD) Z-200Z-100Z-000",
      "answer": "\nThe output when the state value is S_2S_1S_0 = 001 will be Z_2Z_1Z_0 = 010."
    }
  },
  {
    "textbook-paragraph": "Each state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/'').",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the internal state value and output for state \"q2\"?",
      "answer": "\nThe internal state value and output for state \"q2\" is S_2S_1S_0/Z_2Z_1Z_0."
    }
  },
  {
    "textbook-paragraph": "Each state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/'').",
    "GPT-3-Generations": {
      "question": "\n\nHow does the state transition diagram work?",
      "answer": "\nThe state transition diagram works by mapping the current state to the next state based on the input. The output is generated based on the current state."
    }
  },
  {
    "textbook-paragraph": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is the next state of S2 based on the transition diagram?\nA. 0\nB. 1\nC. 2\nD. 3",
      "answer": "\n1. What is the next state of S2 based on the transition diagram?\n\nThe next state of S2 is 1."
    }
  },
  {
    "textbook-paragraph": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the next-state value for S_2^+?",
      "answer": "\nThe next-state value for S_2^+ is 1 when both current inputs are 1, and 0 otherwise."
    }
  },
  {
    "textbook-paragraph": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime.",
    "GPT-3-Generations": {
      "question": "\n\nHow do we fill in the K-maps for the next-state values S_2^+, S_1^+, and S_0^+?",
      "answer": "\nThe K-maps for the next-state values S_2^+, S_1^+, and S_0^+ can be filled in by looking at the transition diagram and determining which inputs will result in a '1' output for each of the next-state values. For example, for S_2^+, we can see that when A is '1' and B is '0', the output will be '1'. Therefore, we would fill in a '1' in the K-map for S_2^+ at the A=1, B=0 position. We can continue this process until all of the next-state values have been filled in."
    }
  },
  {
    "textbook-paragraph": "For our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*}",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the output of the state machine for the input sequence 1101?\n\nA) 0110\nB) 1001\nC) 0011\nD) 0100",
      "answer": "\nThe output of the state machine for the input sequence 1101 is 1001."
    }
  },
  {
    "textbook-paragraph": "For our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*}",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the next state logic for S2, S1, and S0?",
      "answer": "\nThe next state logic for S2, S1, and S0 is as follows:\n\nS2: S2S0 + S1S0\nS1: S2S0 + S1S0\nS0: S2S1 + S2S1"
    }
  },
  {
    "textbook-paragraph": "For our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*}",
    "GPT-3-Generations": {
      "question": "\n\nHow do we obtain the next-state logic?",
      "answer": "\nThe next-state logic is obtained by multiplying the current state matrix by the transition matrix."
    }
  },
  {
    "textbook-paragraph": "Notice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}.",
    "GPT-3-Generations": {
      "question": "\n\nThis is because:\n\nA) S_2^+ is the conjugate acid of S_1^-\nB) S_1^+ is the conjugate acid of S_0\nC) S_1^+ is the conjugate base of S_2^- \nD) All of the above.\n\nD) All of the above.",
      "answer": "\nThe reason that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}, is because S_2^+ is the conjugate acid of S_1^- and S_1^+ is the conjugate acid of S_0."
    }
  },
  {
    "textbook-paragraph": "Notice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the term in the equation for S_2^+ that is also in the equation for S_1^+?",
      "answer": "\nThe term in the equation for S_2^+ that is also in the equation for S_1^+ is S_1{{S_0}}."
    }
  },
  {
    "textbook-paragraph": "Notice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the presence of a common term in two equations affect the way we solve for those equations?",
      "answer": "\nThe presence of a common term in two equations does not affect the way we solve for those equations. We can still solve for the equations by using the same methods as before."
    }
  },
  {
    "textbook-paragraph": "Looking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the order of the vertical lines in the figure?",
      "answer": "\nThe vertical lines are ordered from left to right as S_0S_0S_1S_1S_2S_2."
    }
  },
  {
    "textbook-paragraph": "Early graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right.",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is not a color that can be represented by this system?\nA. Black\nB. Green\nC. Yellow\nD. Blue\nE. Magenta\nF. White\n\nWhich of the following is not a color that can be represented by this system?\nA. Black  B. Green  C. Yellow   D. Blue   E. Magenta   F. White",
      "answer": "\nThe color mapping for this system does not include black, yellow, blue, or magenta."
    }
  },
  {
    "textbook-paragraph": "Early graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the color mapping for a three-bit RGB encoding system?",
      "answer": "\nThe color mapping for a three-bit RGB encoding system is as follows:\n\nRed: 000 -> 255\nGreen: 001 -> 191\nBlue: 010 -> 63\nCyan: 011 -> 31\nMagenta: 100 -> 159\nYellow: 101 -> 223\nWhite: 110 -> 239"
    }
  },
  {
    "textbook-paragraph": "Early graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the three-bit RGB encoding system work?",
      "answer": "\nThe three-bit RGB encoding system uses a three-bit code to represent the red, green, and blue components of a color. Each color is represented by a different combination of bits. For example, the color red can be represented by the code 000, green by001, and blue by 010."
    }
  },
  {
    "textbook-paragraph": "Imagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is",
    "GPT-3-Generations": {
      "question": " to have the light turn on at a\nlow intensity for each color, then ramp up to full intensity over the\ncourse of five seconds, and then ramp back down to low intensity and off\nover the course of five seconds.  After each color has been displayed in\nthis manner, the next color in the sequence should be displayed.\n\nWhich of the following is not a necessary component of the circuit?\nA) A 555 Timer IC  B) A Resistor  C) An Inductor  D) A Capacitor  \n\nC) An Inductor",
      "answer": "\nAn inductor is not a necessary component of the circuit."
    }
  },
  {
    "textbook-paragraph": "Imagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is",
    "GPT-3-Generations": {
      "question": " for the light to transition from\nred, to green, and then to blue.\n\nWhat is the RGB input for the light?",
      "answer": "\nThe RGB input for the light is 255, 0, 0 for red, 0, 255, 0 for green, and 0, 0, 255 for blue."
    }
  },
  {
    "textbook-paragraph": "Imagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is",
    "GPT-3-Generations": {
      "question": " to have the light change to each\nof the colors in the sequence, one at a time, and then repeat.\n\nHow would you create a counter to drive a light through a sequence of colors?",
      "answer": "\nTo create a counter to drive a light through a sequence of colors, I would use an RGB input. I would set the colors in the sequence as Red, Green, and Blue. Then, I would use a for loop to repeat the sequence."
    }
  },
  {
    "textbook-paragraph": "{off (black)     yellow     violet     green     blue}",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the order of colors in a rainbow?\n\nA) off (black), yellow, violet, green, blue\nB) yellow, green, blue, violet, off (black)\nC) green, blue, violet, off (black), yellow \nD) violet, blue, green , yellow ,off (black)",
      "answer": "\nThe order of colors in a rainbow is: violet, blue, green , yellow ,off (black)."
    }
  },
  {
    "textbook-paragraph": "{off (black)     yellow     violet     green     blue}",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the order of the colors in the spectrum of visible light?",
      "answer": "\nThe order of the colors in the spectrum of visible light is violet, blue, green, yellow and red."
    }
  },
  {
    "textbook-paragraph": "{off (black)     yellow     violet     green     blue}",
    "GPT-3-Generations": {
      "question": "\n\nWhy are there five colors in a traffic light?",
      "answer": "\nThe traffic light is a universal symbol that is used to indicate when it is safe to cross the street. The five colors in a traffic light represent the following: red, yellow, green, blue, and violet. Each color has a specific meaning: \n\nRed: Stop \nYellow: Caution \nGreen: Go \nBlue: Proceed with caution \nViolet: Do not enter"
    }
  },
  {
    "textbook-paragraph": "You immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as",
    "GPT-3-Generations": {
      "question": " outputs are black,\nwhite, and gray,\n\nHow many flip-flops are needed to create a counter with 5 states?",
      "answer": "\nAt least three flip-flops are needed to create a counter with 5 states."
    }
  },
  {
    "textbook-paragraph": "You immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as",
    "GPT-3-Generations": {
      "question": " output are green,\namber, and red, we can use a 2-input AND gate to control the red\nlight (see Figure 1).\n\nHow many flip-flops will be needed in order to create a counter with five states?",
      "answer": "\nAt least three flip-flops will be needed in order to create a counter with five states."
    }
  },
  {
    "textbook-paragraph": "{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the color represented by the binary number \"111\"?\n\na) black\nb) blue\nc) green \nd) cyan \ne) red \nf) violet \ng) yellow \nh) white",
      "answer": "\nThe color represented by the binary number \"111\" is white."
    }
  },
  {
    "textbook-paragraph": "{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the RGB color for black?",
      "answer": "\nThe RGB color for black is 000."
    }
  },
  {
    "textbook-paragraph": "{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white",
    "GPT-3-Generations": {
      "question": "\n\n\nHow does the RGB color model work?",
      "answer": "\nThe RGB color model is an additive color model in which red, green, and blue light are added together in various ways to reproduce a broad array of colors. The name of the model comes from the initials of the three additive primary colors, red, green, and blue."
    }
  },
  {
    "textbook-paragraph": "outputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values.",
    "GPT-3-Generations": {
      "question": "\n\n1) What is the best way to generate outputs that are all unique bit patterns? \nA) Use a counter's internal state directly as our output values\nB) Use a random number generator\nC) Use a pre-determined sequence of values\nD) Use an algorithm that generates all possible bit patterns",
      "answer": "\nA) Use a counter's internal state directly as our output values"
    }
  },
  {
    "textbook-paragraph": "outputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the output of a bit pattern?",
      "answer": "\nThe output of a bit pattern is the unique sequence of bits that it produces."
    }
  },
  {
    "textbook-paragraph": "outputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values.",
    "GPT-3-Generations": {
      "question": "\n\nWhy are outputs all unique bit patterns?",
      "answer": "\nThe outputs of a counter are all unique bit patterns because the output is determined by the number of clock pulses applied to the input. The number of clock pulses is directly related to the binary value of the output."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the output of the color sequencer when it is in state S_000?\nA) RGB=000\nB) RGB=001\nC) RGB=010\nD) RGB=011",
      "answer": "\nThe output of the color sequencer when it is in state S_000 is RGB=000."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the output RGB for state S_2S_1S_0?",
      "answer": "\nThe output RGB for state S_2S_1S_0 is R:255, G:191, B:63."
    }
  },
  {
    "textbook-paragraph": "{Extending Keyless Entry with a Timeout}\n\nThis set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the goal of this extension?\n\nA. To make the alarm time out, turning itself off after a fixed amount of time.\nB. To fully expand the design to explicitly illustrate all possible states.\nC. To use components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possible states.\nD. None of the above",
      "answer": "\nC. To use components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possible states."
    }
  },
  {
    "textbook-paragraph": "{Extending Keyless Entry with a Timeout}\n\nThis set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the goal of this extension?",
      "answer": "\nThe goal of this extension is to illustrate how we can make use of components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possible states."
    }
  },
  {
    "textbook-paragraph": "{Extending Keyless Entry with a Timeout}\n\nThis set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nHow does this extension work?",
      "answer": "\nThe extension to the keyless entry control FSM works by using a counter to make the alarm time out. The goal of this extension is to illustrate how we can make use of components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possible states."
    }
  },
  {
    "textbook-paragraph": "{Physical Design, Sensors, and Timing}\n\n\nTo begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right.\n\nThe four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on.\n\nTransition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton.\n\nIn this design, once a user presses the panic button P, the alarm\nsounds until the user presses the\n\n\n{file=part3/figs/ke-trans-diag-brief.eps,width=4.2in}\n\n\nlock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states?\n\nInstead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter.\n\nLet's say that we want our timeout to be T cycles.\n\nWhen we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs.\n\nTo load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down. \n\nThe counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm.\n\nYou should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit.\n\nHow many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value.\n\n\nWe expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value).\n\nWe need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer.\n\nThe only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm.\n\nFinally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED.\n\nNow that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero.\n\n\n{file=part3/figs/ke-alarm-expansion.eps,width=1.75in}\n\n\nThe first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input.\n\nThe second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1).\n\nThe last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur? \n\nFirst, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop.\n\nThe extension thus requires only a counter, a mux, and a gate, as shown below.\n\n{{file=part3/figs/ke-alarm-exp-impl.eps,width=2.65in}}\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "Q: What is the output of the system when the user presses the panic button P while in any of the ALARM(t) states?\n\nA. The system returns to ALARM(1).",
      "answer": "\nThe system returns to the ALARM(1) state when the user presses the panic button P while in any of the ALARM(t) states."
    }
  },
  {
    "textbook-paragraph": "{Physical Design, Sensors, and Timing}\n\n\nTo begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right.\n\nThe four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on.\n\nTransition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton.\n\nIn this design, once a user presses the panic button P, the alarm\nsounds until the user presses the\n\n\n{file=part3/figs/ke-trans-diag-brief.eps,width=4.2in}\n\n\nlock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states?\n\nInstead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter.\n\nLet's say that we want our timeout to be T cycles.\n\nWhen we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs.\n\nTo load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down. \n\nThe counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm.\n\nYou should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit.\n\nHow many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value.\n\n\nWe expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value).\n\nWe need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer.\n\nThe only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm.\n\nFinally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED.\n\nNow that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero.\n\n\n{file=part3/figs/ke-alarm-expansion.eps,width=1.75in}\n\n\nThe first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input.\n\nThe second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1).\n\nThe last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur? \n\nFirst, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop.\n\nThe extension thus requires only a counter, a mux, and a gate, as shown below.\n\n{{file=part3/figs/ke-alarm-exp-impl.eps,width=2.65in}}\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy did we choose to expand the ALARM state into T separate states based on the value of the counter?",
      "answer": " \n \n \n\n\n\n\n\n\nAnswer : We chose to expand the ALARM state into T separate states based on the value of the counter because we wanted to use the counter value to keep track of time. By doing this, we can easily determine when a timeout has occurred and move back to the LOCKED state."
    }
  },
  {
    "textbook-paragraph": "{Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between a bit-sliced design and a serial design?",
      "answer": "\nAnswer : The main difference between a bit-sliced design and a serial design is that in a bit-sliced design, each bit slice's output is fed back into the bit slice in the next cycle, whereas in a serial design, the outputs of all the bit slices are stored in flip-flops and then fed back into the respective bit slices in the next cycle."
    }
  },
  {
    "textbook-paragraph": "{Finite State Machines}\n\nA { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}.\n\nAn FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.  \n\nWhen an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits.\n\nFor a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable).\n\nAnd, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions.\n\nIn this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops.\n\nIn this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems.\n\nThe table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly.\n\nBy including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class.\n\n\n\n{\n\nmeaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes\n\n}\n\n\n\nAnother tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm. \n\n\n\n{\n\nstate& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM\n\n}\n\n\n\n\nA { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another.\n\nPutting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered.\n\nImplementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks.\n\n\n{file=part3/figs/ke-abs-trans-diag.eps,width=3.45in}\n\n\nFor now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning. \n\n{\n\noutputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed\n\n}\n\nWe can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example).\n\n{\n\n& & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1\n\n}\n\n\n\nWe can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0). \n\n\n{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11 \n\n}\n\n\nIn the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples.\n\n\n{file=part3/figs/ke-trans-diag.eps,width=4.2in}\n\n\nWe have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework.\n\n\n",
    "GPT-3-Generations": {
      "question": "Why is it important for transition rules to be complete when designing a digital FSM?",
      "answer": " \n \n \n It is important for transition rules to be complete when designing a digital FSM because any implementation will produce bits, not x's."
    }
  },
  {
    "textbook-paragraph": "{Synchronous Counters}\n\nA { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle.\n\nNot all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.  \n\nExcept for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters.\n\n {The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state, \n\n\nThe design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns. \n\nThe task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle. \n\nThe cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n{\n\n{ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0\n\n\n\n{file=part3/figs/sbin3-s2.eps,width=1in}\n{file=part3/figs/sbin3-s1.eps,width=1in}\n{file=part3/figs/sbin3-s0.eps,width=1in} {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*}\n\n}\n\nThe first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious.\n\nWe can also derive the pattern intuitively by asking the following:\n\ngiven a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left.\n\n{{file=part3/figs/ser-gating.eps,width=4.5in}}\n\nThe calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches.\n\n{{file=part3/figs/par-gating.eps,width=4.5in}}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nQ: What is the main difference between serial gating and parallel gating in terms of design?\n\nA. Serial gating implies that an N-bit counter requires more than N-2 gate delays to settle into the next state, while parallel gating calculates each input independently with a single logic gate.",
      "answer": "\nSerial gating implies that an N-bit counter requires more than N-2 gate delays to settle into the next state, while parallel gating calculates each input independently with a single logic gate."
    }
  },
  {
    "textbook-paragraph": "{Synchronous Counters}\n\nA { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle.\n\nNot all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.  \n\nExcept for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters.\n\n {The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state, \n\n\nThe design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns. \n\nThe task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle. \n\nThe cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n{\n\n{ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0\n\n\n\n{file=part3/figs/sbin3-s2.eps,width=1in}\n{file=part3/figs/sbin3-s1.eps,width=1in}\n{file=part3/figs/sbin3-s0.eps,width=1in} {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*}\n\n}\n\nThe first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious.\n\nWe can also derive the pattern intuitively by asking the following:\n\ngiven a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left.\n\n{{file=part3/figs/ser-gating.eps,width=4.5in}}\n\nThe calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches.\n\n{{file=part3/figs/par-gating.eps,width=4.5in}}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the design of synchronous counter circuits change if the internal state bits are allowed to differ from the output bits?",
      "answer": "\nIf the internal state bits are allowed to differ from the output bits, then the design of synchronous counter circuits becomes more complex. This is because the internal state bits can no longer be directly mapped to output patterns. As a result, the designer needs to carefully consider how the internal state bits will transition between states in order to produce the desired output pattern."
    }
  },
  {
    "textbook-paragraph": "{Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy is a second class of counter called a ripple counter?",
      "answer": "\nA second class of counter is called a ripple counter because it uses a clock signal to drive some of its flip-flops and feeds the outputs of these flip-flops into the clock inputs of remaining flip-flops. This causes the effect of a clock edge to ripple through the flip-flops, hence the name."
    }
  },
  {
    "textbook-paragraph": "{Timing Issues*}\n\nRipple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.\n\nMore aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.\n\nIf you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.\n\n\n",
    "GPT-3-Generations": {
      "question": "\n1. Why is clock gating an attractive strategy?\nA)It uses less power\nB)It is easier to design\nC)It is faster\nD)It requires less time to transition between the on and off states",
      "answer": "\nA) Clock gating is an attractive strategy because it uses less power. By hiding the clock signal from flip-flops, the amount of leakage current is reduced. \nB) Clock gating is an attractive strategy because it is easier to design. CAD tools can automatically insert clock gating logic, which simplifies the design process. \nC) Clock gating is an attractive strategy because it is faster. Flip-flops transition more quickly between the on and off states when the clock signal is hidden. \nD) Clock gating is an attractive strategy because it requires less time to transition between the on and off states. When the clock signal is hidden, flip-flops can transition more quickly between states."
    }
  },
  {
    "textbook-paragraph": "{Machine Models}\n\nBefore we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.\n\nHistorically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.\n\nHowever, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.\n\nWhat is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.\n\nAs we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.\n\nThe disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.\n\nIn practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.\n\nThe coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.\n\nBy adding more states to the FSM, we were able to hold the servo in\nplace, as desired.\n\nWhy are we protecting you from the model used in practice?\n\nFirst, timing issues add complexity to a topic that is complex enough \nfor an introductory course.\n\nAnd, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.\n\nIn many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.\n\nWe now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.\n\nAs already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.  \n\nA Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.\n\nThe machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.\n\nNotice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.\n\nNotice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.\n\n\n\n{{file=part3/figs/lec17-3.eps,width=5in}}\n\n\n\nFor a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.\n\n\n\n{{file=part3/figs/lec17-4.eps,width=5in}}\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy does the Mealy machine require fewer states than the Moore machine?",
      "answer": "\nThe Mealy machine requires fewer states than the Moore machine because it can express outputs as functions of both internal state and FSM inputs. This means that the number of internal state bits can be reduced."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 3 of the Course}\n\n\nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nStudents often find this part of the course more challenging than the\nearlier parts of the course.\n\nIn addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.  \n\nStudents typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).  \n\nWe'll start with the easy stuff.  \n\nYou should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy\n\n}\n\n{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)\n\n}\n\n{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram\n\n}\n\n{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers\n\n}\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size\n\n}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM\n\n}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)\n\n}\n{processor datapath}\n\n{control signal}\n\n}\n\n{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux\n\n}\n\n{instruction processing}\n{-}{{}{}\n{}{}{}\n\n\n\n{register transfer language (RTL)}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}\n\n{data movement}\n{control flow}\n\n{addressing modes}\n{-}{{}{}\n{}{}{}\n\n\n{PC-relative}\n\n{base + offset}\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n\nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}\n\n\n\n\n\n{   }  empty 3rd page\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the difference between the Moore and Mealy machine models?",
      "answer": "\nThe Moore and Mealy machine models are two ways of designing finite state machines (FSMs). In a Moore machine, the outputs depend only on the current state. In a Mealy machine, the outputs also depend on the current inputs."
    }
  },
  {
    "textbook-paragraph": "{Addressing Architectures*}\n\nOne question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D.\n\nA binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt}\n\nIf all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt}\n\nThe assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long.\n\nAt the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture). \n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nEight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.  \n\nArchitectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt}\n\nThe second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed.\n\n\n\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nThe assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture.\n\nSeveral ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below.\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt}\n\nAccumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt}\n\nThe last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators.\n\nA { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation):\n\n{-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt}\n\nThe resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture:\n\n{-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt}\n\nThe values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the name of the addressing architecture that uses a special-purpose register called the accumulator?\n\nAccumulator architectures use a special-purpose register called the accumulator.",
      "answer": ""
    }
  },
  {
    "textbook-paragraph": "{Procedure and System Calls*}\n\nA { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs.\n\nFor our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call.\n\n{-6pt}\n\n=DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1\n\n>DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN\n\n{-6pt}\n\nThe procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again.\n\nAs you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below.\n\n\n{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*}\n\n\n{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*}\n\n\nWhile an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed.\n\nThe term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it.\n\nCalling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee).\n\n\n\n\n\nA typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables.\n\nAs an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure.\n\n\n{file=part4/figs/lec23-2.eps,width=1.25in}\n\n\n[t]\n{\n[t]\n\nint =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n}\n\nprintf (``d'', add3 (10, 20, 30));\n\n\n\nby convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6\n\n\n\n[t]\n\nadd3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1\n\n\n\nThe add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation.\n\n{ System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\n\nWhat is a procedure?\n\nA) A sequence of instructions that executes a particular task. \nB) A concept of programming that appears in every high-level language \nC) A standard for deciding what information is passed between a caller and a callee.\nD) All of the above.",
      "answer": "   \n  \n \n\n \n\n \n\n \n\n \nThe answer is A) A sequence of instructions that executes a particular task."
    }
  },
  {
    "textbook-paragraph": "{Procedure and System Calls*}\n\nA { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs.\n\nFor our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call.\n\n{-6pt}\n\n=DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1\n\n>DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN\n\n{-6pt}\n\nThe procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again.\n\nAs you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below.\n\n\n{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*}\n\n\n{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*}\n\n\nWhile an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed.\n\nThe term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it.\n\nCalling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee).\n\n\n\n\n\nA typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables.\n\nAs an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure.\n\n\n{file=part4/figs/lec23-2.eps,width=1.25in}\n\n\n[t]\n{\n[t]\n\nint =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n}\n\nprintf (``d'', add3 (10, 20, 30));\n\n\n\nby convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6\n\n\n\n[t]\n\nadd3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1\n\n\n\nThe add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation.\n\n{ System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\n\nWhat is a procedure?",
      "answer": "\nA procedure is a sequence of instructions that executes a particular task. Procedures are used as building blocks for multiple, larger tasks. The concept of a procedure is fundamental to programming, and appears in some form in every high-level language as well as in most ISAs."
    }
  },
  {
    "textbook-paragraph": "{Control Flow Conditions*}\n\nControl flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.  \n\nUnconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.\n\nMany ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.\n\n\n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}\n\nThe status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}\n\nFinally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}\n\nThe three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the three approaches to implementing status bits?\nA) keeping them in general-purpose registers\nB) storing them in special-purpose registers\nC) calculating them within a single instruction",
      "answer": "\nThe three approaches to implementing status bits are keeping them in general-purpose registers, storing them in special-purpose registers and calculating them within a single instruction."
    }
  },
  {
    "textbook-paragraph": "{Stack Operations*}\n\nTwo types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage.\n\nStack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.  \n\nThe third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the third use of stack operations?\n\nA. To support procedure calls\nB. To provide temporary storage during interrupts\nC. To support spill code generated by compilers",
      "answer": "\nC. To support spill code generated by compilers"
    }
  },
  {
    "textbook-paragraph": "{Stack Operations*}\n\nTwo types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage.\n\nStack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.  \n\nThe third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the third use of stack operations?",
      "answer": "\nThe third use of stack operations is to support spill code generated by compilers."
    }
  },
  {
    "textbook-paragraph": "{Stack Operations*}\n\nTwo types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage.\n\nStack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.  \n\nThe third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the stack support procedure calls?",
      "answer": "\nThe stack supports procedure calls by storing the return address and any values that need to be passed to the called procedure. The called procedure can then access these values as needed."
    }
  },
  {
    "textbook-paragraph": "{Control Unit Design}\n\nAppendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.\n\nIn this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.\n\nThe control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.\n\nIn this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.\n\nLet's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.\n\nOther von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.  \n\nThe processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).  \n\nThe outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.\n\nThe basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.\n\n",
    "GPT-3-Generations": {
      "question": "Who is the control unit responsible for fetching instructions from memory and executing them?",
      "answer": "\nThe control unit is responsible for fetching instructions from memory and executing them. This is done by first fetching the instruction from memory, then decoding it, and finally executing it."
    }
  },
  {
    "textbook-paragraph": "{LC-3 Datapath Control Signals}\n\nAs we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.  \n\nThe figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege.\n\n\nSome of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design.\n\nLet's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right.\n\nFirst, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values.\n\nNext, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can\n\n\n\nsignal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory \n\n\n\nappear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short.\n\n\n\n{file=part4/patt-patel-appendix-C/APPC03.eps,width=6.5in}\n\n\n\nThe third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWho/What/When/Where/How:\nWhat are the control signals for the LC-3 datapath?",
      "answer": "\nThe control signals for the LC-3 datapath are LD.MAR, LD.MDR, LD.IR, LD.BEN, LD.REG, LD.CC and LD.PC. These signals specify whether registers in the datapath load new values. GatePC, GateMDR, GateALU and GateMARMUX are 1-bit signals for tri-state buffers that control access to the bus. PCMUX and DRMUX are 2-bit signals that select values to write to the program counter and destination register respectively. SR1MUX is a 2-bit signal that selects a register to read from the register file while ADDR1MUX and ADDR2MUX are 1-bit and 2-bit signals respectively that select components of an address. MARMUX is a 1-bit signal that selects the type of address generation while ALUK is a 2-bit signal that selects arithmetic logic unit operation. MIO_EN is a 1-bit signal that enables memory while R_W is a 1 bit signal indicating whether to read or write from memory.\""
    }
  },
  {
    "textbook-paragraph": "{Example Control Word: ADD}\n\nBefore we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege).\n\n{{file=part4/patt-patel-appendix-C/APPC02.eps,width=6.25in}}\n\nConsider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state.\n\nThe RTL for the state is: DR  SR + OP2, set CC.\n\nWe can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath. \n\nLet's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low.\n\nWhat about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA).\n\nIf the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged.\n\n\nThe earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6).\n\nSome of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to\n\n\n{file=part4/patt-patel-appendix-C/APPC06.eps,width=4.45in}\n\n\npass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier).\n\nThe rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter.\n\nThe ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the value of the control signals LD.REG and LD.CC for the state that implements the ADD instruction?",
      "answer": "\nAnswer : LD.REG and LD.CC are high for the state that implements the ADD instruction."
    }
  },
  {
    "textbook-paragraph": "{Example Control Word: LDR}\n\nAs a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page.\n\nThe RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram.\n\nWhat is the control word for this state?\n\nLet's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0.\n\nThe address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low.\n\nMore of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL.\n\nThe SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written.\n\nThe output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care.\n\nThese 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nA. LD.MAR=0, GateMARMUX=1, ADDR1MUX=0, ADDR2MUX=0, MARMUX=0, PCMUX=0, DRMUX=0\nB. LD.MAR=1, GatePCPlus4 = 1, GateALU = 1, GateMDR = 1\nC. LD.MAR=1, GateMARMUX = 1 , ADDR1MUX = 2 , ADDR2MUX = 1 , MARMUX = 1 \nD. LD.MAR=1, GatePCPlus4 = 0 , GateALU = 0 ,Gate MDR = 0",
      "answer": "\nC. LD.MAR=1, GateMARMUX = 1 , ADDR1MUX = 2 , ADDR2MUX = 1 , MARMUX = 1"
    }
  },
  {
    "textbook-paragraph": "{Example Control Word: LDR}\n\nAs a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page.\n\nThe RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram.\n\nWhat is the control word for this state?\n\nLet's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0.\n\nThe address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low.\n\nMore of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL.\n\nThe SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written.\n\nThe output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care.\n\nThese 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the control word for state 6 in the figure?",
      "answer": "\nThe control word for state 6 in the figure is: LD.MAR=1, GateMARMUX=1, ADDR2MUX=1, SR1MUX=1, and ADDR1MUX=0."
    }
  },
  {
    "textbook-paragraph": "{Example Control Word: LDR}\n\nAs a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page.\n\nThe RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram.\n\nWhat is the control word for this state?\n\nLet's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0.\n\nThe address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low.\n\nMore of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL.\n\nThe SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written.\n\nThe output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care.\n\nThese 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWho is responsible for writing the RTL for state number 6?",
      "answer": "\nThe RTL for state number 6 is written by the BaseR."
    }
  },
  {
    "textbook-paragraph": "{Hardwired Control}\n\n\nNow we are ready to think about how control signals can be generated.\n\nAs illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction.\n\nLet's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a\n\n\n{file=part4/figs/inst-loop.eps,width=2.5in}\n\n\ncounter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.}\n\nThis approach in general is called { hardwired control}.\n\nHow many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath.\n\nGiven a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access.\n\nSuch a low clock rate is usually not acceptable.\n\nMore generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode.\n\nAlthough the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).  \n\nIn fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory.\n\nThe control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory.\n\n\nThe figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure.\n\n\n\n\n\nHow complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?  \n\nHere's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts).\n\nThe control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12].\n\nThe control signals are thus reduced to fairly simple functions.\n\nLet's imagine building a hardwired control unit for the {LC-3}.\n\nLet's start by being more precise about the number of inputs to\nthe combinational logic.\n\nAlthough most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic.\n\nHow many datapath status signals are needed?\n\nWhen the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN.\n\nThese two datapath status signals suffice for our design.\n\nHow many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter.\n\nWe thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter.\n\nAdding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables.\n\nThat's still a lot of big {K-maps} to solve.  Is there an\neasier way?\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the control unit for such a design called?\n\nA. Hardwired control\nB. Single-cycle control\nC. Multi-cycle control\nD. Decoupled control",
      "answer": "\nC. Multi-cycle control"
    }
  },
  {
    "textbook-paragraph": "{Hardwired Control}\n\n\nNow we are ready to think about how control signals can be generated.\n\nAs illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction.\n\nLet's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a\n\n\n{file=part4/figs/inst-loop.eps,width=2.5in}\n\n\ncounter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.}\n\nThis approach in general is called { hardwired control}.\n\nHow many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath.\n\nGiven a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access.\n\nSuch a low clock rate is usually not acceptable.\n\nMore generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode.\n\nAlthough the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).  \n\nIn fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory.\n\nThe control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory.\n\n\nThe figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure.\n\n\n\n\n\nHow complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?  \n\nHere's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts).\n\nThe control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12].\n\nThe control signals are thus reduced to fairly simple functions.\n\nLet's imagine building a hardwired control unit for the {LC-3}.\n\nLet's start by being more precise about the number of inputs to\nthe combinational logic.\n\nAlthough most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic.\n\nHow many datapath status signals are needed?\n\nWhen the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN.\n\nThese two datapath status signals suffice for our design.\n\nHow many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter.\n\nWe thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter.\n\nAdding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables.\n\nThat's still a lot of big {K-maps} to solve.  Is there an\neasier way?\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the five inputs to the control unit's combinational logic?",
      "answer": "\nThe five inputs to the control unit's combinational logic are IR[15:11], R, BEN, a 3-bit counter and RESET."
    }
  },
  {
    "textbook-paragraph": "{Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhy does the microprogrammed control unit need a way to decide which microinstruction should execute next?",
      "answer": "\nThe microprogrammed control unit needs a way to decide which microinstruction should execute next because sometimes there's no choice: we have only one next microinstruction.  One simple approach is then to add the address (the 5-bit state ID) of the next microinstruction to the control ROM."
    }
  },
  {
    "textbook-paragraph": "{Redundancy and Coding}\n\nThis set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.\n\nWe begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.\n\nBuilding on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.\n\nWe leave discussion of more sophisticated schemes to classes on\ncoding and information theory.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is an example of a representation in which some bit patterns have no meaning?",
      "answer": "\nAn example of a representation in which some bit patterns have no meaning is a parity check. A parity check is a method of checking for errors in data transmission. A parity check uses a system of odd and even numbers to check for errors. If the number of 1s in the data is odd, the parity bit is set to 1. If the number of 1s in the data is even, the parity bit is set to 0."
    }
  },
  {
    "textbook-paragraph": "{Redundancy and Coding}\n\nThis set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.\n\nWe begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.\n\nBuilding on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.\n\nWe leave discussion of more sophisticated schemes to classes on\ncoding and information theory.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does using sparsely populated representations protect against accidental changes to bits?",
      "answer": "\nSparsely populated representations protect against accidental changes to bits by making it more difficult for errors to go undetected. When a bit changes accidentally, it is more likely to be noticed if there are fewer bits that can change without affecting the meaning of the representation. This makes it easier to detect errors, and also makes it more likely that any errors that do occur will be corrected."
    }
  },
  {
    "textbook-paragraph": "{Sparse Representations}\n\nRepresentations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values.\n\nLet's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class.\n\nThe first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday!\n\n\nThe second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3).\n\nThe third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.  \n\nOnly patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems).\n\n\n{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\n\n\nWhich of the following is not a type of code used to represent decimal digits?\na) Binary-coded Decimal\nb) 2-out-of-5\nc) Excess-3\nd) All of the above are types of codes used to represent decimal digits.",
      "answer": "\nd) All of the above are types of codes used to represent decimal digits."
    }
  },
  {
    "textbook-paragraph": "{Sparse Representations}\n\nRepresentations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values.\n\nLet's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class.\n\nThe first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday!\n\n\nThe second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3).\n\nThe third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.  \n\nOnly patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems).\n\n\n{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "Why is it important to note that BCD is not the same as the unsigned representation?",
      "answer": "\nThe reason it is important to note that BCD is not the same as the unsigned representation is because BCD uses a different encoding for each decimal digit. In BCD, the patterns 1010, 1011, 1111 are unused, which means that these patterns do not represent any value."
    }
  },
  {
    "textbook-paragraph": "{Error Detection}\n\nErrors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system.\n\nAs a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}.\n\n\n\nDigital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically.\n\nOften, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error.\n\nWhen a bit error occurs, however, we must assume that \nit can happen to any of the bits.\n\nThe use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}.\n\nLet's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001.\n\nAs we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.  \n\nNotice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred.\n\nWhat if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error!\n\n\n",
    "GPT-3-Generations": {
      "question": "\n1. What is the simplest model of an error in a digital system?\na. A bit error is when a bit that should have the value 0 instead has the value 1, or a bit that should have the value 1 instead has the value 0.\nb. A byte error is when a byte that should have the value 0 instead has the value 1, or a byte that should have the value 1 instead has the value 0.\nc. A cosmic ray error is when a cosmic ray hits and changes a bit from 0 to 1 or from 1 to 0.\nd. All of the above",
      "answer": "\nAnswer : \n\nThe simplest model of an error in a digital system is a bit error. A bit error is when a bit that should have the value 0 instead has the value 1, or when a bit that should have the value 1 instead has the value 0."
    }
  },
  {
    "textbook-paragraph": "{Error Detection}\n\nErrors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system.\n\nAs a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}.\n\n\n\nDigital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically.\n\nOften, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error.\n\nWhen a bit error occurs, however, we must assume that \nit can happen to any of the bits.\n\nThe use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}.\n\nLet's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001.\n\nAs we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.  \n\nNotice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred.\n\nWhat if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error!\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the 2-out-of-5 code enable a digital system to detect any single bit error?",
      "answer": "\nThe 2-out-of-5 code enables a digital system to detect any single bit error by the fact that regardless of which digit is represented, and regardless of which bit has an error, the resulting error pattern never has a meaning in the code. So if an error is detected, it can be assumed that a single bit error has occurred."
    }
  },
  {
    "textbook-paragraph": "\n\nThe ability to detect any single bit error is certainly useful.\nHowever, so far we have only shown how to protect ourselves when we\nwant to represent decimal digits.  Do we need to develop a separate\nerror-tolerant representation for every type of information that\nwe might want to represent?  Or can we instead come up with a more\ngeneral approach?\n\nThe answer to the second question is yes: we can, in fact, systematically\ntransform any representation into a representation that allows detection of a\nsingle bit error.  The key to this transformation is the idea of\n{ parity}.\n\n\nConsider an arbitrary representation for some type of information.\nBy way of example, we use the {3-bit} unsigned representation.\nFor each pattern used in the representation, we can count the number\nof 1s.  The resulting count is either odd or even.  By adding an extra\nbit---called a { parity bit}---to the representation, and \nselecting the parity bit's value \nappropriately for each bit pattern, we can ensure that the count of 1s\nis odd (called { odd parity}) or even (called { even parity})\nfor all values represented.  The idea is\n\nillustrated in the table\nto the right for the {3-bit} unsigned representation.  The parity\nbits are shown in bold.\n\n\n{c|c|c|c|c}\nvalue      &   3-bit & number& with odd& with even \nrepresented& unsigned& of 1s & parity  & parity  \n0& 000& 0& 000{ 1}& 000{ 0}\n1& 001& 1& 001{ 0}& 001{ 1}\n2& 010& 1& 010{ 0}& 010{ 1}\n3& 011& 2& 011{ 1}& 011{ 0}\n4& 100& 1& 100{ 0}& 100{ 1}\n5& 101& 2& 101{ 1}& 101{ 0}\n6& 110& 2& 110{ 1}& 110{ 0}\n7& 111& 3& 111{ 0}& 111{ 1}\n\n\n\nEither approach to selecting the parity bits ensures that any single\nbit error can be detected.  For example, if we choose to use odd\nparity, a single\nbit error changes either a 0 into a 1 or a 1 into a 0.\nThe number of 1s in the resulting error pattern thus\ndiffers by exactly one from the original pattern, and the parity of\nthe error pattern is even.  But all valid patterns have odd parity,\nso any single bit error can be detected by simply counting the number\nof 1s.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between odd parity and even parity?",
      "answer": "\nThe difference between odd parity and even parity is that odd parity results in the number of 1s in the pattern being odd, while even parity results in the number of 1s in the pattern being even."
    }
  },
  {
    "textbook-paragraph": "\n\nThe ability to detect any single bit error is certainly useful.\nHowever, so far we have only shown how to protect ourselves when we\nwant to represent decimal digits.  Do we need to develop a separate\nerror-tolerant representation for every type of information that\nwe might want to represent?  Or can we instead come up with a more\ngeneral approach?\n\nThe answer to the second question is yes: we can, in fact, systematically\ntransform any representation into a representation that allows detection of a\nsingle bit error.  The key to this transformation is the idea of\n{ parity}.\n\n\nConsider an arbitrary representation for some type of information.\nBy way of example, we use the {3-bit} unsigned representation.\nFor each pattern used in the representation, we can count the number\nof 1s.  The resulting count is either odd or even.  By adding an extra\nbit---called a { parity bit}---to the representation, and \nselecting the parity bit's value \nappropriately for each bit pattern, we can ensure that the count of 1s\nis odd (called { odd parity}) or even (called { even parity})\nfor all values represented.  The idea is\n\nillustrated in the table\nto the right for the {3-bit} unsigned representation.  The parity\nbits are shown in bold.\n\n\n{c|c|c|c|c}\nvalue      &   3-bit & number& with odd& with even \nrepresented& unsigned& of 1s & parity  & parity  \n0& 000& 0& 000{ 1}& 000{ 0}\n1& 001& 1& 001{ 0}& 001{ 1}\n2& 010& 1& 010{ 0}& 010{ 1}\n3& 011& 2& 011{ 1}& 011{ 0}\n4& 100& 1& 100{ 0}& 100{ 1}\n5& 101& 2& 101{ 1}& 101{ 0}\n6& 110& 2& 110{ 1}& 110{ 0}\n7& 111& 3& 111{ 0}& 111{ 1}\n\n\n\nEither approach to selecting the parity bits ensures that any single\nbit error can be detected.  For example, if we choose to use odd\nparity, a single\nbit error changes either a 0 into a 1 or a 1 into a 0.\nThe number of 1s in the resulting error pattern thus\ndiffers by exactly one from the original pattern, and the parity of\nthe error pattern is even.  But all valid patterns have odd parity,\nso any single bit error can be detected by simply counting the number\nof 1s.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the idea of parity allow us to systematically transform any representation into a representation that allows detection of a single bit error?",
      "answer": "\nThe idea of parity allows us to systematically transform any representation into a representation that allows detection of a single bit error by adding an extra bit---called a parity bit---to the representation, and selecting the parity bit's value appropriately for each bit pattern. This ensures that the count of 1s is odd (called odd parity) or even (called even parity) for all values represented."
    }
  },
  {
    "textbook-paragraph": "{Error Correction}\n\nDetection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it.\n\nMany real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them.\n\nCan we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher.\n\n\n\nLet's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too?\n\nImagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity.\n\nThe same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors.\n\n\nError correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance).\n\nAs a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer.\n\n\n{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111\n\n\n\nSince a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result.\n\nLet's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the Hamming distance of a code with error correction?\n\nA) 2\nB) 3\nC) 4\nD) 5",
      "answer": "\nThe Hamming distance of a code with error correction is 4."
    }
  },
  {
    "textbook-paragraph": "{Error Correction}\n\nDetection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it.\n\nMany real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them.\n\nCan we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher.\n\n\n\nLet's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too?\n\nImagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity.\n\nThe same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors.\n\n\nError correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance).\n\nAs a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer.\n\n\n{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111\n\n\n\nSince a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result.\n\nLet's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the Hamming distance of a code?",
      "answer": "\nThe Hamming distance of a code is the minimum number of bit errors that can occur in a code word and still be detected by the code."
    }
  },
  {
    "textbook-paragraph": "{Error Correction}\n\nDetection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it.\n\nMany real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them.\n\nCan we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher.\n\n\n\nLet's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too?\n\nImagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity.\n\nThe same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors.\n\n\nError correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance).\n\nAs a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer.\n\n\n{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111\n\n\n\nSince a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result.\n\nLet's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhy is it important to be able to correct errors?",
      "answer": "\nOne motivation for error correction is to enable digital systems to function in the presence of noise. \nNoise can come from a variety of sources, including electrical interference, cosmic rays, and even the thermal motion of atoms. \nIf a digital system is not able to correct errors introduced by noise, then it may produce incorrect results or fail entirely. \n\nAnother motivation for error correction is to maintain the integrity of stored data. \nFor example, consider a database that contains medical records. \nIf even a single bit is corrupted in the database, then the information stored may be incorrect or completely meaningless. \nError correction can help to ensure that the data stored in such a database is accurate and can be used reliably."
    }
  },
  {
    "textbook-paragraph": "{SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nHow does adding a parity bit enable a system to perform both single error correction and double error detection?",
      "answer": "\nAdding a parity bit enables a system to perform both single error correction and double error detection because it increases the Hamming distance to 4. This means that if two bit errors occur, they will be detected as they will be at least Hamming distance 2 apart."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "1. Which of the following is not a control signal?\nA. Memory data register\nB. Memory address register\nC. Instruction register\nD. Program counter\n2. What is the purpose of an assembler?\nA. To encode instructions into machine code\nB. To decode instructions into machine code\nC. To translate assembly code into machine code\nD. To translate machine code into assembly code",
      "answer": "\nThe purpose of an assembler is to translate assembly code into machine code."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat are the three types of subdivisions in systematic decomposition?",
      "answer": "\nThe three types of subdivisions in systematic decomposition are sequential, conditional and iterative."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWho is responsible for designing the control unit of a processor?",
      "answer": "\nThe control unit of a processor is designed by the hardware engineers."
    }
  },
  {
    "textbook-paragraph": "{Example: Bit-Sliced Addition}\n\nIn this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers.\n\n",
    "GPT-3-Generations": {
      "question": "Q: Which of the following is an advantage of using a ripple-carry adder over other types of adders?\nA. It is faster\nB. It uses less power\nC. It is smaller\nD. All of the above",
      "answer": "\nA. Ripple-carry adders are faster than other types of adders because they use a carry-lookahead structure.\n\nB. Ripple-carry adders use less power than other types of adders because they have a smaller number of transistors.\n\nC. Ripple-carry adders are smaller than other types of adders because they have a smaller number of transistors."
    }
  },
  {
    "textbook-paragraph": "{Example: Bit-Sliced Addition}\n\nIn this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers.\n\n",
    "GPT-3-Generations": {
      "question": "What is an example of an important abstraction for logic design that is introduced in these notes?",
      "answer": "\nThe important abstraction for logic design introduced in these notes is the concept of a ripple-carry adder. A ripple-carry adder is a type of adder that uses the carry bit from one addition to determine the carry bit for the next addition. This type of adder is often used in hardware designs because it is relatively simple to implement."
    }
  },
  {
    "textbook-paragraph": "{Example: Bit-Sliced Addition}\n\nIn this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers.\n\n",
    "GPT-3-Generations": {
      "question": "How does one design a ripple-carry adder?",
      "answer": "\nThe ripple-carry adder is a logic circuit that performs addition on two integers. The adder is designed by first breaking down the addition problem into smaller, more manageable subproblems. For example, when adding two single-digit numbers, the addition problem can be broken down into four subproblems:\n\n1. Determine the sum of the two digits.\n2. Determine if there is a carry out from the addition of the two digits.\n3. Propagate the carry out to the next digit in the integer (if there is one).\n4. Repeat steps 1-3 for each subsequent digit in the integer until all digits have been added."
    }
  },
  {
    "textbook-paragraph": "{One Bit at a Time}\n\nMany of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.\n\nWhen we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.\n\nWhen we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.\n\nWhen we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.\n\nThe resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the significance of \"bit-sliced\" in relation to adders?",
      "answer": "\nBit-sliced refers to the process of breaking down an operation into repeated operations on individual bits. This is done in order to build combinational logic that implements the desired calculation. Bit-slicing can be used to create adders of any size, by using copies of a bit-sliced adder circuit. This approach is less efficient than designing a circuit specifically for adding two numbers of a certain size, but it is much simpler."
    }
  },
  {
    "textbook-paragraph": "{Adders and Word Size}\n\n\nNow that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right.\n\nThe adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder.\n\n\n{file=part2/figs/adder-trad.eps,width=1.3in}\n\n\nYou may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size.\n\n\nHaving seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right.\n\nIf we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement).\n\n\n{file=part2/figs/adder-x2.eps,width=2.15in}\n\n\nYou should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the word size for most desktop and laptop computers?",
      "answer": "\nAnswer : The word size for most desktop and laptop computers is 64 bits."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 2 of the Course}\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nThe difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nWe'll start with the skills, and leave the easy stuff for the next page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.}\n\n{Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.}\n\n{Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).}\n\n{More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.}\n\n\n\nWhen designing combinational logic, we expect you to be able to apply\nthe following design strategies:\n\n{}{{}{}\n{}{}{}\n\n{Make use of human algorithms \n(for example, multiplication from addition).}\n\n{Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.}\n\n{Break truth tables into parts so as to solve each part of a function \nseparately.}\n\n{Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.}\n\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).}\n\n{Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.}\n\n{Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''}\n\n{Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.}\n\n{Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.}\n\n{Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work.\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  \n\n[t]\n{}{{}{}\n{}{}{}\n\n{Boolean functions and logic gates}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n\n\n{majority function}\n\n\n{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load}\n\n\n{design metrics}\n{-}{{}{}\n{}{}{}\n\n\n\n\n{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay}\n\n\n{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube}\n\n\n{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)}\n\n{prime implicant}\n{bit-slicing}\n{timing diagram}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain}\n\n\n{Boolean logic terms}\n{-}{{}{}\n{}{}{}\n\n{algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence}\n\n\n{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)}\n\n{sequential logic}\n{-}{{}{}\n{}{}{}\n\n{active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered}\n\n{clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating}\n\n{clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift}\n\n\n\n\n\n\n\n\n\n{   }  blank 3rd page\n\n\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\n\n\n\n\n\n\nHow can you apply DeMorgan's laws to simplify the form of the complement of a Boolean expression?",
      "answer": "\nDeMorgan's laws can be applied to simplify the form of the complement of a Boolean expression by breaking it down into smaller parts. For example, if the expression is A + B, the complement would be (A' + B')'."
    }
  },
  {
    "textbook-paragraph": "\n\nThis set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design.\n\n { In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.}\n\n\n",
    "GPT-3-Generations": {
      "question": "\n1) What is a register? \nA) A group of bits used for storage in digital systems.\nB) A type of computer memory that stores data and programs.\nC) A device used to input data into a computer.\nD) A device used to output data from a computer.",
      "answer": "\nA register is a group of bits used for storage in digital systems."
    }
  },
  {
    "textbook-paragraph": "\n\n\nA { register} is a storage element composed from one or more\nflip-flops operating on a common clock.\n\nIn addition to the flip-flops,\nmost registers include logic to control the bits stored by the register.\n\nFor example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle).\n\nTo enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right.\n\nThe LOAD input controls the clock signals through a method known as\n{ clock gating}.\n\n\n{{file=part2/figs/lec16-1a.eps,width=2.45in}}\n{file=part2/figs/lec16-1b.eps,width=3.4in}\n\n\nWhen LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value.\n\nThe problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that\n\nuse your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram.\n\n\nA better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.  \n\nWhen LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.  \n\nWhen LOAD is high, the mux selects the IN input, and the register \nloads a new value.  \n\nThe result is similar to a gated D latch with distinct write enable \nand clock lines.\n\n\n{file=part2/figs/lec16-2.eps,width=2in}\n\n\n\nWe can use this extended flip-flop as a bit slice for a multi-bit register.\n\nA four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput,\n\n\n{file=part2/figs/lec16-3.eps,width=5in}\n\n\nand the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nQ: What is the main advantage of using a mux and feedback loop to create a register, as opposed to clock gating?\nA. Clock skew is less likely to cause timing problems.\nB. The LOAD signal can be lowered while the clock is high without causing incorrect behavior.\nC. There is no need for extra logic to control the bits stored by the register.\nD. All of the above.",
      "answer": "\nThe main advantage of using a mux and feedback loop to create a register, as opposed to clock gating, is that clock skew is less likely to cause timing problems."
    }
  }
]