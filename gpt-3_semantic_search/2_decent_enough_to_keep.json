[
  {
    "textbook-paragraph": "This set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is not a power of two?\nA) 4\nB) 8 \nC) 16 \nD) 24",
      "answer": "\nD) 24"
    }
  },
  {
    "textbook-paragraph": "We then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the best design in terms of other metrics, such as logic complexity?",
      "answer": "\nThere is no definitive answer to this question as it depends on the specific design and what metric is being considered. However, in general, designs that are more intuitive and easier for humans to understand typically make the best designs in terms of other metrics, such as logic complexity. This is because simpler designs are typically easier to implement and require less logic to function correctly."
    }
  },
  {
    "textbook-paragraph": "Before we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\nWhy does the author follow a six-step process when designing an FSM?",
      "answer": "\nThe six-step process is followed in order to design an FSM in a systematic and efficient manner. This process helps to ensure that all the necessary aspects of the FSM are taken into account, and that the final design is both effective and robust."
    }
  },
  {
    "textbook-paragraph": "{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is not a step in developing a sequential circuit?\n\nA) Choose a state representation.\nB) Specify I/O behavior.\nC) Develop an abstract model.\nD) Implement with flip-flops and gates.\nE) Calculate logic expressions.",
      "answer": "\nThe answer is C) Develop an abstract model."
    }
  },
  {
    "textbook-paragraph": "{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the final step in the process of designing a sequential circuit?",
      "answer": "\nThe final step in the process of designing a sequential circuit is to implement it with flip-flops and gates."
    }
  },
  {
    "textbook-paragraph": "In Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the desired behavior?",
      "answer": "\nThe desired behavior is to have a model with states and desired behavior."
    }
  },
  {
    "textbook-paragraph": "Once we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.",
    "GPT-3-Generations": {
      "question": "\n\nHow do we start to make any implicit assumptions clear and to make any other decisions necessary to the design?",
      "answer": "\n\nWe start to make any implicit assumptions clear and to make any other decisions necessary to the design by first understanding the I/O behavior for our FSM. Once we know the input and output requirements, we can start to determine what kind of logic is needed to implement the design. We may also need to make decisions about how state transitions will occur, and what kind of conditions will trigger specific actions."
    }
  },
  {
    "textbook-paragraph": "In Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the internal representation for the bits necessary to encode the state of our FSM?",
      "answer": "\nThe internal representation for the bits necessary to encode the state of our FSM can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand."
    }
  },
  {
    "textbook-paragraph": "In the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly.",
    "GPT-3-Generations": {
      "question": "\n\n1. What is the main idea of the context?\n\nA. The design can leverage meaningful information from the design when selecting the representation.\nB. The implementation is simpler and easier to build correctly.\nC. The design can select the representation that is best suited for the data.\nD. All of the above.",
      "answer": "\nThe main idea of the context is that the design can select the representation that is best suited for the data. This can lead to an implementation that is simpler and easier to build correctly."
    }
  },
  {
    "textbook-paragraph": "In the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is an example of a small design that can leverage meaningful information from the design?",
      "answer": "\nOne example of a small design that can leverage meaningful information from the design is a task scheduler. By understanding the dependencies between tasks, the scheduler can optimize the order in which those tasks are executed to minimize overall completion time."
    }
  },
  {
    "textbook-paragraph": "We also show how one can\nuse abstraction to simplify an implementation.",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is not a benefit of using abstraction to simplify an implementation?\n\nA) It can make the code more readable.\nB) It can make the code more modular.\nC) It can make the code more efficient.\nD) It can make the code more bug-free.",
      "answer": "\nAbstraction can make the code more readable, modular, and bug-free. It can also make the code more efficient."
    }
  },
  {
    "textbook-paragraph": "We also show how one can\nuse abstraction to simplify an implementation.",
    "GPT-3-Generations": {
      "question": "\n\nHow does abstraction help simplify an implementation?",
      "answer": "\nAbstraction helps simplify an implementation by hiding the details of the implementation from the user. This allows the user to focus on the functionality of the system, without having to worry about the details of how it is implemented."
    }
  },
  {
    "textbook-paragraph": "For simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the definition of an FSM?",
      "answer": "\nAn FSM is a finite state machine."
    }
  },
  {
    "textbook-paragraph": "The inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.",
    "GPT-3-Generations": {
      "question": "\n\nHow does a counter work?",
      "answer": "\nA counter is a device that counts or measures something. It can be used to count people, objects, time, or events. Counters are often used in manufacturing and scientific applications."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the two-bit Gray code counter work?",
      "answer": "\nThe two-bit Gray code counter is a digital circuit that counts in binary-coded gray code. The output of the circuit is a two-bit number that represents the current state of the counter. The counter cycles through all four possible states, with each state representing a different binary number."
    }
  },
  {
    "textbook-paragraph": "Each state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the state machine remember which state it is in?",
      "answer": "\n\nThe state machine remembers which state it is in by storing the value of the current state in a register."
    }
  },
  {
    "textbook-paragraph": "We then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.",
    "GPT-3-Generations": {
      "question": "\n\nWhy does the next-state logic need to be completed in order to develop the implementation?",
      "answer": "\nThe next-state logic needs to be completed in order to develop the implementation because it defines the behavior of the counter. Without knowing how the counter will behave, it is not possible to implement it."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle.",
    "GPT-3-Generations": {
      "question": "\n\nHow does this fully-specified transition diagram for a counter work?",
      "answer": "\nThe fully-specified transition diagram for a counter works by moving from one state to the next each cycle. The states form a loop, with the counter moving from one state to the next each cycle."
    }
  },
  {
    "textbook-paragraph": "This design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right.",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about reducing design complexity by making use of common algebraic terms and sub-expressions for different outputs?\n\nA) It is not possible to reduce design complexity in this way.\nB) This can only be done with a computer program.\nC) It is possible to reduce design complexity in this way, but it requires manual identification of opportunities.\nD) It is possible to reduce design complexity in this way, and modern design processes automate the identification of opportunities.",
      "answer": "\nD) It is possible to reduce design complexity in this way, and modern design processes automate the identification of opportunities."
    }
  },
  {
    "textbook-paragraph": "This design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right.",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the purpose of reducing design complexity by making use of common algebraic terms and sub-expressions?",
      "answer": "\n\nThe purpose of reducing design complexity by making use of common algebraic terms and sub-expressions is to enable the development of good equations for the next-state logic. This can be achieved by explicitly identifying and making use of common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program. However, it is important to understand how they arise."
    }
  },
  {
    "textbook-paragraph": "This design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right.",
    "GPT-3-Generations": {
      "question": "\n\nHow does identifying common algebraic terms and sub-expressions for different outputs reduce design complexity?",
      "answer": "\n\nIdentifying common algebraic terms and sub-expressions for different outputs can reduce design complexity by reducing the number of AND gates needed to implement the logic. For example, if two outputs share a common term, then only one AND gate is needed to implement the logic for both outputs. This can reduce the overall complexity of the design."
    }
  },
  {
    "textbook-paragraph": "You immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as",
    "GPT-3-Generations": {
      "question": " output are {Black, Blue, Cyan,\nGray, Green},\n\nHow many flip-flops will we need in order to produce the colors {Black, Blue, Cyan, Gray, Green} as output? \n\nA. 2 \nB. 3 \nC. 4 \nD. 5",
      "answer": "\nAt least three flip-flops will be needed in order to produce the colors {Black, Blue, Cyan, Gray, Green} as output."
    }
  },
  {
    "textbook-paragraph": "A fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB.",
    "GPT-3-Generations": {
      "question": "\n\nHow does the color sequencer transition from one state to the next?",
      "answer": "\nThe color sequencer transitions from one state to the next based on the value of the input signal. If the input signal is high, the color sequencer will transition to the next state. If the input signal is low, the color sequencer will remain in the current state."
    }
  },
  {
    "textbook-paragraph": "{State Representation and Logic Expressions}\n\nLet's think about the representation for the FSM states.  The FSM has \nfive states, so we could use as few as three flip-flops.  Instead, we choose\nto use a { one-hot encoding}, in which any valid bit pattern has exactly\none 1 bit.  In other words, we use five flip-flops instead of three,\nand our states are represented with the bit patterns 10000, 01000, 00100,\n00010, and 00001.\n\nThe table below shows the mapping from each high-level state to \nboth the five-bit encoding for the state as well as the six control signals \nneeded for the datapath.  For each state, the values of the control signals\ncan be found by examining the actions necessary in that state.\n\n{\n{|c|c|cccccc|}\nstate& { S_4S_3S_2S_1S_0}& { IDX.RST}& { IDX.CNT}& { MIN.LD}& { A.LD}& { B.LD}& { CNT.RST} \n{ WAIT}& { 1 0 0 0 0}& 1& 0& 0& 0& 0& 0\n{ INIT}& { 0 1 0 0 0}& 0& 1& 1& 0& 0& 0\n{ PREP}& { 0 0 1 0 0}& 0& 0& 0& 1& 1& 1\n{ COMPARE}& { 0 0 0 1 0}& 0& 0& 0& 0& 0& 0\n{ COPY}& { 0 0 0 0 1}& 0& 1& { THEN}& 0& 0& 0 \n{}\n\n}\n\nThe { WAIT} state needs to set { IDX} to 0 but need not affect \nother register or counter values, so { WAIT} produces a 1 only for\n{ IDX.RST}.  The { INIT} state needs to load { values[0]} into \nthe { MIN} register while simultaneously incrementing the { IDX}\ncounter (from 0 to 1), so { INIT} produces 1s for { IDX.CNT}\nand { MIN.LD}.  The { PREP} state loads both shift registers\nand resets the counter { CNT} by producing 1s for { A.LD},\n{ B.LD}, and { CNT.RST}.  The { COMPARE} state does not\nchange any register values, so it produces all 0s.  Finally, the { COPY}\nstate increments the { IDX} counter while simultaneously loading a\nnew value into the { MIN} register.  The { COPY} state produces 1\nfor { IDX.CNT}, but must use the signal { THEN} coming from the\ndatapath to decide whether or not { MIN} is loaded.\n\n\n\nThe advantage of a one-hot encoding becomes obvious when we write\nequations for the six control signals and the next-state logic, as shown\nto the right.  \n\nImplementing the logic to complete our design now requires only a handful \nof small logic gates.\n\n\n{eqnarray*}\n{ IDX.RST} & = & { S}_4\n{ IDX.CNT} & = & { S}_3 + { S}_0\n{ MIN.LD} & = & { S}_3 + { S}_0  { THEN}\n{ A.LD} & = & { S}_2\n{ B.LD} & = & { S}_2\n{ CNT.RST} & = & { S}_2\n{eqnarray*}\n\n\n{eqnarray*}\n{ S}_4^+ & = & { S}_4  { START} + { S}_0  { DONE}\n{ S}_3^+ & = & { S}_4  { START}\n{ S}_2^+ & = & { S}_3 + { S}_0  { DONE}\n{ S}_1^+ & = & { S}_2 + { S}_1  { LAST}\n{ S}_0^+ & = & { S}_1  { LAST}\n{eqnarray*}\n\n\nNotice that the terms in each control signal can be read directly from \nthe rows of the state table and OR'd together.  The terms in each of the\nnext-state equations represent the incoming arcs for the corresponding\nstate.  For example, the { WAIT} state has one self-loop (the first\nterm) and a transition arc coming from the { COPY} state when the\nloop is done.\n\nThese expressions complete our design.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy did the designer choose to use a one-hot encoding for the FSM states?",
      "answer": "\nThe designer chose to use a one-hot encoding for the FSM states because it is easy to read from the rows of the state table and OR together."
    }
  },
  {
    "textbook-paragraph": "{Physical Design, Sensors, and Timing}\n\n\nTo begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right.\n\nThe four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on.\n\nTransition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton.\n\nIn this design, once a user presses the panic button P, the alarm\nsounds until the user presses the\n\n\n{file=part3/figs/ke-trans-diag-brief.eps,width=4.2in}\n\n\nlock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states?\n\nInstead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter.\n\nLet's say that we want our timeout to be T cycles.\n\nWhen we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs.\n\nTo load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down. \n\nThe counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm.\n\nYou should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit.\n\nHow many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value.\n\n\nWe expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value).\n\nWe need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer.\n\nThe only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm.\n\nFinally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED.\n\nNow that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero.\n\n\n{file=part3/figs/ke-alarm-expansion.eps,width=1.75in}\n\n\nThe first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input.\n\nThe second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1).\n\nThe last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur? \n\nFirst, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop.\n\nThe extension thus requires only a counter, a mux, and a gate, as shown below.\n\n{{file=part3/figs/ke-alarm-exp-impl.eps,width=2.65in}}\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the state transition diagram for the extended design?",
      "answer": "\nThe state transition diagram for the extended design is shown in Figure 1. The four states are labeled with state bits and output bits, S_1S_0/DRA, where D indicates that the driver's door should be unlocked, R indicates that the rest of the doors should be unlocked, and A indicates that the alarm should be on. Transition arcs in the diagram are labeled with concise versions of the inputs ULP (using don't cares), where U represents an unlock button, L represents a lock button, and P represents a panic button. In this design, once a user presses the panic button P, the alarm sounds until either the user presses the lock button L to turn it off or a timeout occurs. If a timeout occurs, the system returns to the locked state even if the lock button has not been pressed."
    }
  },
  {
    "textbook-paragraph": "{Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the main advantage of using a serial design over a bit-sliced design?\n\nA. A serial design is usually much faster than a bit-sliced design.\nB. A serial design is usually smaller in area than a bit-sliced design.\nC. A serial design can handle more bits than a bit-sliced design.\nD. A serial design uses less power than a bit-sliced design.",
      "answer": "\nThe main advantage of using a serial design over a bit-sliced design is that a serial design is usually smaller in area than a bit-sliced design."
    }
  },
  {
    "textbook-paragraph": "{Serialization: Comparator Example}\n\n\nWe now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored\n\n\n{file=part3/figs/ser-compare.eps,width=3.5in}\n\n\ninto flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here).\n\n\nHow does the serial design compare with the bit-sliced design?\n\nAs an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.  \n\nEach bit slice requires six {2-input} gates and two inverters.  \n\nAssume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates\n\n\n{file=part3/figs/comparator-opt-nn.eps,width=4.1in}\n\n\nand six inverters to handle any number of bits.\n\nThus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N.\n\nWhat about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design?\n\nThe performance of the serial design is likely to be much worse\nfor three reasons.\n\nFirst, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.  \n\nSecond, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops.\n\nFinally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly.\n\nWhat is the longest path through our serial comparator?\n\nLet's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays.\n\nIf we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers.\n\nYou might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths.\n\nThe bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two.\n\nSometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware.\n\nIn computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software.\n\nAs a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant.\n\nThis approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice.\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the main difference between the serial and bit-sliced design?\n\nA) The serial design is smaller than the bit-sliced design, and the benefit grows with N.\nB) The performance of the serial design is likely to be much worse for three reasons.\nC) The bit-sliced and serial designs represent two extreme points in a broad space of design possibilities. \nD) Sometimes these tradeoffs may happen at a higher level.",
      "answer": "    \n    \n    \n    The main difference between the serial and bit-sliced design is that the serial design is smaller than the bit-sliced design, and the benefit grows with N."
    }
  },
  {
    "textbook-paragraph": "{Serialization: Comparator Example}\n\n\nWe now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored\n\n\n{file=part3/figs/ser-compare.eps,width=3.5in}\n\n\ninto flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here).\n\n\nHow does the serial design compare with the bit-sliced design?\n\nAs an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.  \n\nEach bit slice requires six {2-input} gates and two inverters.  \n\nAssume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates\n\n\n{file=part3/figs/comparator-opt-nn.eps,width=4.1in}\n\n\nand six inverters to handle any number of bits.\n\nThus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N.\n\nWhat about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design?\n\nThe performance of the serial design is likely to be much worse\nfor three reasons.\n\nFirst, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.  \n\nSecond, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops.\n\nFinally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly.\n\nWhat is the longest path through our serial comparator?\n\nLet's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays.\n\nIf we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers.\n\nYou might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths.\n\nThe bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two.\n\nSometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware.\n\nIn computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software.\n\nAs a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant.\n\nThis approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice.\n\n\n\n",
    "GPT-3-Generations": {
      "question": "What is the difference between a serial and bit-sliced design?",
      "answer": "\nThe main difference between a serial and bit-sliced design is that a serial design processes input bits one at a time, while a bit-sliced design processes multiple input bits simultaneously. This can lead to different performance and area tradeoffs for the two approaches."
    }
  },
  {
    "textbook-paragraph": "{Serialization: Comparator Example}\n\n\nWe now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored\n\n\n{file=part3/figs/ser-compare.eps,width=3.5in}\n\n\ninto flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here).\n\n\nHow does the serial design compare with the bit-sliced design?\n\nAs an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.  \n\nEach bit slice requires six {2-input} gates and two inverters.  \n\nAssume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates\n\n\n{file=part3/figs/comparator-opt-nn.eps,width=4.1in}\n\n\nand six inverters to handle any number of bits.\n\nThus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N.\n\nWhat about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design?\n\nThe performance of the serial design is likely to be much worse\nfor three reasons.\n\nFirst, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.  \n\nSecond, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops.\n\nFinally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly.\n\nWhat is the longest path through our serial comparator?\n\nLet's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays.\n\nIf we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers.\n\nYou might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths.\n\nThe bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two.\n\nSometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware.\n\nIn computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software.\n\nAs a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant.\n\nThis approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice.\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy is the serial design likely to be much worse in performance than the bit-sliced design?",
      "answer": "    \n\n\nThe performance of the serial design is likely to be much worse for three reasons.\n\nFirst, all paths in the design matter, not just the paths from bit slice to bit slice.  None of the inputs can be assumed to be available before the start of the clock cycle, so we must consider all paths from input to output.  \n\nSecond, we must also count gate delays for the selection logic as well as the gates embedded in the flip-flops.\n\nFinally, the result of these calculations may not matter, since the clock speed may well be limited by other logic elsewhere in "
    }
  },
  {
    "textbook-paragraph": "{Finite State Machines}\n\nA { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}.\n\nAn FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.  \n\nWhen an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits.\n\nFor a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable).\n\nAnd, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions.\n\nIn this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops.\n\nIn this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems.\n\nThe table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly.\n\nBy including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class.\n\n\n\n{\n\nmeaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes\n\n}\n\n\n\nAnother tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm. \n\n\n\n{\n\nstate& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM\n\n}\n\n\n\n\nA { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another.\n\nPutting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered.\n\nImplementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks.\n\n\n{file=part3/figs/ke-abs-trans-diag.eps,width=3.45in}\n\n\nFor now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning. \n\n{\n\noutputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed\n\n}\n\nWe can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example).\n\n{\n\n& & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1\n\n}\n\n\n\nWe can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0). \n\n\n{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11 \n\n}\n\n\nIn the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples.\n\n\n{file=part3/figs/ke-trans-diag.eps,width=4.2in}\n\n\nWe have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nQ: What is a finite state machine?\n\nA. A mathematical model of computation used to design digital logic circuits.\n\nB. A model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.\n\nC. A digital logic circuit that can be used to control the behavior of a system by reading a sequence of input signals and outputting a corresponding sequence of output signals.\n\nD. A tool used to describe, develop, and analyze implementations of FSMs with digital logic.",
      "answer": "\nA finite state machine (FSM) is a mathematical model of computation used to design digital logic circuits. It is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs."
    }
  },
  {
    "textbook-paragraph": "{Finite State Machines}\n\nA { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}.\n\nAn FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.  \n\nWhen an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits.\n\nFor a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable).\n\nAnd, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions.\n\nIn this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops.\n\nIn this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems.\n\nThe table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly.\n\nBy including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class.\n\n\n\n{\n\nmeaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes\n\n}\n\n\n\nAnother tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm. \n\n\n\n{\n\nstate& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM\n\n}\n\n\n\n\nA { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another.\n\nPutting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered.\n\nImplementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks.\n\n\n{file=part3/figs/ke-abs-trans-diag.eps,width=3.45in}\n\n\nFor now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning. \n\n{\n\noutputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed\n\n}\n\nWe can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example).\n\n{\n\n& & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1\n\n}\n\n\n\nWe can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0). \n\n\n{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11 \n\n}\n\n\nIn the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples.\n\n\n{file=part3/figs/ke-trans-diag.eps,width=4.2in}\n\n\nWe have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is a finite state machine?",
      "answer": "\nA finite state machine is a mathematical model of computation that can be used to design both digital and analog circuits. It is a system with a finite number of states, which can transition from one state to another in response to input signals. The output of the machine depends on its current state and the inputs."
    }
  },
  {
    "textbook-paragraph": "{Synchronous Counters}\n\nA { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle.\n\nNot all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.  \n\nExcept for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters.\n\n {The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state, \n\n\nThe design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns. \n\nThe task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle. \n\nThe cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n{\n\n{ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0\n\n\n\n{file=part3/figs/sbin3-s2.eps,width=1in}\n{file=part3/figs/sbin3-s1.eps,width=1in}\n{file=part3/figs/sbin3-s0.eps,width=1in} {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*}\n\n}\n\nThe first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious.\n\nWe can also derive the pattern intuitively by asking the following:\n\ngiven a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left.\n\n{{file=part3/figs/ser-gating.eps,width=4.5in}}\n\nThe calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches.\n\n{{file=part3/figs/par-gating.eps,width=4.5in}}\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main difference between synchronous and asynchronous counters?",
      "answer": "\nThe main difference between synchronous and asynchronous counters is that all flip-flops in a synchronous counter are required to use the same clock signal, while this is not the case for an asynchronous counter."
    }
  },
  {
    "textbook-paragraph": "{Timing Issues*}\n\nRipple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.\n\nMore aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.\n\nIf you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main difference between clock gating and power gating?",
      "answer": "\nThe main difference between clock gating and power gating is that clock gating uses logic to control the visibility of a clock signal to flip-flops (or latches), while power gating removes the voltage difference, and the circuit uses no power at all."
    }
  },
  {
    "textbook-paragraph": "{Machine Models}\n\nBefore we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.\n\nHistorically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.\n\nHowever, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.\n\nWhat is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.\n\nAs we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.\n\nThe disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.\n\nIn practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.\n\nThe coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.\n\nBy adding more states to the FSM, we were able to hold the servo in\nplace, as desired.\n\nWhy are we protecting you from the model used in practice?\n\nFirst, timing issues add complexity to a topic that is complex enough \nfor an introductory course.\n\nAnd, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.\n\nIn many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.\n\nWe now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.\n\nAs already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.  \n\nA Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.\n\nThe machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.\n\nNotice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.\n\nNotice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.\n\n\n\n{{file=part3/figs/lec17-3.eps,width=5in}}\n\n\n\nFor a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.\n\n\n\n{{file=part3/figs/lec17-4.eps,width=5in}}\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is true about Moore machines?\n\nA) Outputs depend only on the internal state bits of the FSM.\nB) Outputs may be expressed as functions both of internal state and FSM inputs.\nC) The number of combinations of states and inputs is greater than the number of combinations of states alone.\nD) Allowing a function to depend on inputs reduces the number of internal states needed.",
      "answer": "    \n    \n    \n    A) Outputs depend only on the internal state bits of the FSM."
    }
  },
  {
    "textbook-paragraph": "{Summary of Part 3 of the Course}\n\n\nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nStudents often find this part of the course more challenging than the\nearlier parts of the course.\n\nIn addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.  \n\nStudents typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).  \n\nWe'll start with the easy stuff.  \n\nYou should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy\n\n}\n\n{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)\n\n}\n\n{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram\n\n}\n\n{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers\n\n}\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size\n\n}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM\n\n}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)\n\n}\n{processor datapath}\n\n{control signal}\n\n}\n\n{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux\n\n}\n\n{instruction processing}\n{-}{{}{}\n{}{}{}\n\n\n\n{register transfer language (RTL)}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}\n\n{data movement}\n{control flow}\n\n{addressing modes}\n{-}{{}{}\n{}{}{}\n\n\n{PC-relative}\n\n{base + offset}\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n\nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}\n\n\n\n\n\n{   }  empty 3rd page\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the difference between the Moore and Mealy machine models, as well as why you might find each of them useful when designing an FSM?",
      "answer": "\nThe Moore and Mealy machine models are two ways of designing a finite state machine (FSM). A Moore machine has output logic that depends only on the present state, while a Mealy machine's output logic depends on both the present state and the input. \n\nThere are advantages and disadvantages to each model. A Moore machine is usually simpler to design and understand, since the output logic only depends on the current state. However, this can also be a disadvantage, since it can be more difficult to add features that depend on inputs as well. \n\nA Mealy machine's output logic depending on both inputs and states can make it more flexible, but also more complex. In some cases, it may be possible to design a hybrid of the two models which takes advantage of both approaches."
    }
  },
  {
    "textbook-paragraph": "{Instruction Set Architecture*}\n\nThis set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA.\n\nAs you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA.\n\n",
    "GPT-3-Generations": {
      "question": "Which of the following is not a design principle of a successful ISA?\n\nA) The ISA must be able to support multiple programming paradigms.\nB) The ISA must be easy for compilers to target.\nC) The ISA must expose the underlying hardware in a simple and direct way.\nD) The ISA should be designed with extensibility in mind.",
      "answer": "\nD) The ISA should be designed with extensibility in mind."
    }
  },
  {
    "textbook-paragraph": "{Instruction Set Architecture*}\n\nThis set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA.\n\nAs you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA.\n\n",
    "GPT-3-Generations": {
      "question": "What are the design principles of a successful ISA?",
      "answer": "\nAnswer : \nThe design principles of a successful ISA are:\n1. Simplicity: The ISA should be as simple as possible, to reduce the complexity of the hardware and software implementations.\n2. Regularity: The ISA should be regular, to allow for easy decoding and encoding of instructions.\n3. Flexibility: The ISA should be flexible, to allow for different implementations of the hardware and software.\n4. Efficiency: The ISA should be efficient, to reduce the number of instructions required to perform a task."
    }
  },
  {
    "textbook-paragraph": "{Instruction Set Architecture*}\n\nThis set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA.\n\nAs you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat are some of the design principles that go into designing a successful ISA?",
      "answer": "\nSome of the design principles that go into designing a successful ISA are:\n- simplicity: the ISA should be as simple as possible, to make it easy to understand and use\n- orthogonality: the ISA should be designed so that instructions are independent of each other, to make it easier to write programs\n- efficiency: the ISA should be designed so that instructions can be executed quickly and efficiently"
    }
  },
  {
    "textbook-paragraph": "{Formats and Fields*}\n\nThe LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats.\n\nRecall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.  \n\nAs a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value.\n\nSeveral questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register?\n\nThe answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words.\n\n{ Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA.\n\nFor example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA.\n\n\n\n\nMoving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type.\n\n{{file=part4/figs/lec23-1.eps,width=4in}}\n\nAs a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations.\n\n{ Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted.\n\nSimilarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer.\n\nMemory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.  \n\nAt the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation.\n\n{ Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use.\n\nAs full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.  \n\nRelative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset.\n\nSegmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the advantage of fixed-length instructions?\n\nA. They are easy to fetch and decode.\nB. They simplify the datapath by restricting instructions to the size of the bus and always fetching properly aligned instructions.\nC. They allow more flexibility; indeed, extensions to a variable-length ISA can incorporate new addressing modes that require longer instructions without affecting the original ISA.\nD. All of the above",
      "answer": "\nThe answer is D. All of the above."
    }
  },
  {
    "textbook-paragraph": "{Formats and Fields*}\n\nThe LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats.\n\nRecall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.  \n\nAs a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value.\n\nSeveral questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register?\n\nThe answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words.\n\n{ Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA.\n\nFor example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA.\n\n\n\n\nMoving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type.\n\n{{file=part4/figs/lec23-1.eps,width=4in}}\n\nAs a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations.\n\n{ Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted.\n\nSimilarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer.\n\nMemory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.  \n\nAt the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation.\n\n{ Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use.\n\nAs full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.  \n\nRelative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset.\n\nSegmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat are the three questions that must be answered in order to define the possible instruction formats for an ISA?",
      "answer": "\nThe three questions that must be answered in order to define the possible instruction formats for an ISA are:\n1) Are instructions fixed-length or variable-length?\n2) How many addresses are needed for each instruction, and how many of the addresses can be memory addresses?\n3) What forms of addresses are possible for each operand?"
    }
  },
  {
    "textbook-paragraph": "{Formats and Fields*}\n\nThe LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats.\n\nRecall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.  \n\nAs a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value.\n\nSeveral questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register?\n\nThe answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words.\n\n{ Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA.\n\nFor example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA.\n\n\n\n\nMoving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type.\n\n{{file=part4/figs/lec23-1.eps,width=4in}}\n\nAs a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations.\n\n{ Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted.\n\nSimilarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer.\n\nMemory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.  \n\nAt the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation.\n\n{ Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use.\n\nAs full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.  \n\nRelative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset.\n\nSegmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy did designers choose to employ a fixed-length instruction format for the LC-3 ISA?",
      "answer": "\nThe designers of the LC-3 ISA chose to employ a fixed-length instruction format in order to take advantage of the many benefits that this approach offers. Fixed-length instructions are easy to fetch and decode, and they simplify the datapath by restricting instructions to the size of the bus and always fetching properly aligned instructions. Additionally, using fixed-length instructions eliminates many addressing modes in order to limit the size of the instructions."
    }
  },
  {
    "textbook-paragraph": "{Addressing Architectures*}\n\nOne question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D.\n\nA binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt}\n\nIf all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt}\n\nThe assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long.\n\nAt the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture). \n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nEight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.  \n\nArchitectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt}\n\nThe second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed.\n\n\n\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nThe assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture.\n\nSeveral ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below.\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt}\n\nAccumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt}\n\nThe last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators.\n\nA { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation):\n\n{-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt}\n\nThe resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture:\n\n{-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt}\n\nThe values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the difference between a memory-to-memory architecture and a load-store architecture?",
      "answer": "\nA memory-to-memory architecture is an architecture in which all three addresses can be memory addresses, while a load-store architecture is an architecture in which only loads and stores can use memory addresses."
    }
  },
  {
    "textbook-paragraph": "{Addressing Architectures*}\n\nOne question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D.\n\nA binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt}\n\nIf all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt}\n\nThe assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long.\n\nAt the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture). \n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nEight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.  \n\nArchitectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt}\n\nThe second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed.\n\n\n\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nThe assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture.\n\nSeveral ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below.\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt}\n\nAccumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt}\n\nThe last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators.\n\nA { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation):\n\n{-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt}\n\nThe resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture:\n\n{-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt}\n\nThe values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nHow does the number of addresses needed for each instruction determine the addressing architecture implemented by an ISA?",
      "answer": "\nThe number of addresses needed for each instruction determines the addressing architecture implemented by an ISA in the following way:\n\nThe number of addresses needed for each instruction ranges from zero to three, and is rarely allowed to go beyond three. The answer to the second part determines the {addressing architecture} implemented by an ISA. \n\nThere are five distinct addressing architectures which are illustrated through the use of a running example, the assignment X=AB+C/D.\n\nIf all three addresses can be memory addresses, the ISA is dubbed a {memory-to-memory architecture}. Such architectures may have small register sets or even lack a register file completely. \n\nAt the other extreme is the {load-store architecture} used by the {LC-3} ISA. In a load-store architecture, only loads and stores can use memory addresses; all other operations use only registers. As most instructions use only registers, this type of addressing architecture is also called a {register-to-register architecture}. \n\nArchitectures that use other combinations of memory and register addresses with {3-address} formats are not named. Unary operators and transfer operators require only one source operand, thus can use a {2-address} format (for example, NOT A,B). Binary operations can also use {{2-address}} format if one operand is implicit, as in the following instructions: >>ADD>A,B>; M[A]  M[A] + M[B] >or>ADD>R1,B>; R1  R1 + M[B] The second instruction defines a {register"
    }
  },
  {
    "textbook-paragraph": "{Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the zero register work?",
      "answer": "\nThe zero register is a read-only register that contains the value zero. It is used as a constant and as a destination for operations that are performed only for their side-effects. The availability of a zero register allows certain opcodes to serve double duty. A register-to-register add instruction becomes a register move instruction when one source operand is zero. Similarly, an immediate add instruction becomes an immediate load instruction when one source operand is zero."
    }
  },
  {
    "textbook-paragraph": "{Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow did the reduced instruction set computer come to be?",
      "answer": "\nThe reduced instruction set computer came to be as a result of the increasing density of integrated circuit technology. With the trend toward larger memories, code length became less important, and the performance advantage of fixed-length instructions, which simplifies the datapath and enables pipelining, became attractive. Researchers leveraged these ideas to create reduced instruction set computers. The competing VAXen were labeled complex instruction set computers. RISC machines employ fixed-length instructions and a load-store architecture, allowing only a few addressing modes and small offsets. This combination of design decisions enables deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and for years, RISC machines were viewed by many researchers as the proper design for future ISAs. However, companies such as Intel soon learned to pipeline microoperations after decoding instructions, and CISC architectures now offer competitive if not superior performance in comparison with RISC machines."
    }
  },
  {
    "textbook-paragraph": "{Procedure and System Calls*}\n\nA { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs.\n\nFor our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call.\n\n{-6pt}\n\n=DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1\n\n>DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN\n\n{-6pt}\n\nThe procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again.\n\nAs you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below.\n\n\n{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*}\n\n\n{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*}\n\n\nWhile an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed.\n\nThe term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it.\n\nCalling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee).\n\n\n\n\n\nA typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables.\n\nAs an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure.\n\n\n{file=part4/figs/lec23-2.eps,width=1.25in}\n\n\n[t]\n{\n[t]\n\nint =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n}\n\nprintf (``d'', add3 (10, 20, 30));\n\n\n\nby convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6\n\n\n\n[t]\n\nadd3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1\n\n\n\nThe add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation.\n\n{ System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy is it important for a procedure to have a return value?",
      "answer": "\nIt is important for a procedure to have a return value so that it can communicate the result of its execution to the caller. The return value can be used to indicate whether the procedure was successful or not, and can also provide information about any errors that may have occurred."
    }
  },
  {
    "textbook-paragraph": "{Interrupts and Exceptions*}\n\nUnexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.\n\n{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no \n\n}\n\nInterrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.\n\nThe code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.\n\nInterrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.\n\nAs several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhich of the following is not a characteristic of interrupts?\nA) They are generated by external devices.\nB) They are generated by errors in the program.\nC) They occur asynchronously with respect to the program.\nD) They are checked only after completing an instruction.\nE) They are handled by a handler found in a vector table.",
      "answer": "\nInterrupts are generated by external devices, they occur asynchronously with respect to the program, and they are checked only after completing an instruction."
    }
  },
  {
    "textbook-paragraph": "{Interrupts and Exceptions*}\n\nUnexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.\n\n{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no \n\n}\n\nInterrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.\n\nThe code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.\n\nInterrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.\n\nAs several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhy do interrupts have to be handled asynchronously?",
      "answer": "\nInterrupts have to be handled asynchronously because they are generated by external devices."
    }
  },
  {
    "textbook-paragraph": "{Control Flow Conditions*}\n\nControl flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.  \n\nUnconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.\n\nMany ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.\n\n\n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}\n\nThe status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}\n\nFinally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}\n\nThe three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the control flow work in a computer?",
      "answer": "\nThe control flow in a computer is based on status bits that are set by instructions. These instructions can be either comparisons or bit tests. The most common type of control flow instruction is a branch, which is based on the status of these bits."
    }
  },
  {
    "textbook-paragraph": "{I/O*}\n\nAs a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.\n\nThe question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.\n\nAlternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "How are I/O ports accessed?",
      "answer": "\nThere are two ways to access I/O ports - either using special instructions, or by mapping them to memory addresses. The first method, known as independent I/O, uses distinct instructions for each type of operation. The second method, called memory-mapped I/O, requires no new instructions but demands that a region of the memory address space be set aside for I/O."
    }
  },
  {
    "textbook-paragraph": "{I/O*}\n\nAs a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.\n\nThe question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.\n\nAlternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nHow does a processor connect to other devices to allow input and output?",
      "answer": "\nA processor connects to other devices for input and output using I/O ports. These ports are either read-only or write-only registers which are attached to devices with opposite semantics. This allows communication of data between the processor and the device."
    }
  },
  {
    "textbook-paragraph": "{Control Unit Design}\n\nAppendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.\n\nIn this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.\n\nThe control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.\n\nIn this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.\n\nLet's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.\n\nOther von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.  \n\nThe processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).  \n\nThe outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.\n\nThe basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.\n\n",
    "GPT-3-Generations": {
      "question": "What is the basic problem that must be solved for control unit design?",
      "answer": "\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."
    }
  },
  {
    "textbook-paragraph": "{LC-3 Datapath Control Signals}\n\nAs we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.  \n\nThe figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege.\n\n\nSome of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design.\n\nLet's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right.\n\nFirst, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values.\n\nNext, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can\n\n\n\nsignal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory \n\n\n\nappear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short.\n\n\n\n{file=part4/patt-patel-appendix-C/APPC03.eps,width=6.5in}\n\n\n\nThe third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts.\n\n",
    "GPT-3-Generations": {
      "question": "Which of the following is not a control signal in the LC-3 datapath?\nA) LD.MDR\nB) GatePC\nC) ADDR1MUX\nD) MIO.EN\nE) LD.CC",
      "answer": " \n \n \n \n\n\n\n\n\n The answer is : E) LD.CC"
    }
  },
  {
    "textbook-paragraph": "{LC-3 Datapath Control Signals}\n\nAs we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.  \n\nThe figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege.\n\n\nSome of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design.\n\nLet's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right.\n\nFirst, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values.\n\nNext, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can\n\n\n\nsignal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory \n\n\n\nappear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short.\n\n\n\n{file=part4/patt-patel-appendix-C/APPC03.eps,width=6.5in}\n\n\n\nThe third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts.\n\n",
    "GPT-3-Generations": {
      "question": "What are the three groups of control signals for the datapath without support for privilege and interrupts?",
      "answer": "    \n \n \nThe three groups of control signals for the datapath without support for privilege and interrupts are: 1) a set of seven 1-bit control signals that specifies whether registers in the datapath load new values; 2) four 1-bit signals for tri-state buffers that control access to the bus; and 3) a group of 10 signals that controls multiplexers in the datapath."
    }
  },
  {
    "textbook-paragraph": "{Example Control Word: ADD}\n\nBefore we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege).\n\n{{file=part4/patt-patel-appendix-C/APPC02.eps,width=6.25in}}\n\nConsider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state.\n\nThe RTL for the state is: DR  SR + OP2, set CC.\n\nWe can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath. \n\nLet's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low.\n\nWhat about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA).\n\nIf the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged.\n\n\nThe earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6).\n\nSome of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to\n\n\n{file=part4/patt-patel-appendix-C/APPC06.eps,width=4.45in}\n\n\npass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier).\n\nThe rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter.\n\nThe ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the control word for the state that implements the ADD instruction?\n\nA) LD.REG and LD.CC must be high, GateALU=1, and the other Gate signals---GatePC, GateMDR, and GateMARMUX---must all be 0.\nB) DR  SR + OP2, set CC\nC) LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC must be low\nD) DRMUX must select its IR[11:9] input in order to write to the destination register specified by the ADD instruction",
      "answer": "\nThe control word for the state that implements the ADD instruction is LD.REG and LD.CC must be high, GateALU=1, and the other Gate signals---GatePC, GateMDR, and GateMARMUX---must all be 0."
    }
  },
  {
    "textbook-paragraph": "{Example Control Word: ADD}\n\nBefore we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege).\n\n{{file=part4/patt-patel-appendix-C/APPC02.eps,width=6.25in}}\n\nConsider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state.\n\nThe RTL for the state is: DR  SR + OP2, set CC.\n\nWe can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath. \n\nLet's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low.\n\nWhat about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA).\n\nIf the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged.\n\n\nThe earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6).\n\nSome of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to\n\n\n{file=part4/patt-patel-appendix-C/APPC06.eps,width=4.45in}\n\n\npass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier).\n\nThe rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter.\n\nThe ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the control unit generate the control signals for the ADD instruction?",
      "answer": "\nThe control unit for the ADD instruction must generate a control word with the following signal values: LD.REG = 1, LD.CC = 1, GateALU = 1, DRMUX = IR[11:9], SR1MUX = IR[8:6], and ALUK = 0. The values of the other signals are don't cares."
    }
  },
  {
    "textbook-paragraph": "{Hardwired Control}\n\n\nNow we are ready to think about how control signals can be generated.\n\nAs illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction.\n\nLet's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a\n\n\n{file=part4/figs/inst-loop.eps,width=2.5in}\n\n\ncounter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.}\n\nThis approach in general is called { hardwired control}.\n\nHow many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath.\n\nGiven a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access.\n\nSuch a low clock rate is usually not acceptable.\n\nMore generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode.\n\nAlthough the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).  \n\nIn fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory.\n\nThe control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory.\n\n\nThe figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure.\n\n\n\n\n\nHow complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?  \n\nHere's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts).\n\nThe control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12].\n\nThe control signals are thus reduced to fairly simple functions.\n\nLet's imagine building a hardwired control unit for the {LC-3}.\n\nLet's start by being more precise about the number of inputs to\nthe combinational logic.\n\nAlthough most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic.\n\nHow many datapath status signals are needed?\n\nWhen the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN.\n\nThese two datapath status signals suffice for our design.\n\nHow many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter.\n\nWe thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter.\n\nAdding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables.\n\nThat's still a lot of big {K-maps} to solve.  Is there an\neasier way?\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the difference between single-cycle and multi-cycle hardwired control?",
      "answer": "\nAnswer :\n\nThe difference between single-cycle and multi-cycle hardwired control is that, in single-cycle control, the entire instruction is processed in one clock cycle, while in multi-cycle control, the instruction is processed in multiple clock cycles."
    }
  },
  {
    "textbook-paragraph": "{Using a Memory for Logic Functions}\n\nConsider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.\n\nSynthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.\n\nThis strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.\n\nIn programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.\n\nThe FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.\n\nFor many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.\n\nLet's return to our {LC-3} example.\n\nInstead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).\n\nWe just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.\n\nThe ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.\n\nWe can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.\n\nNext, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.\n\nAnd our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.\n\nFinally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.\n\nOur final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the main advantage of using a memory to compute logic functions as opposed to using other methods?",
      "answer": "\nThe main advantage of using a memory to compute logic functions is that it is easier to modify the functions later. If a mistake is made in computing one of the functions, only a few bits in the memory need to be changed, rather than solving equations and reoptimizing and replacing logic gates. Additionally, if more space is needed, the design can be easily extended by adding more functions to the memory."
    }
  },
  {
    "textbook-paragraph": "{Using a Memory for Logic Functions}\n\nConsider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.\n\nSynthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.\n\nThis strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.\n\nIn programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.\n\nThe FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.\n\nFor many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.\n\nLet's return to our {LC-3} example.\n\nInstead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).\n\nWe just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.\n\nThe ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.\n\nWe can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.\n\nNext, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.\n\nAnd our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.\n\nFinally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.\n\nOur final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does using a memory to compute logic functions simplify the design process?",
      "answer": "\nAnswer :\n\nUsing a memory to compute logic functions simplifies the design process by reducing the number of gates needed and by making it easier to modify the functions later."
    }
  },
  {
    "textbook-paragraph": "{Sparse Representations}\n\nRepresentations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values.\n\nLet's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class.\n\nThe first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday!\n\n\nThe second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3).\n\nThe third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.  \n\nOnly patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems).\n\n\n{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000\n\n\n\n\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the Excess-3 code?",
      "answer": "\nThe Excess-3 code is a code in which each decimal digit is represented by the pattern corresponding to the 4-bit unsigned pattern for d+3."
    }
  },
  {
    "textbook-paragraph": "{Error Detection}\n\nErrors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system.\n\nAs a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}.\n\n\n\nDigital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically.\n\nOften, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error.\n\nWhen a bit error occurs, however, we must assume that \nit can happen to any of the bits.\n\nThe use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}.\n\nLet's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001.\n\nAs we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.  \n\nNotice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred.\n\nWhat if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error!\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the definition of a bit error?",
      "answer": "\nA bit error is an error that occurs when a bit that should have the value 0 instead has the value 1, or a bit that should have the value 1 instead has the value 0."
    }
  },
  {
    "textbook-paragraph": "{Hamming Distance}\n\nNext, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.\n\nAs a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.  \n\nWe refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.  \n\nThe metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.\n\nThe Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.\n\nThe Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.\n\nIn contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.\n\nNow let's think about the problem slightly differently.\n\nGiven a particular representation, \n\nhow many bit errors can we detect in values using that representation?\n\n{ A representation with Hamming distance d can detect up to d-1 bit errors.}\n\nTo understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.\n\nA digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the Hamming distance of the {2-out-of-5} code?",
      "answer": "\nThe Hamming distance of the {2-out-of-5} code is 2."
    }
  },
  {
    "textbook-paragraph": "{Hamming Distance}\n\nNext, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.\n\nAs a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.  \n\nWe refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.  \n\nThe metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.\n\nThe Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.\n\nThe Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.\n\nIn contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.\n\nNow let's think about the problem slightly differently.\n\nGiven a particular representation, \n\nhow many bit errors can we detect in values using that representation?\n\n{ A representation with Hamming distance d can detect up to d-1 bit errors.}\n\nTo understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.\n\nA digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the Hamming distance of a representation relate to the number of bit errors that can be detected in values using that representation?",
      "answer": "\nThe Hamming distance of a representation is the minimum number of bit errors that can occur before a system might fail to detect errors in a stored value. A digital system using the representation can detect up to d-1 errors, where d is the Hamming distance of the representation."
    }
  },
  {
    "textbook-paragraph": "{Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the Hamming code work?",
      "answer": "\nThe Hamming code works by adding parity check bits to a data stream. These parity check bits are used to detect and correct errors that occur during transmission. The number of parity check bits added is determined by the size of the data stream. For example, a 7-bit Hamming code can detect and correct up to 2 bit errors."
    }
  },
  {
    "textbook-paragraph": "{One Bit at a Time}\n\nMany of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.\n\nWhen we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.\n\nWhen we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.\n\nWhen we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.\n\nThe resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nHow does the bit-sliced adder circuit work?",
      "answer": "\nThe bit-sliced adder circuit is a combinational logic circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size. \n\nThe bit-sliced adder circuit works by first adding the least significant bits, then moving to the second least significant, and so on. As it goes, it may need to carry from lower bits into higher bits. When it reaches the most significant bits, it compares them and if they are equal, the two numbers are equal."
    }
  },
  {
    "textbook-paragraph": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals.",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the design process for a digital FSM?\\n",
      "answer": "\nThe design process for a digital FSM includes six steps: developing an abstract model, specifying I/O behavior, completing the specification, choosing a state representation, calculating logic expressions, and implementing with flip-flops and gates."
    }
  },
  {
    "textbook-paragraph": "level state, to simplify the implementation.} {Use counters to insert time-based transitions between states (such  as timeouts).} {Implement an FSM using logic components such as registers,  counters, comparators, and adders as building blocks.} {Explain the basic organization of a computer's microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions.} {Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a  processor control unit state machine diagram.}   And, at the highest level, we expect that you will be able to do the following:  {}{{}{} {}{}{}  {Explain the difference between the Moore and Mealy machine models,  as well as why you might find each of them useful when designing an FSM.} {Understand the need for initialization of an FSM, be able to analyze  and identify potential problems arising from lack of initialization, and  be able to extend an implementation to include initialization to an  appropriate state when necessary.} {Understand how the choice of internal state bits for an FSM can  affect the complexity of the implementation of next-state and output  logic, and be able to select a reasonable state assignment.} {Identify and fix design flaws in simple FSMs by analyzing an existing  implementation, comparing it with the specification, and removing any  differences by making any necessary changes to the implementation.}      {   }  empty 3rd page      {Instruction Set Architecture*}  This set of notes discusses  tradeoffs and design elements of instruction set architectures (ISAs). { The material is beyond the scope of our class, and is provided purely for your interest.}  Those who find these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA.  As you know, the ISA defines the interface between software and hardware, abstracting the capabilities of a computer's datapath and standardizing the format of instructions to utilize those capabilities.  Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA.  Rather, they are extended, and their original forms must be supported for decades (consider, for example, the IBM 360 and the Intel x86).  Employing sound design principles is thus imperative in an ISA.  {Formats and Fields*}  The LC-3 ISA  employs fixed-length instructions and a load-store architecture, two aspects that help to reduce the design space to a manageable set of choices.  In a general ISA design, many other options exist for instruction formats.  Recall the idea of separating the bits of an instruction into (possibly non-contiguous) fields.  One of the fields must contain an opcode, which specifies the type of operation to be performed by the instruction.  In the {LC-3} ISA, most opcodes specify both the type of operation and the types of arguments to the operation.   More generally, many addressing modes are possible for each operand, and we can think of the bits that specify the addressing mode as a separate field, known as the { mode} field.    As a simple example, the {LC-3's} ADD and AND instructions  contain a {1-bit} mode field that specifies whether the second operand of the ADD/AND comes from a register or is an immediate value.  Several questions must be answered in order to define the possible instruction formats for an ISA.  First, are instructions fixed-length or variable-length?  Second, how many addresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, what forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited offsets relative to a register?  The answer to the first question depends on many factors, but several clear advantages exist for both answers.  { Fixed-length instructions} are easy to fetch and decode.  A",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the need for FSM initialization?\\n",
      "answer": "\nAn FSM must be initialized to ensure that it starts in a known state and to avoid undefined behavior. Without initialization, an FSM may produce incorrect results or may not work at all."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How are next-state variables and output signals implemented in a FSM?\\n",
      "answer": "\nA FSM is implemented as a digital system, with all states represented as patterns using a fixed number of bits, all inputs translated into bits, and all outputs translated into bits. Transition rules must be complete, meaning that given any state of the FSM and any pattern of input bits, a transition must be defined from that state to another state. Outputs for a digital FSM are calculated using Boolean logic expressions."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Why is it important to make implicit assumptions clear in the design process?\\n",
      "answer": "\nThere are a few reasons why it is important to make implicit assumptions clear in the design process. First, if assumptions are not made clear, it can lead to confusion and misunderstanding among the team members working on the design. Second, making assumptions explicit can help to ensure that everyone is on the same page and working towards the same goal. Finally, explicit assumptions can help to prevent errors and mistakes in the design process."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How do you develop the implementation for a Gray code counter?', '",
      "answer": "\nThe implementation for a Gray code counter is developed by first filling in the K-maps for the next-state values, then deriving algebraic expressions. The next-state logic is then used to develop the implementation."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the purpose of a Gray code counter?\\n",
      "answer": "\nA Gray code counter is a type of digital counter that is used to count the number of clock pulses. It is named after Frank Gray, who invented the code in 1953. The main advantage of a Gray code counter over other types of counters is that it is less likely to produce errors when counting."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are some applications of a Gray code counter?\\n",
      "answer": "\nA Gray code counter is used in applications where a digital value must be changed incrementally with the least number of steps. This is because a Gray code counter only changes one bit at a time, which minimizes the number of digital values that must be changed."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How do you design a Gray code counter?']",
      "answer": "\nA Gray code counter is designed by first creating a transition diagram that shows the states and outputs of the counter. Next, the next-state logic is developed by creating K-maps for the next-state values and deriving algebraic expressions. Finally, the next-state logic is implemented in a diagram."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How do you design a Gray code counter?']",
      "answer": "\nA Gray code counter is designed by first creating a transition diagram that shows the states and outputs of the counter. Next, the next-state logic is developed by creating K-maps for the next-state values and deriving algebraic expressions. Finally, the next-state logic is implemented in a diagram."
    }
  },
  {
    "textbook-paragraph": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the color mapping for a three-bit RGB encoding system?\\n\\n",
      "answer": "\nThe color mapping for a three-bit RGB encoding system is a system where colors are represented by a combination of red, green, and blue light. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors."
    }
  },
  {
    "textbook-paragraph": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How do you implement an FSM?']",
      "answer": "\nThe FSM is implemented using registers, counters, and a comparator. The comparator is used to implement the if statement in the program. The registers and counters are used to implement the variables idx and min in the program. The memory is used to implement the array values in the program."
    }
  },
  {
    "textbook-paragraph": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the purpose of a Gray code?\\n",
      "answer": "\nA Gray code is a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns."
    }
  },
  {
    "textbook-paragraph": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the purpose of a Gray code?\\n",
      "answer": "\nA Gray code is a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What do the states represent in the transition diagram?\\n",
      "answer": "\nThe states in the transition diagram represent the different colors of the stoplight. The first two states are red, followed by yellow, and then green. The last two states are again red."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What does the uninterrupted counting cycle represent?\\n",
      "answer": "\nThe uninterrupted counting cycle represents the duration of each state in the FSM. This is used to calculate the duration of each state in the FSM when F=0."
    }
  },
  {
    "textbook-paragraph": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the output behavior?\\n ",
      "answer": "\nThe output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30."
    }
  },
  {
    "textbook-paragraph": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the purpose of using a Gray code?\\n ",
      "answer": "\nThe purpose of using a Gray code is to create a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit."
    }
  },
  {
    "textbook-paragraph": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango.",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How do you determine what should happen if a user presses both buttons (HG=11)?\\n",
      "answer": "\nIf a user presses both buttons (HG=11), the ice cream cup overflows. This is because the outputs are C_L=11 and C_M=10, so our dispenser releases a full serving of each flavor, or two servings total."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Why is it important to consider all possible input conditions from all possible states?\\n",
      "answer": "\nOne important reason to consider all possible input conditions from all possible states is that it can help ensure that a design is correct. For example, if a design only ever receives input values that are within a certain range, it is possible that the design is not actually correct for all input values. Considering all possible input conditions can help catch errors like this.\n\nAnother reason to consider all possible input conditions is that it can help optimize a design. For example, if a design only ever receives input values that are within a certain range, it is possible to simplify the design or make it more efficient. Considering all possible input conditions can help identify opportunities for optimization."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How can a transition diagram help with this process?\\n",
      "answer": "\nA transition diagram can help with this process by helping to identify the most important metrics and by helping to balance the design effectively according to those metrics."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How can the state table form make the missing parts of the specification even more obvious?\\n",
      "answer": "\nThe state table form of a specification can make the missing parts of the specification more obvious by showing the different states that the system can be in and the transitions between those states. This can help to identify what parts of the specification are missing and what needs to be added in order to complete the specification."
    }
  },
  {
    "textbook-paragraph": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango.",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What should happen if a user presses the ``go'' button (HG=01)?\\n",
      "answer": "\nIf a user presses the \"go\" button (HG=01), the machine should dispense one and a half servings of lychee and a half serving of mango."
    }
  },
  {
    "textbook-paragraph": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the right strategy for selecting a state representation?\\n",
      "answer": "\nThe right strategy is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Why is it often best to start by selecting a representation that makes sense to a human?\\n",
      "answer": "\nOne reason it is often best to start by selecting a representation that makes sense to a human is that it can be easier to understand and work with. Additionally, starting with a representation that makes sense to a human can help ensure that the final product is also easy to understand and use."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does the number of state bits affect the design complexity?\\n",
      "answer": "\nThe number of state bits affects the design complexity because it determines the number of possible states that the FSM can be in. A larger number of state bits means that there are more possible states and the design must be able to handle all of them."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does the use of abstraction in selecting a representation impact the design metrics?']",
      "answer": "\nAbstraction in selecting a representation can impact the design metrics by reducing the number of metrics that need to be considered. This can simplify the process of designing a solution and make it easier to find an optimal solution."
    }
  },
  {
    "textbook-paragraph": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the value of HOLD when the counter is in the COUNT A state and S_2=0?\\n",
      "answer": "\nHOLD is 1 when the counter is in the COUNT A state and S_2=0. This is because the counter is already halted and we didn't press the \"go\" button."
    }
  },
  {
    "textbook-paragraph": "unit.  We define S as the number represented by the bit pattern produced by the add unit.  When overflow  occurs, S=C, but we always have (S=C)  2^N.  { Theorem:}   Addition of two {N-bit} 2's complement numbers A and B overflows if and only if one of the following conditions holds:  {A<0 and B<0 and S} {A and B and S<0}   { Proof:}  We once again start with the ``if'' direction.  That is, if condition 1  or condition 2 holds, we have an overflow.  The proofs are straightforward. Given condition 1, we can add the two inequalities A<0 and B<0 to  obtain C=A+B<0.  But S, so clearly S=C, thus overflow  has occurred.  Similarly, if condition 2 holds, we can add the inequalities A and B to obtain C=A+B.  Here we have S<0, so again S=C, and we have an overflow.  We must now prove the ``only if'' direction, showing that any overflow implies either condition 1 or condition 2.  By the  { contrapositive}{If we have a statement of the form (p implies q), its contrapositive is the  statement (not q implies not p). Both statements have the same truth value.  In this case, we can turn our Lemma around as stated.} of our Lemma, we know that if an overflow occurs, either both operands are  negative, or they are both positive.      Let's start with the case in which both operands are negative, so A<0 and B<0, and thus the real sum C<0 as well.  Given that A and B are represented as {N-bit} 2's complement, they must fall in the representable range, so we can write  {eqnarray*} -2^{N-1}  & A & < 0 -2^{N-1}  & B & < 0 {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} -2^N  & C & < 0 {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C<0, it cannot be larger than the largest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} -2^N  & C & < -2^{N-1} {eqnarray*}  We now add 2^N to each part to obtain  {eqnarray*} 0  & C + 2^N & < 2^{N-1} {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} 0  & S & < 2^{N-1} {eqnarray*}  Thus, if we have an overflow and both A<0 and B<0, the resulting sum S, and condition 1 holds.  The proof for the case in which we observe an overflow when  both operands are non-negative (A and B) is similar, and leads to condition 2.  We again begin with inequalities for A and B:  {eqnarray*} 0  & A & < 2^{N-1} 0  & B & < 2^{N-1} {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} 0  & C < & 2^N {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C{}0, it cannot be smaller than the smallest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} 2^{N-1}  & C & < 2^N {eqnarray*}  We now subtract 2^N to each part to obtain  {eqnarray*} -2^{N-1}  & C - 2^N & < 0 {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} -2^{N-1}  & S & < 0 {eqnarray*}  Thus, if we have an overflow and both A and B, the resulting sum S<0, and condition 2 holds.  Thus overflow implies either condition 1 or condition 2, completing our proof.",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the value of S_2 when the counter is in the COUNT A state and HOLD=0?\\n",
      "answer": "\nThe value of S_2 when the counter is in the COUNT A state and HOLD=0 is undefined. This is because, as the passage states, overflow occurs when either both operands are negative or both operands are positive. If both operands are negative, then the resulting sum S is equal to C. However, if both operands are positive, then the resulting sum S is less than C. Thus, it is impossible to determine the value of S_2 without knowing the values of both operands."
    }
  },
  {
    "textbook-paragraph": "{Designing the Logic}\n\nNow we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below.\n\nTo the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}.\n\nWe suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations.\n\n{\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1\n\n\n\n{file=part2/figs/add-cin.eps,width=1in}\n{file=part2/figs/add-s.eps,width=1in}\n\n\n{eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*}\n\n}\n\nThe equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement.\n\nWe rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version.\n\n{{file=part2/figs/adder-bit.eps,width=2.55in}\n{file=part2/figs/adder-bit-nn.eps,width=2.55in}}\n\nThe gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR.\n\nLet's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors).\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway.\n\nWhen we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the majority function?",
      "answer": "\nThe majority function is a logic function that produces a carry whenever at least two out of the three input bits are 1s."
    }
  },
  {
    "textbook-paragraph": "{Designing the Logic}\n\nNow we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below.\n\nTo the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}.\n\nWe suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations.\n\n{\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1\n\n\n\n{file=part2/figs/add-cin.eps,width=1in}\n{file=part2/figs/add-s.eps,width=1in}\n\n\n{eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*}\n\n}\n\nThe equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement.\n\nWe rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version.\n\n{{file=part2/figs/adder-bit.eps,width=2.55in}\n{file=part2/figs/adder-bit-nn.eps,width=2.55in}}\n\nThe gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR.\n\nLet's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors).\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway.\n\nWhen we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays.\n\n\n",
    "GPT-3-Generations": {
      "question": "\nWhat is the majority function?",
      "answer": "\nThe majority function is a logic function that produces a carry whenever at least two out of the three input bits are 1s."
    }
  },
  {
    "textbook-paragraph": "\n\n\nA { register} is a storage element composed from one or more\nflip-flops operating on a common clock.\n\nIn addition to the flip-flops,\nmost registers include logic to control the bits stored by the register.\n\nFor example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle).\n\nTo enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right.\n\nThe LOAD input controls the clock signals through a method known as\n{ clock gating}.\n\n\n{{file=part2/figs/lec16-1a.eps,width=2.45in}}\n{file=part2/figs/lec16-1b.eps,width=3.4in}\n\n\nWhen LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value.\n\nThe problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that\n\nuse your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram.\n\n\nA better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.  \n\nWhen LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.  \n\nWhen LOAD is high, the mux selects the IN input, and the register \nloads a new value.  \n\nThe result is similar to a gated D latch with distinct write enable \nand clock lines.\n\n\n{file=part2/figs/lec16-2.eps,width=2in}\n\n\n\nWe can use this extended flip-flop as a bit slice for a multi-bit register.\n\nA four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput,\n\n\n{file=part2/figs/lec16-3.eps,width=5in}\n\n\nand the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is a register?",
      "answer": "\nA register is a storage element composed from one or more flip-flops operating on a common clock. In addition to the flip-flops, most registers include logic to control the bits stored by the register."
    }
  },
  {
    "textbook-paragraph": "{Boolean Properties and Don't Care Simplification}\n\nThis set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables.\n\nWe then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation.\n\nThis technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly.\n\nWe conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems.\n\n",
    "GPT-3-Generations": {
      "question": "What is the value of underspecifying a logic function?",
      "answer": "\nThe value of underspecifying a logic function is that it allows for selection of the simplest possible implementation. This technique must be used carefully to avoid incorrect behavior, so we illustrate the possibility of misuse with an example, then talk about several ways of solving the example correctly."
    }
  },
  {
    "textbook-paragraph": "{Boolean Properties and Don't Care Simplification}\n\nThis set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables.\n\nWe then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation.\n\nThis technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly.\n\nWe conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems.\n\n",
    "GPT-3-Generations": {
      "question": "Why is it important to be able to simplify Boolean expressions?",
      "answer": "\nAnswer : There are two main reasons for wanting to simplify Boolean expressions. The first is that it can lead to more efficient implementations. The second is that it can help to reduce the number of potential errors."
    }
  },
  {
    "textbook-paragraph": "{Logic Properties}\n\nTable  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.\n\nAs mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.\n\nTo calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.\n\n{ Variables are not changed when finding the dual form.}\n\nThe dual form of a dual form is the original logic statement.\n\nBe careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.\n\n{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}\n\nDuality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).\n\nThe rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.\n\nSecond, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.  \n\nA function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.\n\nHowever, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').\n\nFinally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.\n\n",
    "GPT-3-Generations": {
      "question": "\n\n1. Which of the following is not a property of Boolean logic?\nA) Commutative Property\nB) Associative Property\nC) Distributive Property\nD) Identity Property\nE) Negation Property\nF) Inverse Property\nG) Transitive Property",
      "answer": " \n\nThe answer is C) Distributive Property."
    }
  },
  {
    "textbook-paragraph": "{Logic Properties}\n\nTable  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.\n\nAs mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.\n\nTo calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.\n\n{ Variables are not changed when finding the dual form.}\n\nThe dual form of a dual form is the original logic statement.\n\nBe careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.\n\n{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}\n\nDuality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).\n\nThe rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.\n\nSecond, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.  \n\nA function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.\n\nHowever, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').\n\nFinally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nWhat is the principle of duality?",
      "answer": "\nThe principle of duality states that any theorem or identity has the same truth value in dual form."
    }
  },
  {
    "textbook-paragraph": "{Logic Properties}\n\nTable  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.\n\nAs mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.\n\nTo calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.\n\n{ Variables are not changed when finding the dual form.}\n\nThe dual form of a dual form is the original logic statement.\n\nBe careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.\n\n{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}\n\nDuality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).\n\nThe rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.\n\nSecond, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.  \n\nA function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.\n\nHowever, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').\n\nFinally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.\n\n",
    "GPT-3-Generations": {
      "question": "\n\nHow does duality have several useful practical applications?",
      "answer": "\nDuality has several useful practical applications.  First, the principle of duality states that any theorem or identity has the same truth value in dual form (we do not prove the principle here). The rows of Table are organized according to this principle: each row contains two equations that are the duals of one another.  However, we rename variables freely to simplify the dual form.  For example, taking the dual of 1+A=1, we obtain 0=0.  But we created variable A solely for the purpose of expressing the property, so we might instead have called the complementary value A.  We rename (using the same name!) to obtain 0=0, as shown in the table. Second, the dual form is useful when designing certain types of logic, such as networks of transistors connecting output of a CMOS gate to high voltage and ground. If you look at gate designs in textbook (and particularly those in exercises), you will notice that these networks are duals."
    }
  }
]